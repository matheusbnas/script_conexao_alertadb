#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“¦ EXPORTAR TABELA PLUVIOMÃ‰TRICOS PARA PARQUET

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ PROPÃ“SITO DESTE SCRIPT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Este script exporta a tabela "pluviometricos" do banco alertadb_cor para 
arquivos Parquet. Os arquivos Parquet sÃ£o eficientes, comprimidos e ideais 
para backup, transferÃªncia de dados ou anÃ¡lise offline.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ O QUE ESTE SCRIPT FAZ:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Conecta ao banco alertadb_cor
âœ… Exporta dados da tabela pluviometricos
âœ… Salva em formato Parquet (comprimido e eficiente)
âœ… Divide dados por ano (opcional) para facilitar gerenciamento
âœ… Mostra progresso detalhado durante a exportaÃ§Ã£o
âœ… Exibe estatÃ­sticas finais (tamanho dos arquivos, total de registros)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ COMO USAR:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Configure o arquivo .env com as credenciais:
   
   # Banco ORIGEM para EXPORTAÃ‡ÃƒO (alertadb_cor)
   DB_COPIA_ORIGEM_HOST=10.50.30.166
   DB_COPIA_ORIGEM_PORT=5432
   DB_COPIA_ORIGEM_NAME=alertadb_cor
   DB_COPIA_ORIGEM_USER=postgres
   DB_COPIA_ORIGEM_PASSWORD=

2. Execute: python scripts/exportar_pluviometricos_parquet.py

3. Os arquivos serÃ£o salvos em: exports/pluviometricos_YYYY.parquet

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¦ DEPENDÃŠNCIAS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

pip install pandas pyarrow psycopg2-binary sqlalchemy python-dotenv

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# ğŸ”§ Importar bibliotecas necessÃ¡rias
import psycopg2
import pandas as pd
from sqlalchemy import create_engine
import os
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path
import sys
import warnings

# Suprimir warnings do pandas sobre DBAPI2
warnings.filterwarnings('ignore', category=UserWarning, module='pandas')

# Carregar variÃ¡veis de ambiente
project_root = Path(__file__).parent.parent.parent
load_dotenv(dotenv_path=project_root / '.env')

def obter_variavel(nome, obrigatoria=True):
    """ObtÃ©m variÃ¡vel de ambiente, lanÃ§a erro se obrigatÃ³ria e nÃ£o encontrada."""
    valor = os.getenv(nome)
    if obrigatoria and not valor:
        raise ValueError(f"âŒ VariÃ¡vel de ambiente obrigatÃ³ria nÃ£o encontrada: {nome}")
    return valor

def carregar_configuracoes():
    """Carrega todas as configuraÃ§Ãµes do arquivo .env."""
    try:
        # âš™ï¸ ConfiguraÃ§Ãµes de conexÃ£o ORIGEM (alertadb_cor)
        origem = {
            'host': obter_variavel('DB_COPIA_ORIGEM_HOST'),
            'port': obter_variavel('DB_COPIA_ORIGEM_PORT', obrigatoria=False) or '5432',
            'dbname': obter_variavel('DB_COPIA_ORIGEM_NAME'),
            'user': obter_variavel('DB_COPIA_ORIGEM_USER'),
            'password': obter_variavel('DB_COPIA_ORIGEM_PASSWORD'),
            'connect_timeout': 10
        }
        
        # Criar string de conexÃ£o SQLAlchemy
        connection_string = f"postgresql://{origem['user']}:{origem['password']}@{origem['host']}:{origem['port']}/{origem['dbname']}"
        
        return origem, connection_string
    
    except ValueError as e:
        print("=" * 60)
        print("âŒ ERRO DE CONFIGURAÃ‡ÃƒO")
        print("=" * 60)
        print(str(e))
        print("\nğŸ“ Verifique se o arquivo .env existe e contÃ©m todas as variÃ¡veis necessÃ¡rias")
        print("\nğŸ’¡ VariÃ¡veis necessÃ¡rias:")
        print("   - DB_COPIA_ORIGEM_HOST, DB_COPIA_ORIGEM_NAME, DB_COPIA_ORIGEM_USER, DB_COPIA_ORIGEM_PASSWORD")
        print("=" * 60)
        raise

# Carregar configuraÃ§Ãµes
ORIGEM, CONNECTION_STRING = carregar_configuracoes()

def criar_diretorio_exports():
    """Cria o diretÃ³rio exports se nÃ£o existir."""
    exports_dir = project_root / 'exports'
    exports_dir.mkdir(exist_ok=True)
    return exports_dir

def testar_conexao():
    """Testa a conexÃ£o com o banco."""
    print("=" * 60)
    print("TESTE DE CONEXÃƒO")
    print("=" * 60)
    
    try:
        conn = psycopg2.connect(**ORIGEM)
        print(f"   âœ… CONEXÃƒO ({ORIGEM['host']}:{ORIGEM['port']}/{ORIGEM['dbname']}): SUCESSO!")
        conn.close()
        return True
        
    except Exception as e:
        print(f"   âŒ ERRO: {e}")
        return False

def verificar_tabela():
    """Verifica se a tabela existe e retorna estatÃ­sticas."""
    conn = None
    cur = None
    
    try:
        conn = psycopg2.connect(**ORIGEM)
        cur = conn.cursor()
        
        # Verificar se a tabela existe
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = 'pluviometricos'
            );
        """)
        existe = cur.fetchone()[0]
        
        if not existe:
            print("   âŒ ERRO: A tabela 'pluviometricos' nÃ£o existe no banco!")
            return False, None
        
        # Contar registros
        cur.execute("SELECT COUNT(*) FROM pluviometricos;")
        total = cur.fetchone()[0]
        
        # Obter perÃ­odo dos dados
        cur.execute("SELECT MIN(dia), MAX(dia) FROM pluviometricos;")
        datas = cur.fetchone()
        data_min = datas[0] if datas[0] else None
        data_max = datas[1] if datas[1] else None
        
        print(f"   âœ… Tabela encontrada!")
        print(f"   ğŸ“Š Total de registros: {total:,}")
        if data_min and data_max:
            print(f"   ğŸ“… PerÃ­odo: {data_min} atÃ© {data_max}")
        
        return True, {'total': total, 'data_min': data_min, 'data_max': data_max}
        
    except Exception as e:
        print(f"   âŒ Erro ao verificar tabela: {e}")
        return False, None
    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()

def exportar_por_ano(engine, exports_dir):
    """Exporta dados divididos por ano."""
    print("\n" + "=" * 60)
    print("EXPORTANDO DADOS POR ANO")
    print("=" * 60)
    
    # Obter lista de anos disponÃ­veis
    query_anos = """
        SELECT DISTINCT EXTRACT(YEAR FROM dia)::INTEGER as ano
        FROM pluviometricos
        ORDER BY ano;
    """
    df_anos = pd.read_sql_query(query_anos, engine)
    anos = df_anos['ano'].tolist()
    
    print(f"\nğŸ“… Encontrados {len(anos)} anos: {anos}")
    print("   Exportando cada ano em um arquivo separado...\n")
    
    arquivos_criados = []
    total_registros = 0
    tamanho_total = 0
    
    for ano in anos:
        print(f"   ğŸ“¦ Exportando ano {ano}...")
        
        # Ler dados do ano
        query = f"""
        SELECT dia, m05, m10, m15, h01, h04, h24, h96, estacao, estacao_id
        FROM pluviometricos
        WHERE EXTRACT(YEAR FROM dia) = {ano}
        ORDER BY dia, estacao_id;
        """
        
        df = pd.read_sql_query(query, engine)
        
        if len(df) == 0:
            print(f"      âš ï¸  Nenhum dado encontrado para {ano}")
            continue
        
        # Salvar em Parquet
        arquivo = exports_dir / f'pluviometricos_{ano}.parquet'
        df.to_parquet(arquivo, compression='snappy', index=False)
        
        tamanho_mb = arquivo.stat().st_size / (1024 * 1024)
        arquivos_criados.append(arquivo)
        total_registros += len(df)
        tamanho_total += tamanho_mb
        
        print(f"      âœ… {len(df):,} registros exportados â†’ {arquivo.name} ({tamanho_mb:.2f} MB)")
    
    return arquivos_criados, total_registros, tamanho_total

def exportar_tudo(engine, exports_dir):
    """Exporta todos os dados em um Ãºnico arquivo."""
    print("\n" + "=" * 60)
    print("EXPORTANDO TODOS OS DADOS")
    print("=" * 60)
    
    print("\nğŸ“¥ Lendo dados do banco...")
    print("   Isso pode levar alguns minutos dependendo do volume...\n")
    
    inicio = datetime.now()
    
    # Ler todos os dados
    query = """
    SELECT dia, m05, m10, m15, h01, h04, h24, h96, estacao, estacao_id
    FROM pluviometricos
    ORDER BY dia, estacao_id;
    """
    
    # Usar chunksize para processar em lotes e nÃ£o sobrecarregar memÃ³ria
    chunksize = 100000
    arquivos_criados = []
    total_registros = 0
    chunk_numero = 1
    
    for chunk_df in pd.read_sql_query(query, engine, chunksize=chunksize):
        print(f"   ğŸ“¦ Processando chunk {chunk_numero} ({len(chunk_df):,} registros)...")
        
        # Se Ã© o primeiro chunk, criar arquivo novo
        if chunk_numero == 1:
            arquivo = exports_dir / 'pluviometricos_completo.parquet'
            chunk_df.to_parquet(arquivo, compression='snappy', index=False, engine='pyarrow')
        else:
            # Para chunks subsequentes, ler arquivo existente, concatenar e salvar
            df_existente = pd.read_parquet(arquivo)
            df_combinado = pd.concat([df_existente, chunk_df], ignore_index=True)
            df_combinado.to_parquet(arquivo, compression='snappy', index=False, engine='pyarrow')
        
        total_registros += len(chunk_df)
        chunk_numero += 1
    
    tempo_decorrido = (datetime.now() - inicio).total_seconds()
    tamanho_mb = arquivo.stat().st_size / (1024 * 1024)
    arquivos_criados.append(arquivo)
    
    print(f"\n   âœ… ExportaÃ§Ã£o concluÃ­da!")
    print(f"      Arquivo: {arquivo.name}")
    print(f"      Registros: {total_registros:,}")
    print(f"      Tamanho: {tamanho_mb:.2f} MB")
    print(f"      Tempo: {tempo_decorrido:.1f} segundos ({tempo_decorrido/60:.1f} minutos)")
    
    return arquivos_criados, total_registros, tamanho_mb

def main():
    """FunÃ§Ã£o principal que executa a exportaÃ§Ã£o."""
    print("=" * 60)
    print("ğŸ“¦ EXPORTAR TABELA PLUVIOMÃ‰TRICOS PARA PARQUET")
    print("=" * 60)
    print()
    print("ğŸ¯ PROPÃ“SITO:")
    print("   Este script exporta a tabela 'pluviometricos' do banco alertadb_cor")
    print("   para arquivos Parquet (formato eficiente e comprimido).")
    print()
    print("ğŸ“‹ O QUE SERÃ FEITO:")
    print("   âœ… Verificar conexÃ£o com o banco")
    print("   âœ… Verificar se a tabela existe")
    print("   âœ… Exportar dados para arquivos Parquet")
    print("   âœ… Mostrar estatÃ­sticas finais")
    print()
    print("=" * 60)
    
    # Testar conexÃ£o
    if not testar_conexao():
        print("\nâŒ Falha no teste de conexÃ£o. Abortando...")
        return
    
    # Verificar tabela
    print("\nğŸ“‹ Verificando tabela...")
    existe, stats = verificar_tabela()
    if not existe:
        print("\nâŒ NÃ£o foi possÃ­vel continuar. Abortando...")
        return
    
    # Criar diretÃ³rio de exports
    exports_dir = criar_diretorio_exports()
    print(f"\nğŸ“ DiretÃ³rio de exportaÃ§Ã£o: {exports_dir}")
    
    # Perguntar se deseja dividir por ano
    print("\nâ“ Como deseja exportar os dados?")
    print("   1. Dividir por ano (um arquivo por ano) - Recomendado para grandes volumes")
    print("   2. Um Ãºnico arquivo (todos os dados)")
    
    opcao = input("\n   Escolha (1 ou 2): ").strip()
    
    # Criar engine SQLAlchemy (recomendado pelo pandas)
    engine = create_engine(CONNECTION_STRING, pool_pre_ping=True)
    
    try:
        if opcao == '1':
            arquivos, total, tamanho = exportar_por_ano(engine, exports_dir)
        else:
            arquivos, total, tamanho = exportar_tudo(engine, exports_dir)
        
        # EstatÃ­sticas finais
        print("\n" + "=" * 60)
        print("âœ… EXPORTAÃ‡ÃƒO FINALIZADA COM SUCESSO!")
        print("=" * 60)
        print(f"ğŸ“Š Total de registros exportados: {total:,}")
        print(f"ğŸ“ Arquivos criados: {len(arquivos)}")
        print(f"ğŸ’¾ Tamanho total: {tamanho:.2f} MB")
        print(f"ğŸ“‚ LocalizaÃ§Ã£o: {exports_dir}")
        print("\nğŸ“‹ Arquivos criados:")
        for arquivo in arquivos:
            tamanho_arquivo = arquivo.stat().st_size / (1024 * 1024)
            print(f"   â€¢ {arquivo.name} ({tamanho_arquivo:.2f} MB)")
        print("=" * 60)
        
    finally:
        engine.dispose()

if __name__ == "__main__":
    main()

