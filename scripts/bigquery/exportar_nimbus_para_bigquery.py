#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üåßÔ∏è EXPORTA√á√ÉO DIRETA - NIMBUS ‚Üí BigQuery

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéØ PROP√ìSITO DESTE SCRIPT:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Este script exporta dados diretamente do banco NIMBUS (alertadb) para o 
BigQuery, usando a MESMA l√≥gica de coleta dos scripts servidor166.

ARQUITETURA:
    NIMBUS (alertadb) ‚Üí Parquet ‚Üí BigQuery
              ‚Üë [ESTE SCRIPT - DIRETO]

QUERY UTILIZADA:
    ‚úÖ DISTINCT ON (el."horaLeitura", el.estacao_id)
    ‚úÖ ORDER BY el."horaLeitura" ASC, el.estacao_id ASC, el.id DESC
    ‚úÖ Mesma l√≥gica de carregar_pluviometricos_historicos.py
    ‚úÖ Garante apenas um registro por (dia, estacao_id) - o mais recente

VANTAGENS:
    ‚úÖ Mais r√°pido (menos camadas)
    ‚úÖ Dados sempre da fonte original (NIMBUS)
    ‚úÖ BigQuery otimizado para an√°lises
    ‚úÖ Ideal para stakeholders
    ‚úÖ Formato Parquet (5-10x mais r√°pido que CSV)
    ‚úÖ Exporta√ß√£o completa (todos os dados desde 1997)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã O QUE ESTE SCRIPT FAZ:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ Conecta diretamente ao banco NIMBUS (alertadb)
‚úÖ Busca TODOS os dados usando DISTINCT ON (mesma l√≥gica dos scripts servidor166)
‚úÖ Exporta para formato Parquet completo (n√£o dividido por ano)
‚úÖ Carrega no BigQuery automaticamente
‚úÖ Cria/atualiza tabela no BigQuery
‚úÖ Processa em lotes de 100.000 registros para otimizar mem√≥ria
‚úÖ Preserva tipos de dados e timezone corretamente

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã CONFIGURA√á√ÉO:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Vari√°veis obrigat√≥rias no .env:
- DB_ORIGEM_HOST, DB_ORIGEM_NAME, DB_ORIGEM_USER, DB_ORIGEM_PASSWORD
- BIGQUERY_PROJECT_ID

Vari√°veis opcionais:
- BIGQUERY_DATASET_ID (padr√£o: pluviometricos)
- BIGQUERY_TABLE_ID (padr√£o: pluviometricos)
- BIGQUERY_CREDENTIALS_PATH (opcional: caminho para credentials.json)
- BIGQUERY_CONNECTION_ID (opcional: ID da conex√£o BigQuery existente)

üìö GUIA COMPLETO: Veja docs/BIGQUERY_CONFIGURAR_VARIAVEIS.md para saber
   onde encontrar cada configura√ß√£o no console GCP/BigQuery.
"""

import psycopg2
import pandas as pd
from sqlalchemy import create_engine
from google.cloud import bigquery
from google.oauth2 import service_account
import os
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path
import tempfile

# Carregar vari√°veis de ambiente
project_root = Path(__file__).parent.parent.parent
load_dotenv(dotenv_path=project_root / '.env')

def obter_variavel(nome, obrigatoria=True, padrao=None):
    """Obt√©m vari√°vel de ambiente."""
    valor = os.getenv(nome)
    if not valor or (isinstance(valor, str) and valor.strip() == ''):
        if obrigatoria:
            raise ValueError(f"‚ùå Vari√°vel obrigat√≥ria n√£o encontrada: {nome}")
        return padrao
    return valor.strip() if isinstance(valor, str) else valor

def carregar_configuracoes():
    """Carrega configura√ß√µes do .env."""
    try:
        # Banco ORIGEM - NIMBUS (alertadb)
        origem = {
            'host': obter_variavel('DB_ORIGEM_HOST'),
            'port': obter_variavel('DB_ORIGEM_PORT', obrigatoria=False, padrao='5432'),
            'dbname': obter_variavel('DB_ORIGEM_NAME'),
            'user': obter_variavel('DB_ORIGEM_USER'),
            'password': obter_variavel('DB_ORIGEM_PASSWORD'),
            'sslmode': obter_variavel('DB_ORIGEM_SSLMODE', obrigatoria=False, padrao='disable'),
            'connect_timeout': 10
        }

        # BigQuery
        # Verificar se existe credentials.json na pasta credentials (padr√£o)
        # project_root j√° est√° definido no escopo do m√≥dulo
        credentials_padrao = project_root / 'credentials' / 'credentials.json'
        
        # Se n√£o foi especificado no .env, usar o padr√£o se existir
        credentials_path_env = obter_variavel('BIGQUERY_CREDENTIALS_PATH', obrigatoria=False)
        if credentials_path_env:
            credentials_path = Path(credentials_path_env)
            if not credentials_path.exists():
                print(f"   ‚ö†Ô∏è  Caminho no .env n√£o encontrado: {credentials_path}")
                print(f"   üí° Tentando usar caminho padr√£o: {credentials_padrao}")
                # Tentar o padr√£o mesmo se o .env especificou um caminho inv√°lido
                if credentials_padrao.exists():
                    credentials_path = credentials_padrao
                else:
                    credentials_path = None
        elif credentials_padrao.exists():
            credentials_path = credentials_padrao
        else:
            credentials_path = None
        
        bigquery_config = {
            'project_id': obter_variavel('BIGQUERY_PROJECT_ID'),
            'dataset_id': obter_variavel('BIGQUERY_DATASET_ID', obrigatoria=False, padrao='pluviometricos'),
            'table_id': obter_variavel('BIGQUERY_TABLE_ID', obrigatoria=False, padrao='pluviometricos'),
            'credentials_path': str(credentials_path) if credentials_path else None,
            'connection_id': obter_variavel('BIGQUERY_CONNECTION_ID', obrigatoria=False)  # Opcional: conex√£o existente
        }
        
        return origem, bigquery_config
    
    except ValueError as e:
        print("=" * 70)
        print("‚ùå ERRO DE CONFIGURA√á√ÉO")
        print("=" * 70)
        print(str(e))
        print("\nüìù Configure no .env:")
        print("   # Banco NIMBUS (origem)")
        print("   DB_ORIGEM_HOST=10.2.223.114")
        print("   DB_ORIGEM_NAME=alertadb")
        print("   DB_ORIGEM_USER=planejamento_cor")
        print("   DB_ORIGEM_PASSWORD=sua_senha")
        print("")
        print("   # BigQuery (destino)")
        print("   BIGQUERY_PROJECT_ID=seu-projeto-gcp")
        print("   BIGQUERY_DATASET_ID=pluviometricos (opcional)")
        print("   BIGQUERY_TABLE_ID=pluviometricos (opcional)")
        print("   BIGQUERY_CREDENTIALS_PATH=/caminho/credentials.json (opcional)")
        print("   BIGQUERY_CONNECTION_ID=projects/.../connections/... (opcional)")
        print("=" * 70)
        raise

ORIGEM, BIGQUERY_CONFIG = carregar_configuracoes()

def testar_conexao_nimbus():
    """Testa conex√£o com NIMBUS."""
    print("=" * 70)
    print("TESTE DE CONEX√ÉO")
    print("=" * 70)
    
    try:
        conn = psycopg2.connect(**ORIGEM)
        cur = conn.cursor()
        cur.execute("SELECT 1;")
        print(f"   ‚úÖ NIMBUS: SUCESSO!")
        print(f"      {ORIGEM['dbname']}@{ORIGEM['host']}:{ORIGEM['port']}")
        cur.close()
        conn.close()
        return True
    except Exception as e:
        print(f"   ‚ùå NIMBUS: FALHA!")
        print(f"      Erro: {e}")
        return False

def query_todos_dados():
    """Retorna query para buscar TODOS os dados dispon√≠veis no banco NIMBUS.
    
    Usa DISTINCT ON para garantir apenas um registro por (dia, estacao_id),
    mantendo o registro com o maior ID (mais recente), que √© exatamente como
    est√° no banco alertadb.
    
    IMPORTANTE: A ordem do ORDER BY deve corresponder √† ordem do DISTINCT ON,
    e depois ordenar por id DESC para pegar o registro mais recente.
    
    Esta √© a MESMA query usada em carregar_pluviometricos_historicos.py e
    sincronizar_pluviometricos_novos.py para garantir consist√™ncia.
    """
    return """
SELECT DISTINCT ON (el."horaLeitura", el.estacao_id)
    el."horaLeitura" AS "Dia",
    elc.m05,
    elc.m10,
    elc.m15,
    elc.h01,
    elc.h04,
    elc.h24,
    elc.h96,
    ee.nome AS "Estacao",
    el.estacao_id
FROM public.estacoes_leitura AS el
JOIN public.estacoes_leiturachuva AS elc
    ON elc.leitura_id = el.id
JOIN public.estacoes_estacao AS ee
    ON ee.id = el.estacao_id
ORDER BY el."horaLeitura" ASC, el.estacao_id ASC, el.id DESC;
"""

def criar_dataset_se_nao_existir(client, dataset_id):
    """Cria dataset no BigQuery se n√£o existir."""
    try:
        dataset_ref = client.dataset(dataset_id)
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "US"  # ou "us-west1" se preferir
        dataset.description = "Dados pluviom√©tricos do NIMBUS"
        
        dataset = client.create_dataset(dataset, exists_ok=True)
        print(f"‚úÖ Dataset '{dataset_id}' criado/verificado no BigQuery!")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar dataset: {e}")
        return False

def criar_tabela_com_schema(client, dataset_id, table_id, schema):
    """Cria tabela no BigQuery com schema se n√£o existir ou atualiza schema se necess√°rio."""
    try:
        table_ref = client.dataset(dataset_id).table(table_id)
        
        # Verificar se a tabela j√° existe
        try:
            table = client.get_table(table_ref)
            # Se existe, verificar se tem schema v√°lido
            if not table.schema or len(table.schema) == 0:
                print(f"   ‚ö†Ô∏è  Tabela existe mas sem schema.")
                # Se tem dados, deletar e recriar (dados ser√£o recarregados com WRITE_TRUNCATE)
                if table.num_rows > 0:
                    print(f"   üìã Tabela tem {table.num_rows:,} registros. Recriando com schema...")
                    client.delete_table(table_ref)
                    # Criar nova tabela com schema
                    table = bigquery.Table(table_ref, schema=schema)
                    table.description = "Dados pluviom√©tricos do NIMBUS (desde 1997)"
                    table = client.create_table(table)
                    print(f"‚úÖ Tabela '{table_id}' recriada com schema!")
                else:
                    # Tabela vazia, apenas atualizar schema
                    print(f"   üìã Atualizando schema da tabela vazia...")
                    table.schema = schema
                    table.description = "Dados pluviom√©tricos do NIMBUS (desde 1997)"
                    table = client.update_table(table, ["schema", "description"])
                    print(f"‚úÖ Schema atualizado na tabela '{table_id}'!")
                return True
            else:
                print(f"‚úÖ Tabela '{table_id}' j√° existe com schema ({len(table.schema)} campos)!")
                return True
        except Exception as e:
            # Tabela n√£o existe, criar
            if "Not found" in str(e) or "404" in str(e) or "does not exist" in str(e).lower():
                print(f"   üìã Criando tabela '{table_id}' com schema...")
            else:
                print(f"   ‚ö†Ô∏è  Erro ao verificar tabela: {e}")
                raise
        
        # Criar tabela com schema
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "Dados pluviom√©tricos do NIMBUS (desde 1997)"
        table = client.create_table(table, exists_ok=False)
        print(f"‚úÖ Tabela '{table_id}' criada com schema no BigQuery!")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar/atualizar tabela: {e}")
        import traceback
        traceback.print_exc()
        return False

def exportar_para_bigquery():
    """Exporta dados do NIMBUS diretamente para BigQuery."""
    engine_nimbus = None
    client_bq = None
    
    timestamp_atual = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    try:
        print("\nüîÑ Iniciando exporta√ß√£o direta NIMBUS ‚Üí BigQuery...")
        print(f"   Origem: alertadb @ NIMBUS")
        print(f"   Destino: BigQuery ({BIGQUERY_CONFIG['project_id']}.{BIGQUERY_CONFIG['dataset_id']}.{BIGQUERY_CONFIG['table_id']})")
        print()
        
        # Conectar ao NIMBUS usando SQLAlchemy (recomendado pelo pandas)
        print("üì¶ Conectando ao NIMBUS...")
        # Criar string de conex√£o PostgreSQL para SQLAlchemy
        connection_string = (
            f"postgresql://{ORIGEM['user']}:{ORIGEM['password']}@"
            f"{ORIGEM['host']}:{ORIGEM['port']}/{ORIGEM['dbname']}"
        )
        engine_nimbus = create_engine(
            connection_string,
            connect_args={'client_encoding': 'UTF8'},
            pool_pre_ping=True  # Verifica conex√£o antes de usar
        )
        
        # Conectar ao BigQuery
        print("üì¶ Conectando ao BigQuery...")
        credentials_path = BIGQUERY_CONFIG.get('credentials_path')
        
        # Se n√£o foi encontrado no carregamento, tentar novamente o caminho padr√£o
        if not credentials_path or not Path(credentials_path).exists():
            credentials_padrao = project_root / 'credentials' / 'credentials.json'
            print(f"   üîç Verificando caminho padr√£o: {credentials_padrao}")
            if credentials_padrao.exists():
                credentials_path = credentials_padrao
                print(f"   ‚úÖ Arquivo encontrado no caminho padr√£o!")
            else:
                print(f"   ‚ö†Ô∏è  Arquivo n√£o encontrado no caminho padr√£o")
        
        if credentials_path and Path(credentials_path).exists():
            print(f"   üîë Usando credenciais: {credentials_path}")
            credentials = service_account.Credentials.from_service_account_file(
                str(credentials_path)  # Garantir que √© string
            )
            client_bq = bigquery.Client(
                project=BIGQUERY_CONFIG['project_id'],
                credentials=credentials
            )
            print("   ‚úÖ Credenciais carregadas com sucesso!")
        elif credentials_path:
            print(f"   ‚ö†Ô∏è  Arquivo de credenciais n√£o encontrado: {credentials_path}")
            print(f"   üîç Tentando caminho padr√£o: {project_root / 'credentials' / 'credentials.json'}")
            raise FileNotFoundError(
                f"‚ùå Arquivo de credenciais n√£o encontrado!\n"
                f"   Procurado em: {credentials_path}\n"
                f"   Caminho padr√£o: {project_root / 'credentials' / 'credentials.json'}\n"
                f"   üí° Coloque o arquivo credentials.json em: {project_root / 'credentials' / 'credentials.json'}"
            )
        else:
            print("   ‚ö†Ô∏è  Nenhum arquivo de credenciais encontrado")
            print(f"   üîç Tentando caminho padr√£o: {project_root / 'credentials' / 'credentials.json'}")
            raise FileNotFoundError(
                f"‚ùå Arquivo de credenciais n√£o encontrado!\n"
                f"   Caminho padr√£o: {project_root / 'credentials' / 'credentials.json'}\n"
                f"   üí° Coloque o arquivo credentials.json em: {project_root / 'credentials' / 'credentials.json'}"
            )
        
        # Criar dataset se n√£o existir
        criar_dataset_se_nao_existir(client_bq, BIGQUERY_CONFIG['dataset_id'])
        
        # Schema do BigQuery
        # Usar FLOAT64 ao inv√©s de NUMERIC porque:
        # - pandas.to_numeric() cria float64 (DOUBLE) no Parquet
        # - BigQuery n√£o converte DOUBLE ‚Üí NUMERIC automaticamente
        # - FLOAT64 tem precis√£o suficiente para dados de precipita√ß√£o (15-17 d√≠gitos)
        schema = [
            bigquery.SchemaField("dia", "TIMESTAMP", mode="REQUIRED"),
            bigquery.SchemaField("dia_original", "STRING", mode="NULLABLE", description="Data/hora original do NIMBUS com timezone (ex: 2009-02-18 00:57:20.000 -0300)"),
            bigquery.SchemaField("m05", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("m10", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("m15", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h01", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h04", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h24", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h96", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("estacao", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("estacao_id", "INTEGER", mode="REQUIRED"),
        ]
        
        # Criar tabela com schema se n√£o existir
        criar_tabela_com_schema(
            client_bq, 
            BIGQUERY_CONFIG['dataset_id'], 
            BIGQUERY_CONFIG['table_id'], 
            schema
        )
        
        # Buscar dados do NIMBUS
        query = query_todos_dados()
        print("üì¶ Buscando dados do NIMBUS...")
        print("   üí° Isso pode levar alguns minutos dependendo do volume de dados...")
        
        inicio_query = datetime.now()
        
        # Ler dados em chunks para n√£o sobrecarregar mem√≥ria
        # Reduzir chunksize para evitar erro de mem√≥ria
        chunksize = 25000  # Reduzido de 100000 para evitar problemas de mem√≥ria
        total_registros = 0
        chunk_numero = 1
        
        table_ref = client_bq.dataset(BIGQUERY_CONFIG['dataset_id']).table(BIGQUERY_CONFIG['table_id'])
        
        # Configurar job de carga
        job_config = bigquery.LoadJobConfig(
            schema=schema,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Substitui dados existentes
            source_format=bigquery.SourceFormat.PARQUET,  # Usar Parquet para melhor performance
        )
        
        print("\nüì¶ Processando e carregando dados no BigQuery...")
        print("   üí° Usando formato Parquet para melhor performance")
        print("   üí° Query usa DISTINCT ON (mesma l√≥gica dos scripts servidor166)")
        print("   üí° Exporta√ß√£o completa (todos os dados desde 1997)\n")
        
        # Criar diret√≥rio tempor√°rio para m√∫ltiplos arquivos Parquet
        # Estrat√©gia: escrever m√∫ltiplos arquivos e carregar todos no BigQuery de uma vez
        # Isso evita problemas de mem√≥ria ao n√£o precisar ler arquivos grandes de volta
        temp_dir = tempfile.mkdtemp()
        parquet_files = []  # Lista de arquivos Parquet criados
        
        # Processar chunks e escrever em arquivos Parquet separados
        import gc  # Para liberar mem√≥ria
        
        chunks_list = []
        batch_size = 4  # Escrever a cada 4 chunks (100k registros total)
        batch_file_num = 1
        
        for chunk_df in pd.read_sql(query, engine_nimbus, chunksize=chunksize):
            print(f"   üì¶ Processando chunk {chunk_numero} ({len(chunk_df):,} registros)...")
            
            # Renomear colunas para corresponder ao schema BigQuery
            # A query retorna "Dia" e "Estacao" (com aspas)
            chunk_df = chunk_df.rename(columns={
                'Dia': 'dia',
                'Estacao': 'estacao'
            })
            
            # Remover coluna TimezoneOffset se existir (n√£o vamos usar)
            if 'TimezoneOffset' in chunk_df.columns:
                chunk_df = chunk_df.drop(columns=['TimezoneOffset'])
            
            # Criar coluna dia_original ANTES de converter para UTC
            # Formato: 2009-02-18 00:57:20.000 -0300 (igual ao banco NIMBUS)
            def formatar_dia_original(dt):
                if pd.isna(dt):
                    return None
                try:
                    # Converter para datetime preservando timezone
                    if isinstance(dt, str):
                        dt_parsed = pd.to_datetime(dt)
                    elif isinstance(dt, pd.Timestamp):
                        dt_parsed = dt
                    else:
                        dt_parsed = pd.to_datetime(dt)
                    
                    # Extrair timezone offset
                    offset_str = "-0300"  # Padr√£o Brasil
                    if isinstance(dt_parsed, pd.Timestamp):
                        if dt_parsed.tz is not None:
                            offset = dt_parsed.tz.utcoffset(dt_parsed)
                            if offset:
                                total_seconds = offset.total_seconds()
                                hours = int(total_seconds // 3600)
                                minutes = int((abs(total_seconds) % 3600) // 60)
                                offset_str = f"{hours:+03d}{minutes:02d}"
                    
                    # Formatar: 2009-02-18 00:57:20.000 -0300
                    timestamp_str = dt_parsed.strftime('%Y-%m-%d %H:%M:%S')
                    if isinstance(dt_parsed, pd.Timestamp) and dt_parsed.microsecond:
                        # Pegar apenas os 3 primeiros d√≠gitos dos microsegundos
                        microsec_str = str(dt_parsed.microsecond)[:3].zfill(3)
                        timestamp_str += f".{microsec_str}"
                    else:
                        timestamp_str += ".000"
                    
                    return f"{timestamp_str} {offset_str}"
                except Exception as e:
                    return None
            
            # Criar dia_original ANTES de converter dia para UTC
            # IMPORTANTE: Preservar o datetime original com timezone antes de converter
            chunk_df['dia_original'] = chunk_df['dia'].apply(formatar_dia_original)
            
            # Converter tipos de data para UTC (padr√£o BigQuery)
            # Converter para UTC mas preservar formato original na coluna dia_original
            try:
                chunk_df['dia'] = pd.to_datetime(chunk_df['dia'], errors='coerce')
                # Se tem timezone, converter para UTC primeiro, depois remover
                if hasattr(chunk_df['dia'].dtype, 'tz') and chunk_df['dia'].dtype.tz is not None:
                    chunk_df['dia'] = chunk_df['dia'].dt.tz_convert('UTC').dt.tz_localize(None)
                else:
                    # Se n√£o tem timezone, converter para UTC
                    chunk_df['dia'] = pd.to_datetime(chunk_df['dia'], utc=True, errors='coerce')
                    # Remover timezone se existir
                    if hasattr(chunk_df['dia'].dtype, 'tz') and chunk_df['dia'].dtype.tz is not None:
                        chunk_df['dia'] = chunk_df['dia'].dt.tz_localize(None)
            except (ValueError, AttributeError):
                # Se falhar, tentar converter diretamente
                chunk_df['dia'] = pd.to_datetime(chunk_df['dia'], utc=True, errors='coerce')
                # Tentar remover timezone se existir
                try:
                    if hasattr(chunk_df['dia'].dtype, 'tz') and chunk_df['dia'].dtype.tz is not None:
                        chunk_df['dia'] = chunk_df['dia'].dt.tz_localize(None)
                except:
                    pass
            
            chunk_df['estacao_id'] = chunk_df['estacao_id'].astype('Int64')
            
            # Garantir que valores num√©ricos sejam do tipo correto
            colunas_numericas = ['m05', 'm10', 'm15', 'h01', 'h04', 'h24', 'h96']
            for col in colunas_numericas:
                chunk_df[col] = pd.to_numeric(chunk_df[col], errors='coerce')
            
            # Filtrar registros com dia NULL (BigQuery n√£o aceita NULL em campo REQUIRED)
            registros_antes = len(chunk_df)
            chunk_df = chunk_df[chunk_df['dia'].notna()]
            registros_depois = len(chunk_df)
            if registros_antes != registros_depois:
                print(f"      ‚ö†Ô∏è  Removidos {registros_antes - registros_depois} registros com dia NULL")
            
            # S√≥ adicionar se ainda tiver registros v√°lidos
            if len(chunk_df) > 0:
                chunks_list.append(chunk_df)
            total_registros += len(chunk_df)
            chunk_numero += 1
            
            # Escrever batch em arquivo Parquet separado quando atingir batch_size
            if len(chunks_list) >= batch_size:
                df_batch = pd.concat(chunks_list, ignore_index=True)
                
                # Converter timestamp para microsegundos (BigQuery espera microsegundos, n√£o nanossegundos)
                # Verificar se tem timezone antes de tentar acessar
                if 'dia' in df_batch.columns and pd.api.types.is_datetime64_any_dtype(df_batch['dia']):
                    # Verificar se tem timezone usando hasattr (mais seguro)
                    if hasattr(df_batch['dia'].dtype, 'tz') and df_batch['dia'].dtype.tz is not None:
                        # Se tem timezone, converter para UTC primeiro, depois remover
                        df_batch['dia'] = df_batch['dia'].dt.tz_convert('UTC').dt.tz_localize(None)
                    # Converter para microsegundos explicitamente
                    df_batch['dia'] = df_batch['dia'].astype('datetime64[us]')
                
                batch_file = Path(temp_dir) / f'pluviometricos_batch_{batch_file_num:04d}.parquet'
                df_batch.to_parquet(
                    batch_file, 
                    index=False, 
                    engine='pyarrow', 
                    compression='snappy',
                    coerce_timestamps='us'  # For√ßar microsegundos
                )
                parquet_files.append(batch_file)
                print(f"      üíæ Batch {batch_file_num} salvo: {batch_file.stat().st_size / (1024*1024):.2f} MB")
                
                # Limpar lista e liberar mem√≥ria
                chunks_list.clear()
                del df_batch
                gc.collect()
                batch_file_num += 1
        
        # Escrever chunks restantes
        if chunks_list:
            df_batch = pd.concat(chunks_list, ignore_index=True)
            
            # Garantir que n√£o h√° valores NULL em dia (campo REQUIRED)
            df_batch = df_batch[df_batch['dia'].notna()]
            
            if len(df_batch) > 0:
                # Converter timestamp para microsegundos (BigQuery espera microsegundos, n√£o nanossegundos)
                # Verificar se tem timezone antes de tentar acessar
                if 'dia' in df_batch.columns and pd.api.types.is_datetime64_any_dtype(df_batch['dia']):
                    # Verificar se tem timezone usando hasattr (mais seguro)
                    if hasattr(df_batch['dia'].dtype, 'tz') and df_batch['dia'].dtype.tz is not None:
                        # Se tem timezone, converter para UTC primeiro, depois remover
                        df_batch['dia'] = df_batch['dia'].dt.tz_convert('UTC').dt.tz_localize(None)
                    # Converter para microsegundos explicitamente
                    df_batch['dia'] = df_batch['dia'].astype('datetime64[us]')
                
                batch_file = Path(temp_dir) / f'pluviometricos_batch_{batch_file_num:04d}.parquet'
                df_batch.to_parquet(
                    batch_file, 
                    index=False, 
                    engine='pyarrow', 
                    compression='snappy',
                    coerce_timestamps='us'  # For√ßar microsegundos
                )
                parquet_files.append(batch_file)
                print(f"      üíæ Batch {batch_file_num} salvo: {batch_file.stat().st_size / (1024*1024):.2f} MB")
                del df_batch
                gc.collect()
        
        if total_registros == 0:
            print("   ‚ö†Ô∏è  Nenhum dado encontrado!")
            return 0
        
        total_size = sum(f.stat().st_size for f in parquet_files) / (1024*1024)
        print(f"\n   ‚úÖ {len(parquet_files)} arquivos Parquet criados: {total_size:.2f} MB total")
        
        tempo_query = (datetime.now() - inicio_query).total_seconds()
        print(f"\n   ‚úÖ Dados processados: {total_registros:,} registros em {tempo_query:.1f} segundos")
        
        # Carregar m√∫ltiplos arquivos Parquet no BigQuery
        print(f"\nüì§ Carregando {len(parquet_files)} arquivos Parquet no BigQuery...")
        print(f"   Tabela: {BIGQUERY_CONFIG['project_id']}.{BIGQUERY_CONFIG['dataset_id']}.{BIGQUERY_CONFIG['table_id']}")
        
        inicio_carga = datetime.now()
        
        # Carregar cada arquivo Parquet no BigQuery
        # Estrat√©gia: carregar um por vez com WRITE_APPEND ap√≥s o primeiro (WRITE_TRUNCATE)
        # Isso evita problemas de mem√≥ria ao n√£o precisar concatenar arquivos grandes
        
        for i, parquet_file in enumerate(parquet_files, 1):
            print(f"   üì§ Carregando arquivo {i}/{len(parquet_files)}: {parquet_file.name}...")
            
            # Criar novo job_config para cada arquivo
            # Para o primeiro arquivo, usar WRITE_TRUNCATE (substitui dados existentes)
            # Para os demais, usar WRITE_APPEND (adiciona aos dados existentes)
            file_job_config = bigquery.LoadJobConfig(
                schema=schema,
                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE if i == 1 else bigquery.WriteDisposition.WRITE_APPEND,
                source_format=bigquery.SourceFormat.PARQUET,
            )
            
            with open(parquet_file, 'rb') as source_file:
                job = client_bq.load_table_from_file(
                    source_file,
                    table_ref,
                    job_config=file_job_config
                )
                # Aguardar conclus√£o antes de carregar o pr√≥ximo (evita sobrecarga)
                job.result()
                print(f"      ‚úÖ Arquivo {i}/{len(parquet_files)} carregado com sucesso")
        
        tempo_carga = (datetime.now() - inicio_carga).total_seconds()
        
        # Limpar arquivos tempor√°rios
        print(f"   üßπ Limpando arquivos tempor√°rios...")
        for parquet_file in parquet_files:
            try:
                parquet_file.unlink()
            except Exception as e:
                print(f"      ‚ö†Ô∏è  Erro ao deletar {parquet_file.name}: {e}")
        try:
            os.rmdir(temp_dir)
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Erro ao deletar diret√≥rio tempor√°rio: {e}")
        
        # Estat√≠sticas finais
        table = client_bq.get_table(table_ref)
        
        print("\n" + "=" * 70)
        print("‚úÖ EXPORTA√á√ÉO PARA BIGQUERY CONCLU√çDA!")
        print("=" * 70)
        print(f"üìä Total de registros: {total_registros:,}")
        print(f"üìä Registros no BigQuery: {table.num_rows:,}")
        print(f"üìä Tamanho da tabela: {table.num_bytes / (1024*1024):.2f} MB")
        print(f"‚è±Ô∏è  Tempo de processamento: {tempo_query:.1f} segundos")
        print(f"‚è±Ô∏è  Tempo de carga no BigQuery: {tempo_carga:.1f} segundos")
        print(f"‚è∞ Conclu√≠do em: {timestamp_atual}")
        print("=" * 70)
        
        print(f"\nüìä Tabela dispon√≠vel em:")
        print(f"   {BIGQUERY_CONFIG['project_id']}.{BIGQUERY_CONFIG['dataset_id']}.{BIGQUERY_CONFIG['table_id']}")
        print(f"\nüí° Voc√™ pode consultar no BigQuery Console:")
        print(f"   https://console.cloud.google.com/bigquery?project={BIGQUERY_CONFIG['project_id']}")
        
        return total_registros

    except Exception as e:
        print(f'\n‚ùå Erro na exporta√ß√£o: {e}')
        import traceback
        traceback.print_exc()
        return 0

    finally:
        if engine_nimbus:
            engine_nimbus.dispose()  # Fecha todas as conex√µes do pool SQLAlchemy

def main():
    """Fun√ß√£o principal."""
    print("=" * 70)
    print("üåßÔ∏è EXPORTA√á√ÉO DIRETA - NIMBUS ‚Üí BigQuery")
    print("=" * 70)
    print()
    print("üéØ PROP√ìSITO:")
    print("   Exportar TODOS os dados diretamente do NIMBUS")
    print("   para o BigQuery (pulando todas as camadas intermedi√°rias)")
    print()
    print("üìã O QUE SER√Å FEITO:")
    print("   ‚úÖ Buscar TODOS os dados do NIMBUS (desde 1997)")
    print("   ‚úÖ Usar DISTINCT ON (mesma l√≥gica dos scripts servidor166)")
    print("   ‚úÖ Criar dataset/tabela no BigQuery se n√£o existir")
    print("   ‚úÖ Exportar em formato Parquet completo (n√£o dividido por ano)")
    print("   ‚úÖ Carregar no BigQuery automaticamente")
    print()
    print("‚ö†Ô∏è  IMPORTANTE:")
    print("   - Requer credenciais do GCP configuradas")
    print("   - Formato: Parquet (mais eficiente para BigQuery)")
    print("   - Exporta√ß√£o completa: todos os dados em um √∫nico arquivo")
    print("   - Query usa DISTINCT ON para garantir unicidade")
    print("=" * 70)
    
    # Testar conex√£o
    if not testar_conexao_nimbus():
        print("\n‚ùå N√£o foi poss√≠vel conectar ao NIMBUS. Verifique as configura√ß√µes.")
        return
    
    # Exportar
    exportar_para_bigquery()
    
    print("\nüí° PR√ìXIMO PASSO:")
    print("   Configure exporta√ß√£o peri√≥dica ou use este script")
    print("   quando precisar atualizar os dados no BigQuery.\n")

if __name__ == '__main__':
    main()

