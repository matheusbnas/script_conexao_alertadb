#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üåßÔ∏è EXPORTA√á√ÉO - Servidor 166 ‚Üí BigQuery

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéØ PROP√ìSITO DESTE SCRIPT:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Este script exporta dados do banco alertadb_cor (servidor 166) para o BigQuery.
Permite controle total dos dados j√° que voc√™ tem acesso como administrador ao banco.

ARQUITETURA:
    Servidor 166 (alertadb_cor) ‚Üí Parquet ‚Üí BigQuery
              ‚Üë [ESTE SCRIPT - COM CONTROLE ADMINISTRATIVO]

VANTAGENS:
    ‚úÖ Controle total dos dados (voc√™ √© admin do banco)
    ‚úÖ Pode fazer tratamentos antes de exportar
    ‚úÖ Dados j√° validados e tratados no servidor 166
    ‚úÖ BigQuery otimizado para an√°lises
    ‚úÖ Formato Parquet (5-10x mais r√°pido que CSV)
    ‚úÖ Coluna dia como TIMESTAMP (timestamptz NOT NULL no banco original da NIMBUS)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã O QUE ESTE SCRIPT FAZ:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ Conecta ao banco alertadb_cor (servidor 166)
‚úÖ Busca TODOS os dados da tabela pluviometricos
‚úÖ Exporta para formato Parquet completo
‚úÖ Carrega no BigQuery automaticamente
‚úÖ Cria/atualiza tabela no BigQuery
‚úÖ Processa em lotes para otimizar mem√≥ria
‚úÖ Preserva formato original da coluna dia (TIMESTAMP - timestamptz NOT NULL)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã CONFIGURA√á√ÉO:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Vari√°veis obrigat√≥rias no .env:
- DB_DESTINO_HOST, DB_DESTINO_NAME, DB_DESTINO_USER, DB_DESTINO_PASSWORD
- BIGQUERY_PROJECT_ID

Vari√°veis opcionais:
- BIGQUERY_DATASET_ID_SERVIDOR166 (padr√£o: alertadb_166_raw) - Dataset para dados servidor166 ‚Üí BigQuery
- BIGQUERY_TABLE_ID (padr√£o: pluviometricos)
- BIGQUERY_CREDENTIALS_PATH (opcional: caminho para credentials.json)
"""

import psycopg2
import pandas as pd
from sqlalchemy import create_engine
from sqlalchemy.exc import OperationalError as SQLAlchemyOperationalError
from google.cloud import bigquery
from google.oauth2 import service_account
import os
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path
import tempfile
import gc
from urllib.parse import quote_plus

# Carregar vari√°veis de ambiente
project_root = Path(__file__).parent.parent.parent
load_dotenv(dotenv_path=project_root / '.env')

def obter_variavel(nome, obrigatoria=True, padrao=None):
    """Obt√©m vari√°vel de ambiente."""
    valor = os.getenv(nome)
    if not valor or (isinstance(valor, str) and valor.strip() == ''):
        if obrigatoria:
            raise ValueError(f"‚ùå Vari√°vel obrigat√≥ria n√£o encontrada: {nome}")
        return padrao
    return valor.strip() if isinstance(valor, str) else valor

def carregar_configuracoes():
    """Carrega configura√ß√µes do .env."""
    try:
        # Banco ORIGEM - Servidor 166 (alertadb_cor)
        origem = {
            'host': obter_variavel('DB_DESTINO_HOST'),
            'port': obter_variavel('DB_DESTINO_PORT', obrigatoria=False, padrao='5432'),
            'dbname': obter_variavel('DB_DESTINO_NAME'),
            'user': obter_variavel('DB_DESTINO_USER'),
            'password': obter_variavel('DB_DESTINO_PASSWORD'),
            'sslmode': obter_variavel('DB_DESTINO_SSLMODE', obrigatoria=False, padrao='disable'),
            'connect_timeout': 10
        }

        # BigQuery
        # Dataset espec√≠fico para dados do servidor166 (NIMBUS ‚Üí servidor166 ‚Üí BigQuery)
        bigquery_config = {
            'project_id': obter_variavel('BIGQUERY_PROJECT_ID'),
            'dataset_id': obter_variavel('BIGQUERY_DATASET_ID_SERVIDOR166', obrigatoria=False, padrao='alertadb_166_raw'),
            'table_id': obter_variavel('BIGQUERY_TABLE_ID', obrigatoria=False, padrao='pluviometricos'),
            'credentials_path': obter_variavel('BIGQUERY_CREDENTIALS_PATH', obrigatoria=False),
        }

        return origem, bigquery_config
    except Exception as e:
        print(f"‚ùå Erro ao carregar configura√ß√µes: {e}")
        raise

def obter_credenciais_bigquery(credentials_path=None):
    """Obt√©m credenciais do BigQuery."""
    if credentials_path and os.path.exists(credentials_path):
        return service_account.Credentials.from_service_account_file(credentials_path)
    
    # Tentar encontrar credentials.json na pasta credentials/
    credentials_file = project_root / 'credentials' / 'credentials.json'
    if credentials_file.exists():
        return service_account.Credentials.from_service_account_file(str(credentials_file))
    
    # Tentar usar credenciais padr√£o do ambiente
    try:
        return None  # BigQuery usar√° credenciais padr√£o do ambiente
    except Exception:
        raise ValueError("‚ùå Credenciais do BigQuery n√£o encontradas. Configure BIGQUERY_CREDENTIALS_PATH ou coloque credentials.json em credentials/")

def testar_conexao_servidor166(origem):
    """Testa conex√£o com banco servidor 166."""
    try:
        # Codificar usu√°rio e senha para URL (trata caracteres especiais)
        user_encoded = quote_plus(origem['user'])
        password_encoded = quote_plus(origem['password'])
        
        engine = create_engine(
            f"postgresql://{user_encoded}:{password_encoded}@{origem['host']}:{origem['port']}/{origem['dbname']}",
            connect_args={'sslmode': origem['sslmode'], 'connect_timeout': origem['connect_timeout']}
        )
        with engine.connect() as conn:
            from sqlalchemy import text
            conn.execute(text("SELECT 1"))
        return True
    except Exception as e:
        print(f"‚ùå Erro ao conectar ao servidor 166: {e}")
        return False

def query_todos_dados():
    """Retorna query para buscar TODOS os dados do servidor 166.
    
    A coluna 'dia' √© TIMESTAMPTZ NOT NULL no servidor 166, preservando o timezone original da NIMBUS.
    O pandas/SQLAlchemy preserva automaticamente o timezone ao ler TIMESTAMPTZ.
    """
    return """
SELECT 
    dia,  -- TIMESTAMPTZ NOT NULL (preserva timezone original)
    m05,
    m10,
    m15,
    h01,
    h04,
    h24,
    h96,
    estacao,
    estacao_id
FROM pluviometricos
ORDER BY dia ASC, estacao_id ASC;
"""

def criar_dataset_se_nao_existir(client, dataset_id):
    """Cria dataset no BigQuery se n√£o existir."""
    try:
        dataset_ref = client.dataset(dataset_id)
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "US"  # ou "us-west1" se preferir
        dataset.description = "Dataset para dados do servidor166 (NIMBUS ‚Üí servidor166 ‚Üí BigQuery)"
        
        dataset = client.create_dataset(dataset, exists_ok=True)
        print(f"‚úÖ Dataset '{dataset_id}' criado/verificado no BigQuery!")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar dataset: {e}")
        return False

def criar_tabela_com_schema(client, dataset_id, table_id, schema):
    """Cria tabela no BigQuery com schema e particionamento por data se n√£o existir ou atualiza schema se necess√°rio."""
    try:
        table_ref = client.dataset(dataset_id).table(table_id)
        
        # Verificar se a tabela j√° existe
        try:
            table = client.get_table(table_ref)
            # Se existe, verificar se tem schema v√°lido
            if not table.schema or len(table.schema) == 0:
                print(f"   ‚ö†Ô∏è  Tabela existe mas sem schema.")
                # Se tem dados, deletar e recriar (dados ser√£o recarregados com WRITE_TRUNCATE)
                if table.num_rows > 0:
                    print(f"   üìã Tabela tem {table.num_rows:,} registros. Recriando com schema e particionamento...")
                    client.delete_table(table_ref)
                    # Criar nova tabela com schema e particionamento
                    table = bigquery.Table(table_ref, schema=schema)
                    table.description = "Dados pluviom√©tricos do servidor 166"
                    # Particionar por coluna dia (TIMESTAMP)
                    table.time_partitioning = bigquery.TimePartitioning(
                        type_=bigquery.TimePartitioningType.DAY,
                        field="dia"  # Particionamento por coluna dia (TIMESTAMP)
                    )
                    table = client.create_table(table)
                    print(f"‚úÖ Tabela '{table_id}' recriada com schema e particionamento por coluna 'dia'!")
                else:
                    # Tabela vazia, recriar com schema e particionamento
                    print(f"   üìã Tabela vazia sem schema. Recriando com schema e particionamento...")
                    client.delete_table(table_ref)
                    # Criar nova tabela com schema e particionamento
                    table = bigquery.Table(table_ref, schema=schema)
                    table.description = "Dados pluviom√©tricos do servidor 166"
                    # Particionar por coluna dia (TIMESTAMP)
                    table.time_partitioning = bigquery.TimePartitioning(
                        type_=bigquery.TimePartitioningType.DAY,
                        field="dia"  # Particionamento por coluna dia (TIMESTAMP)
                    )
                    table = client.create_table(table)
                    print(f"‚úÖ Tabela '{table_id}' recriada com schema e particionamento por coluna 'dia'!")
                return True
            else:
                # Verificar se j√° tem particionamento
                if table.time_partitioning and table.time_partitioning.field:
                    if table.time_partitioning.field != "dia":
                        print(f"   ‚ö†Ô∏è  Tabela '{table_id}' existe com particionamento por campo '{table.time_partitioning.field}'.")
                        print(f"   üí° Precisamos recriar a tabela com particionamento por 'dia'.")
                        print(f"   üîÑ Deletando tabela para recriar com particionamento correto...")
                        client.delete_table(table_ref)
                        # Criar nova tabela com particionamento por coluna dia
                        table = bigquery.Table(table_ref, schema=schema)
                        table.description = "Dados pluviom√©tricos do servidor 166"
                        table.time_partitioning = bigquery.TimePartitioning(
                            type_=bigquery.TimePartitioningType.DAY,
                            field="dia"  # Particionamento por coluna dia (TIMESTAMP)
                        )
                        table = client.create_table(table)
                        print(f"‚úÖ Tabela '{table_id}' recriada com particionamento por coluna 'dia'!")
                    else:
                        print(f"‚úÖ Tabela '{table_id}' j√° existe com schema ({len(table.schema)} campos) e particionamento por coluna 'dia'!")
                elif not table.time_partitioning:
                    # BigQuery n√£o permite converter tabela n√£o particionada em particionada
                    # Se a tabela est√° vazia, podemos deletar e recriar com particionamento
                    if table.num_rows == 0:
                        print(f"   üìã Tabela existe mas sem particionamento e est√° vazia.")
                        print(f"   üîÑ Recriando tabela com particionamento por coluna 'dia'...")
                        client.delete_table(table_ref)
                        # Criar nova tabela com particionamento por coluna dia
                        table = bigquery.Table(table_ref, schema=schema)
                        table.description = "Dados pluviom√©tricos do servidor 166"
                        table.time_partitioning = bigquery.TimePartitioning(
                            type_=bigquery.TimePartitioningType.DAY,
                            field="dia"  # Particionamento por coluna dia (TIMESTAMP)
                        )
                        table = client.create_table(table)
                        print(f"‚úÖ Tabela '{table_id}' recriada com particionamento por coluna 'dia'!")
                    else:
                        # Tabela tem dados, n√£o podemos converter
                        print(f"   ‚ö†Ô∏è  Tabela '{table_id}' existe mas SEM particionamento e tem {table.num_rows:,} registros.")
                        print(f"   üí° BigQuery n√£o permite converter tabela n√£o particionada em particionada.")
                        print(f"   üìã Continuando sem particionamento (dados ser√£o substitu√≠dos com WRITE_TRUNCATE).")
                        print(f"   üí° Para ter particionamento, delete a tabela manualmente e execute o script novamente.")
                return True
        except Exception as e:
            # Tabela n√£o existe, criar
            if "Not found" in str(e) or "404" in str(e) or "does not exist" in str(e).lower():
                print(f"   üìã Criando tabela '{table_id}' com schema e particionamento por coluna 'dia'...")
            else:
                print(f"   ‚ö†Ô∏è  Erro ao verificar tabela: {e}")
                raise
        
        # Criar tabela com schema e particionamento
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "Dados pluviom√©tricos do servidor 166"
        # Particionar por coluna dia (TIMESTAMP)
        table.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="dia"  # Particionamento por coluna dia (TIMESTAMP)
        )
        table = client.create_table(table, exists_ok=False)
        print(f"‚úÖ Tabela '{table_id}' criada com schema e particionamento por coluna 'dia' no BigQuery!")
        print(f"   üí° Particionamento melhora performance de queries e reduz custos")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar/atualizar tabela: {e}")
        import traceback
        traceback.print_exc()
        return False

def processar_dia_timestamp(dt):
    """Processa TIMESTAMPTZ do PostgreSQL para TIMESTAMP do BigQuery (UTC).
    
    A coluna dia no servidor 166 √© TIMESTAMPTZ NOT NULL (preserva timezone original da NIMBUS).
    O BigQuery armazena TIMESTAMP em UTC internamente, ent√£o convertemos preservando o valor correto.
    
    IMPORTANTE: Preserva o timezone original do TIMESTAMPTZ antes de converter para UTC.
    """
    if pd.isna(dt):
        return None
    try:
        # Converter para pandas Timestamp se necess√°rio
        if isinstance(dt, str):
            # Tentar parsear preservando timezone se presente na string
            dt_parsed = pd.to_datetime(dt)
        elif isinstance(dt, pd.Timestamp):
            dt_parsed = dt
        elif hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
            # Se j√° √© datetime com timezone (TIMESTAMPTZ do PostgreSQL)
            dt_parsed = pd.Timestamp(dt)
        else:
            dt_parsed = pd.to_datetime(dt)
        
        # Se j√° tem timezone (TIMESTAMPTZ do PostgreSQL), converter para UTC preservando o valor
        if isinstance(dt_parsed, pd.Timestamp) and dt_parsed.tz is not None:
            # Converter para UTC mantendo o valor absoluto correto
            dt_utc = dt_parsed.tz_convert('UTC')
            # Remover timezone info para BigQuery (ele armazena como UTC internamente)
            return dt_utc.tz_localize(None)
        elif isinstance(dt_parsed, pd.Timestamp):
            # Se n√£o tem timezone, pode ser que o PostgreSQL retornou sem timezone
            # Neste caso, assumir que j√° est√° no timezone do servidor (Brasil -03:00)
            # e converter para UTC
            from datetime import timezone, timedelta
            tz_brasil = timezone(timedelta(hours=-3))
            dt_com_tz = dt_parsed.tz_localize(tz_brasil)
            dt_utc = dt_com_tz.tz_convert('UTC')
            return dt_utc.tz_localize(None)
        else:
            return dt_parsed
    except Exception as e:
        print(f"      ‚ö†Ô∏è  Erro ao processar timestamp: {e}")
        return None

def exportar_para_bigquery():
    """Exporta dados do servidor 166 diretamente para BigQuery."""
    engine_servidor166 = None
    client_bq = None
    
    timestamp_atual = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    try:
        print("\nüîÑ Iniciando exporta√ß√£o Servidor 166 ‚Üí BigQuery...")
        print(f"   Origem: alertadb_cor @ servidor 166")
        print(f"   Destino: BigQuery ({BIGQUERY_CONFIG['project_id']}.{BIGQUERY_CONFIG['dataset_id']}.{BIGQUERY_CONFIG['table_id']})")
        print()
        
        # Conectar ao servidor 166 usando SQLAlchemy
        print("üì¶ Conectando ao servidor 166...")
        # Codificar usu√°rio e senha para URL (trata caracteres especiais)
        user_encoded = quote_plus(ORIGEM['user'])
        password_encoded = quote_plus(ORIGEM['password'])
        
        connection_string = (
            f"postgresql://{user_encoded}:{password_encoded}@"
            f"{ORIGEM['host']}:{ORIGEM['port']}/{ORIGEM['dbname']}"
        )
        engine_servidor166 = create_engine(
            connection_string,
            connect_args={
                'client_encoding': 'UTF8',
                'connect_timeout': 30,
                'keepalives': 1,
                'keepalives_idle': 30,
                'keepalives_interval': 10,
                'keepalives_count': 5,
                'options': '-c statement_timeout=0'  # Desabilitar timeout de statement
            },
            pool_pre_ping=True,
            pool_recycle=3600,  # Reciclar conex√µes ap√≥s 1 hora
            pool_size=5,
            max_overflow=10,
            echo=False
        )
        
        # Conectar ao BigQuery
        print("üì¶ Conectando ao BigQuery...")
        credentials_path = BIGQUERY_CONFIG.get('credentials_path')
        
        if not credentials_path or not Path(credentials_path).exists():
            credentials_padrao = project_root / 'credentials' / 'credentials.json'
            if credentials_padrao.exists():
                credentials_path = credentials_padrao
        
        if credentials_path and Path(credentials_path).exists():
            print(f"   üîë Usando credenciais: {credentials_path}")
            credentials = service_account.Credentials.from_service_account_file(str(credentials_path))
            client_bq = bigquery.Client(
                project=BIGQUERY_CONFIG['project_id'],
                credentials=credentials
            )
        else:
            client_bq = bigquery.Client(project=BIGQUERY_CONFIG['project_id'])
        
        # Criar dataset se n√£o existir
        criar_dataset_se_nao_existir(client_bq, BIGQUERY_CONFIG['dataset_id'])
        
        # Schema do BigQuery
        # Coluna dia como TIMESTAMP (armazena em UTC, vem de TIMESTAMPTZ NOT NULL do servidor 166)
        # O servidor 166 preserva o timezone original da NIMBUS como TIMESTAMPTZ
        schema = [
            bigquery.SchemaField("dia", "TIMESTAMP", mode="REQUIRED", description="Data e hora em que foi realizada a medi√ß√£o. Origem: TIMESTAMPTZ NOT NULL do servidor 166 (preserva timezone original da NIMBUS). Armazenado em UTC no BigQuery."),
            bigquery.SchemaField("m05", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("m10", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("m15", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h01", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h04", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h24", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h96", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("estacao", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("estacao_id", "INTEGER", mode="REQUIRED"),
        ]
        
        # Criar tabela com schema se n√£o existir
        criar_tabela_com_schema(
            client_bq, 
            BIGQUERY_CONFIG['dataset_id'], 
            BIGQUERY_CONFIG['table_id'], 
            schema
        )
        
        # Buscar dados do servidor 166
        print("\nüì¶ Buscando dados do servidor 166...")
        query = query_todos_dados()
        
        # Processar em chunks menores para evitar problemas de mem√≥ria
        chunksize = 10000  # Reduzido de 25000 para evitar erro de mem√≥ria
        batch_size = 4  # Escrever a cada 4 chunks
        chunks_list = []
        total_registros = 0
        parquet_files = []
        batch_file_num = 1
        chunk_numero = 0
        
        temp_dir = Path(tempfile.gettempdir()) / 'bigquery_export'
        temp_dir.mkdir(exist_ok=True)
        
        print(f"   üìä Processando em chunks de {chunksize:,} registros...")
        print(f"   üí° Usando chunks menores para evitar problemas de mem√≥ria")
        print(f"   üîÑ Configura√ß√µes de conex√£o otimizadas para queries longas")
        
        # Fun√ß√£o auxiliar para ler chunk com retry
        def ler_chunks_com_retry(query, engine_ref, chunksize, max_retries=3):
            """L√™ chunks com retry autom√°tico em caso de erro de conex√£o."""
            tentativa_global = 0
            
            while tentativa_global < max_retries:
                try:
                    # Tentar criar iterator de chunks
                    chunk_iterator = pd.read_sql(query, engine_ref['engine'], chunksize=chunksize)
                    
                    # Processar cada chunk
                    for chunk_df in chunk_iterator:
                        yield chunk_df
                    
                    # Se chegou aqui, leitura completa com sucesso
                    return
                    
                except (psycopg2.OperationalError, psycopg2.InterfaceError,
                        SQLAlchemyOperationalError) as e:
                    tentativa_global += 1
                    if tentativa_global < max_retries:
                        wait_time = tentativa_global * 5
                        print(f"      ‚ö†Ô∏è  Erro de conex√£o (tentativa {tentativa_global}/{max_retries}): {str(e)[:100]}")
                        print(f"      üîÑ Reconectando em {wait_time}s...")
                        import time
                        time.sleep(wait_time)
                        # For√ßar reciclagem do pool de conex√µes
                        try:
                            engine_ref['engine'].dispose()
                        except:
                            pass
                        # Recriar engine
                        print(f"      üîÑ Recriando conex√£o...")
                        user_encoded = quote_plus(ORIGEM['user'])
                        password_encoded = quote_plus(ORIGEM['password'])
                        connection_string = (
                            f"postgresql://{user_encoded}:{password_encoded}@"
                            f"{ORIGEM['host']}:{ORIGEM['port']}/{ORIGEM['dbname']}"
                        )
                        engine_ref['engine'] = create_engine(
                            connection_string,
                            connect_args={
                                'client_encoding': 'UTF8',
                                'connect_timeout': 30,
                                'keepalives': 1,
                                'keepalives_idle': 30,
                                'keepalives_interval': 10,
                                'keepalives_count': 5,
                                'options': '-c statement_timeout=0'
                            },
                            pool_pre_ping=True,
                            pool_recycle=3600,
                            pool_size=5,
                            max_overflow=10
                        )
                    else:
                        print(f"      ‚ùå Falha ap√≥s {max_retries} tentativas")
                        raise
        
        # Usar dicion√°rio mut√°vel para permitir atualiza√ß√£o do engine
        engine_ref = {'engine': engine_servidor166}
        
        # Ler chunks com retry autom√°tico
        for chunk_df in ler_chunks_com_retry(query, engine_ref, chunksize):
            chunk_numero += 1
            
            if chunk_df.empty:
                continue
            
            # Processar coluna dia: TIMESTAMPTZ do servidor 166 ‚Üí TIMESTAMP (UTC) do BigQuery
            # O servidor 166 tem TIMESTAMPTZ NOT NULL que preserva o timezone original da NIMBUS
            # O BigQuery armazena TIMESTAMP em UTC, ent√£o convertemos preservando o valor correto
            chunk_df['dia'] = chunk_df['dia'].apply(processar_dia_timestamp)
            
            # Converter tipos
            chunk_df['estacao_id'] = chunk_df['estacao_id'].astype('Int64')
            
            # Converter colunas num√©ricas
            colunas_numericas = ['m05', 'm10', 'm15', 'h01', 'h04', 'h24', 'h96']
            for col in colunas_numericas:
                chunk_df[col] = pd.to_numeric(chunk_df[col], errors='coerce')
            
            # Filtrar registros com dia NULL
            registros_antes = len(chunk_df)
            chunk_df = chunk_df[chunk_df['dia'].notna()]
            registros_depois = len(chunk_df)
            if registros_antes != registros_depois:
                print(f"      ‚ö†Ô∏è  Removidos {registros_antes - registros_depois} registros com dia NULL")
            
            # S√≥ adicionar se ainda tiver registros v√°lidos
            if len(chunk_df) > 0:
                chunks_list.append(chunk_df.copy())  # Usar copy() para evitar refer√™ncias
            total_registros += len(chunk_df)
            
            # Limpar mem√≥ria ap√≥s processar cada chunk
            del chunk_df
            gc.collect()
            
            # Escrever batch em arquivo Parquet separado quando atingir batch_size
            if len(chunks_list) >= batch_size:
                df_batch = pd.concat(chunks_list, ignore_index=True)
                
                # Garantir que coluna dia est√° como datetime64[us] (microsegundos) para BigQuery
                if 'dia' in df_batch.columns:
                    if not pd.api.types.is_datetime64_any_dtype(df_batch['dia']):
                        # Se n√£o for datetime, tentar converter
                        df_batch['dia'] = pd.to_datetime(df_batch['dia'], errors='coerce')
                    # Converter para microsegundos (precis√£o do BigQuery TIMESTAMP)
                    df_batch['dia'] = df_batch['dia'].astype('datetime64[us]')
                
                batch_file = Path(temp_dir) / f'pluviometricos_batch_{batch_file_num:04d}.parquet'
                df_batch.to_parquet(
                    batch_file, 
                    index=False, 
                    engine='pyarrow', 
                    compression='snappy',
                    coerce_timestamps='us'  # For√ßar microsegundos para TIMESTAMP
                )
                parquet_files.append(batch_file)
                print(f"      üíæ Batch {batch_file_num} salvo: {batch_file.stat().st_size / (1024*1024):.2f} MB")
                
                # Limpar lista e liberar mem√≥ria
                chunks_list.clear()
                del df_batch
                gc.collect()
                batch_file_num += 1
        
        # Escrever chunks restantes
        if chunks_list:
            df_batch = pd.concat(chunks_list, ignore_index=True)
            df_batch = df_batch[df_batch['dia'].notna()]
            
            if len(df_batch) > 0:
                # Garantir que coluna dia est√° como datetime64[us] (microsegundos) para BigQuery
                if 'dia' in df_batch.columns:
                    if not pd.api.types.is_datetime64_any_dtype(df_batch['dia']):
                        # Se n√£o for datetime, tentar converter
                        df_batch['dia'] = pd.to_datetime(df_batch['dia'], errors='coerce')
                    # Converter para microsegundos (precis√£o do BigQuery TIMESTAMP)
                    df_batch['dia'] = df_batch['dia'].astype('datetime64[us]')
                
                batch_file = Path(temp_dir) / f'pluviometricos_batch_{batch_file_num:04d}.parquet'
                df_batch.to_parquet(
                    batch_file, 
                    index=False, 
                    engine='pyarrow', 
                    compression='snappy',
                    coerce_timestamps='us'  # For√ßar microsegundos para TIMESTAMP
                )
                parquet_files.append(batch_file)
                print(f"      üíæ Batch {batch_file_num} salvo: {batch_file.stat().st_size / (1024*1024):.2f} MB")
                del df_batch
                gc.collect()
        
        if total_registros == 0:
            print("   ‚ö†Ô∏è  Nenhum dado encontrado!")
            return 0
        
        # Carregar arquivos Parquet no BigQuery
        print(f"\nüì§ Carregando {total_registros:,} registros no BigQuery...")
        table_ref = f"{BIGQUERY_CONFIG['project_id']}.{BIGQUERY_CONFIG['dataset_id']}.{BIGQUERY_CONFIG['table_id']}"
        
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.PARQUET,
            schema=schema,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Substitui dados existentes
        )
        
        # Carregar cada arquivo Parquet
        for i, parquet_file in enumerate(parquet_files, 1):
            print(f"   üì§ Carregando arquivo {i}/{len(parquet_files)}...")
            with open(parquet_file, 'rb') as source_file:
                job = client_bq.load_table_from_file(
                    source_file,
                    table_ref,
                    job_config=job_config
                )
                job.result()  # Aguarda conclus√£o
            
            # Remover arquivo tempor√°rio
            os.unlink(parquet_file)
        
        print(f"\n‚úÖ Exporta√ß√£o conclu√≠da!")
        print(f"   üìä Total exportado: {total_registros:,} registros")
        print(f"   üïê Conclu√≠do em: {timestamp_atual}")
        
        return total_registros
        
    except Exception as e:
        print(f"\n‚ùå Erro durante exporta√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return 0
    
    finally:
        # Limpar conex√µes
        if 'engine_ref' in locals() and engine_ref.get('engine'):
            try:
                engine_ref['engine'].dispose()
            except:
                pass
        elif 'engine_servidor166' in locals() and engine_servidor166:
            try:
                engine_servidor166.dispose()
            except:
                pass

def main():
    """Fun√ß√£o principal."""
    global ORIGEM, BIGQUERY_CONFIG
    
    try:
        print("=" * 80)
        print("üåßÔ∏è EXPORTA√á√ÉO - Servidor 166 ‚Üí BigQuery")
        print("=" * 80)
        print("üéØ PROP√ìSITO: Exportar dados do alertadb_cor (servidor 166) para BigQuery")
        print("üìã O QUE SER√Å FEITO:")
        print("   ‚úÖ Buscar todos os dados do servidor 166")
        print("   ‚úÖ Criar tabela no BigQuery se n√£o existir")
        print("   ‚úÖ Processar em lotes de 25.000 registros")
        print("   ‚úÖ Preservar formato original da coluna dia (STRING)")
        print("=" * 80)
        
        ORIGEM, BIGQUERY_CONFIG = carregar_configuracoes()
        
        # Testar conex√µes
        print("\n" + "=" * 80)
        print("TESTE DE CONEX√ïES")
        print("=" * 80)
        
        print("\nüîç Testando conex√£o com servidor 166...")
        if not testar_conexao_servidor166(ORIGEM):
            print("‚ùå Falha na conex√£o com servidor 166!")
            return
        
        print("‚úÖ ORIGEM (Servidor 166): SUCESSO!")
        print(f"   {ORIGEM['dbname']}@{ORIGEM['host']}:{ORIGEM['port']}")
        
        # Exportar dados
        total = exportar_para_bigquery()
        
        if total > 0:
            print("\n" + "=" * 80)
            print("‚úÖ EXPORTA√á√ÉO CONCLU√çDA COM SUCESSO!")
            print("=" * 80)
            print(f"üìä Total de registros exportados: {total:,}")
        else:
            print("\n‚ö†Ô∏è  Nenhum dado foi exportado.")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrompido pelo usu√°rio.")
    except Exception as e:
        print(f"\n‚ùå Erro fatal: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

