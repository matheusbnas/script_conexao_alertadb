#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üîÑ WORKFLOW PREFECT - Sincroniza√ß√£o BigQuery

Este workflow usa Prefect para orquestrar a sincroniza√ß√£o de dados do NIMBUS
para o BigQuery, com suporte a execu√ß√£o agendada e monitoramento.

Baseado em: https://docs.prefect.io/v3/get-started/quickstart#open-source

IMPORTANTE: Este script usa Prefect Open Source (servidor local).
Certifique-se de que o servidor Prefect est√° rodando:
    prefect server start
"""

import os
# Configura√ß√£o Prefect: Use Cloud ou Local
# Para Prefect Cloud (executa mesmo com m√°quina desligada):
#   - API: cli-41fbdcc9-2a85-4885-a7cd-4390df02c7e4
#   - Configure: prefect cloud login
#   - Crie work pool no Prefect Cloud
#   - N√£o defina PREFECT_API_URL aqui (deixe usar Cloud automaticamente)
# Para Prefect Local (s√≥ funciona com m√°quina ligada):
#   - Descomente a linha abaixo
# os.environ["PREFECT_API_URL"] = "http://127.0.0.1:4200/api"

from prefect import flow, task
try:
    from prefect_gcp import GcpCredentials
    from prefect_gcp.bigquery import BigQueryWarehouse
    HAS_PREFECT_GCP = True
except ImportError:
    HAS_PREFECT_GCP = False
    print("‚ö†Ô∏è  prefect-gcp n√£o instalado. A verifica√ß√£o de status BigQuery ser√° limitada.")
    print("   Para instalar: pip install prefect-gcp")

import subprocess
import sys
from pathlib import Path
from datetime import datetime
from typing import Optional
from dotenv import load_dotenv

# Caminho base do projeto
project_root = Path(__file__).parent.parent.parent

@task(name="Verificar Conex√£o NIMBUS", log_prints=True)
def verificar_conexao_nimbus() -> bool:
    """Verifica se a conex√£o com o banco NIMBUS est√° dispon√≠vel."""
    try:
        import psycopg2
        from dotenv import load_dotenv
        import os
        
        # Carregar vari√°veis de ambiente
        load_dotenv(dotenv_path=project_root / '.env')
        
        # Obter configura√ß√µes do .env
        def obter_variavel(nome, obrigatoria=True, padrao=None):
            valor = os.getenv(nome)
            if not valor or (isinstance(valor, str) and valor.strip() == ''):
                if obrigatoria:
                    raise ValueError(f"Vari√°vel obrigat√≥ria n√£o encontrada: {nome}")
                return padrao
            return valor.strip() if isinstance(valor, str) else valor
        
        origem = {
            'host': obter_variavel('DB_ORIGEM_HOST'),
            'port': obter_variavel('DB_ORIGEM_PORT', obrigatoria=False, padrao='5432'),
            'dbname': obter_variavel('DB_ORIGEM_NAME'),
            'user': obter_variavel('DB_ORIGEM_USER'),
            'password': obter_variavel('DB_ORIGEM_PASSWORD'),
            'sslmode': obter_variavel('DB_ORIGEM_SSLMODE', obrigatoria=False, padrao='disable'),
            'connect_timeout': 10
        }
        
        # Testar conex√£o
        conn = psycopg2.connect(**origem)
        cur = conn.cursor()
        cur.execute("SELECT 1;")
        cur.close()
        conn.close()
        print(f"‚úÖ Conex√£o NIMBUS OK: {origem['dbname']}@{origem['host']}:{origem['port']}")
        return True
    except Exception as e:
        print(f"‚ùå Erro ao verificar conex√£o NIMBUS: {e}")
        import traceback
        traceback.print_exc()
        return False

@task(name="Verificar Credenciais GCP", log_prints=True)
def verificar_credenciais_gcp() -> bool:
    """Verifica se as credenciais do GCP est√£o configuradas."""
    try:
        credentials_path = project_root / 'credentials' / 'credentials.json'
        if credentials_path.exists():
            print(f"‚úÖ Credenciais GCP encontradas: {credentials_path}")
            return True
        else:
            print(f"‚ö†Ô∏è  Credenciais GCP n√£o encontradas em: {credentials_path}")
            return False
    except Exception as e:
        print(f"‚ùå Erro ao verificar credenciais GCP: {e}")
        return False

@task(name="Exporta√ß√£o Completa Pluviom√©tricos", log_prints=True, retries=2, retry_delay_seconds=60)
def exportar_pluviometricos_completo() -> dict:
    """Executa a exporta√ß√£o completa de dados pluviom√©tricos do NIMBUS para BigQuery.
    
    Esta task executa o script de exporta√ß√£o completa, que recarrega todos os dados
    desde 1997. Use com cuidado, pois pode demorar bastante tempo.
    """
    try:
        script_path = project_root / 'scripts' / 'bigquery' / 'exportar_pluviometricos_nimbus_bigquery.py'
        
        print(f"üîÑ Iniciando exporta√ß√£o completa de pluviom√©tricos...")
        print(f"   Script: {script_path}")
        
        inicio = datetime.now()
        
        # Executar script usando subprocess para capturar logs
        result = subprocess.run(
            [sys.executable, str(script_path)],
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=3600  # Timeout de 1 hora
        )
        
        tempo_decorrido = (datetime.now() - inicio).total_seconds()
        
        if result.returncode == 0:
            print(f"‚úÖ Exporta√ß√£o completa conclu√≠da em {tempo_decorrido:.1f} segundos")
            print(f"   Output: {result.stdout[-500:]}")  # √öltimas 500 linhas
            return {
                'sucesso': True,
                'tempo_segundos': tempo_decorrido,
                'mensagem': 'Exporta√ß√£o completa conclu√≠da com sucesso'
            }
        else:
            print(f"‚ùå Erro na exporta√ß√£o completa:")
            print(f"   Return code: {result.returncode}")
            print(f"   Stderr: {result.stderr[-500:]}")
            return {
                'sucesso': False,
                'tempo_segundos': tempo_decorrido,
                'mensagem': f'Erro na exporta√ß√£o: {result.stderr[-200:]}'
            }
            
    except subprocess.TimeoutExpired:
        print(f"‚è±Ô∏è  Timeout: Exporta√ß√£o demorou mais de 1 hora")
        return {
            'sucesso': False,
            'tempo_segundos': 3600,
            'mensagem': 'Timeout ap√≥s 1 hora'
        }
    except Exception as e:
        print(f"‚ùå Erro ao executar exporta√ß√£o completa: {e}")
        return {
            'sucesso': False,
            'tempo_segundos': 0,
            'mensagem': str(e)
        }

@task(name="Sincroniza√ß√£o Incremental Pluviom√©tricos", log_prints=True, retries=2, retry_delay_seconds=60)
def sincronizar_pluviometricos_incremental() -> dict:
    """Executa a sincroniza√ß√£o incremental de dados pluviom√©tricos.
    
    Esta task executa o script de sincroniza√ß√£o incremental, que busca apenas
    os dados novos desde a √∫ltima sincroniza√ß√£o. Ideal para execu√ß√£o peri√≥dica.
    
    Monitora erros de carregamento e retorna informa√ß√µes detalhadas.
    """
    try:
        script_path = project_root / 'scripts' / 'bigquery' / 'sincronizar_pluviometricos_nimbus_bigquery.py'
        
        print(f"üîÑ Iniciando sincroniza√ß√£o incremental de pluviom√©tricos...")
        print(f"   Script: {script_path}")
        print(f"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        inicio = datetime.now()
        
        # Executar script com flag --once
        result = subprocess.run(
            [sys.executable, str(script_path), '--once'],
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=1800  # Timeout de 30 minutos
        )
        
        tempo_decorrido = (datetime.now() - inicio).total_seconds()
        
        # Analisar output para detectar erros espec√≠ficos
        output_completo = result.stdout + result.stderr
        erros_detectados = []
        
        # Verificar erros comuns
        if "Resources exceeded" in output_completo or "10000 partitions" in output_completo:
            erros_detectados.append("ERRO: Limite de parti√ß√µes excedido (precisa recriar tabela com particionamento por M√äS)")
        
        if "TIMESTAMP_NANOS" in output_completo or "Invalid timestamp" in output_completo:
            erros_detectados.append("ERRO: Problema com formato de timestamp")
        
        if "ConnectionResetError" in output_completo or "connection" in output_completo.lower():
            erros_detectados.append("ERRO: Problema de conex√£o com banco de dados")
        
        if "ERRO CR√çTICO" in output_completo or "‚ùå" in output_completo:
            erros_detectados.append("ERRO: Erro cr√≠tico detectado no script")
        
        if result.returncode == 0:
            if erros_detectados:
                print(f"‚ö†Ô∏è  Sincroniza√ß√£o conclu√≠da mas com avisos:")
                for erro in erros_detectados:
                    print(f"   {erro}")
            else:
                print(f"‚úÖ Sincroniza√ß√£o incremental conclu√≠da com sucesso em {tempo_decorrido:.1f} segundos")
            
            # Extrair informa√ß√µes √∫teis do output
            registros_processados = 0
            if "registros" in output_completo.lower():
                import re
                match = re.search(r'(\d[\d,]*)\s+registros', output_completo)
                if match:
                    registros_processados = int(match.group(1).replace(',', ''))
            
            print(f"   üìä Registros processados: {registros_processados:,}")
            print(f"   ‚è±Ô∏è  Tempo de execu√ß√£o: {tempo_decorrido:.1f} segundos")
            
            return {
                'sucesso': True,
                'tempo_segundos': tempo_decorrido,
                'registros_processados': registros_processados,
                'mensagem': 'Sincroniza√ß√£o incremental conclu√≠da com sucesso',
                'avisos': erros_detectados if erros_detectados else None,
                'output_resumo': output_completo[-1000:] if len(output_completo) > 1000 else output_completo
            }
        else:
            print(f"‚ùå ERRO na sincroniza√ß√£o incremental:")
            print(f"   Return code: {result.returncode}")
            print(f"   Erros detectados: {len(erros_detectados)}")
            for erro in erros_detectados:
                print(f"   {erro}")
            
            # Log completo do erro
            print(f"\n   üìã √öltimas linhas do stderr:")
            stderr_lines = result.stderr.split('\n')[-20:]
            for line in stderr_lines:
                if line.strip():
                    print(f"      {line}")
            
            return {
                'sucesso': False,
                'tempo_segundos': tempo_decorrido,
                'return_code': result.returncode,
                'mensagem': f'Erro na sincroniza√ß√£o (code: {result.returncode})',
                'erros_detectados': erros_detectados,
                'stderr': result.stderr[-1000:] if len(result.stderr) > 1000 else result.stderr
            }
            
    except subprocess.TimeoutExpired:
        print(f"‚è±Ô∏è  TIMEOUT: Sincroniza√ß√£o demorou mais de 30 minutos")
        print(f"   Isso pode indicar problema de conex√£o ou volume muito grande de dados")
        return {
            'sucesso': False,
            'tempo_segundos': 1800,
            'mensagem': 'Timeout ap√≥s 30 minutos - verifique conex√£o e volume de dados',
            'erros_detectados': ['TIMEOUT: Processo demorou mais de 30 minutos']
        }
    except Exception as e:
        print(f"‚ùå ERRO FATAL ao executar sincroniza√ß√£o incremental: {e}")
        import traceback
        traceback.print_exc()
        return {
            'sucesso': False,
            'tempo_segundos': 0,
            'mensagem': f'Erro fatal: {str(e)}',
            'erros_detectados': [f'Erro fatal: {str(e)}']
        }

@task(name="Verificar Status BigQuery", log_prints=True)
def verificar_status_bigquery() -> dict:
    """Verifica o status da tabela no BigQuery usando Prefect GCP integration ou subprocess."""
    try:
        credentials_path = project_root / 'credentials' / 'credentials.json'
        
        if not credentials_path.exists():
            return {
                'sucesso': False,
                'mensagem': f'Credenciais n√£o encontradas em: {credentials_path}'
            }
        
        # Tentar usar prefect-gcp se dispon√≠vel
        if HAS_PREFECT_GCP:
            try:
                gcp_credentials = GcpCredentials(service_account_file=str(credentials_path))
                
                with BigQueryWarehouse(gcp_credentials=gcp_credentials) as warehouse:
                    query = """
                    SELECT 
                        COUNT(*) as total_registros,
                        MIN(dia) as data_minima,
                        MAX(dia) as data_maxima,
                        COUNT(DISTINCT estacao_id) as total_estacoes
                    FROM `alertadb_cor_raw.pluviometricos`
                    """
                    
                    result = warehouse.fetch_one(query)
                    
                    if result:
                        print(f"üìä Status BigQuery (via Prefect GCP):")
                        print(f"   Total de registros: {result[0]:,}")
                        print(f"   Data m√≠nima: {result[1]}")
                        print(f"   Data m√°xima: {result[2]}")
                        print(f"   Total de esta√ß√µes: {result[3]}")
                        
                        return {
                            'sucesso': True,
                            'total_registros': result[0],
                            'data_minima': str(result[1]),
                            'data_maxima': str(result[2]),
                            'total_estacoes': result[3]
                        }
            except Exception as e:
                print(f"‚ö†Ô∏è  Erro ao usar Prefect GCP, tentando m√©todo alternativo: {e}")
        
        # M√©todo alternativo: usar bq CLI ou script Python direto
        # Por enquanto, retornar sucesso sem dados detalhados
        print(f"üìä Status BigQuery: Verifica√ß√£o b√°sica (prefect-gcp n√£o dispon√≠vel)")
        return {
            'sucesso': True,
            'mensagem': 'Verifica√ß√£o b√°sica conclu√≠da (instale prefect-gcp para estat√≠sticas detalhadas)'
        }
                
    except Exception as e:
        print(f"‚ùå Erro ao verificar status BigQuery: {e}")
        return {
            'sucesso': False,
            'mensagem': str(e)
        }

@flow(name="Sincroniza√ß√£o BigQuery - Incremental", log_prints=True)
def sincronizacao_incremental_flow() -> dict:
    """Flow principal para sincroniza√ß√£o incremental de dados pluviom√©tricos.
    
    Este flow executa:
    1. Verifica√ß√£o de conex√µes (NIMBUS e GCP)
    2. Sincroniza√ß√£o incremental
    3. Verifica√ß√£o do status final
    4. Monitoramento de erros de carregamento
    
    Ideal para execu√ß√£o peri√≥dica (cron) a cada 5 minutos.
    Monitora e reporta todos os erros de carregamento.
    """
    print("=" * 80)
    print("üîÑ INICIANDO SINCRONIZA√á√ÉO INCREMENTAL - BigQuery")
    print("=" * 80)
    print(f"‚è∞ In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. Verificar conex√µes
    print("üì° Verificando conex√µes...")
    conexao_nimbus = verificar_conexao_nimbus()
    credenciais_gcp = verificar_credenciais_gcp()
    
    if not conexao_nimbus or not credenciais_gcp:
        erro_msg = "‚ùå Falha na verifica√ß√£o de conex√µes. Abortando flow."
        print(erro_msg)
        print(f"   Conex√£o NIMBUS: {'‚úÖ OK' if conexao_nimbus else '‚ùå FALHOU'}")
        print(f"   Credenciais GCP: {'‚úÖ OK' if credenciais_gcp else '‚ùå FALHOU'}")
        return {
            'sucesso': False,
            'mensagem': 'Falha na verifica√ß√£o de conex√µes',
            'conexao_nimbus': conexao_nimbus,
            'credenciais_gcp': credenciais_gcp,
            'erros': ['Falha na verifica√ß√£o de conex√µes'],
            'timestamp': datetime.now().isoformat()
        }
    
    print()
    
    # 2. Executar sincroniza√ß√£o incremental
    print("üì¶ Executando sincroniza√ß√£o incremental...")
    resultado_sync = sincronizar_pluviometricos_incremental()
    
    # Verificar se houve erros
    sucesso_sync = resultado_sync.get('sucesso', False)
    erros_sync = resultado_sync.get('erros_detectados', [])
    avisos_sync = resultado_sync.get('avisos', [])
    
    print()
    
    # 3. Verificar status final no BigQuery
    print("üìä Verificando status final no BigQuery...")
    status_bq = verificar_status_bigquery()
    
    # Compilar todos os erros e avisos
    todos_erros = []
    todos_avisos = []
    
    if erros_sync:
        todos_erros.extend(erros_sync)
    if avisos_sync:
        todos_avisos.extend(avisos_sync)
    if not status_bq.get('sucesso', False):
        todos_erros.append(f"Erro ao verificar status BigQuery: {status_bq.get('mensagem', 'Desconhecido')}")
    
    print()
    print("=" * 80)
    
    # Resumo final
    if sucesso_sync and not todos_erros:
        print("‚úÖ SINCRONIZA√á√ÉO INCREMENTAL CONCLU√çDA COM SUCESSO")
        if todos_avisos:
            print(f"‚ö†Ô∏è  Avisos: {len(todos_avisos)}")
            for aviso in todos_avisos:
                print(f"   - {aviso}")
    elif sucesso_sync and todos_erros:
        print("‚ö†Ô∏è  SINCRONIZA√á√ÉO CONCLU√çDA COM ERROS")
        print(f"‚ùå Erros detectados: {len(todos_erros)}")
        for erro in todos_erros:
            print(f"   - {erro}")
    else:
        print("‚ùå SINCRONIZA√á√ÉO FALHOU")
        print(f"‚ùå Erros detectados: {len(todos_erros)}")
        for erro in todos_erros:
            print(f"   - {erro}")
    
    print("=" * 80)
    print(f"‚è∞ Fim: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Informa√ß√µes de resumo
    if status_bq.get('sucesso'):
        print(f"üìä Status BigQuery:")
        print(f"   Total de registros: {status_bq.get('total_registros', 0):,}")
        print(f"   Data m√≠nima: {status_bq.get('data_minima', 'N/A')}")
        print(f"   Data m√°xima: {status_bq.get('data_maxima', 'N/A')}")
        print(f"   Total de esta√ß√µes: {status_bq.get('total_estacoes', 0)}")
    
    if resultado_sync.get('registros_processados', 0) > 0:
        print(f"üì¶ Registros processados nesta execu√ß√£o: {resultado_sync.get('registros_processados', 0):,}")
    
    return {
        'sucesso': sucesso_sync and not todos_erros,
        'sincronizacao': resultado_sync,
        'status_bigquery': status_bq,
        'erros': todos_erros,
        'avisos': todos_avisos,
        'registros_processados': resultado_sync.get('registros_processados', 0),
        'timestamp': datetime.now().isoformat()
    }

@flow(name="Exporta√ß√£o Completa BigQuery", log_prints=True)
def exportacao_completa_flow() -> dict:
    """Flow principal para exporta√ß√£o completa de dados pluviom√©tricos.
    
    Este flow executa:
    1. Verifica√ß√£o de conex√µes (NIMBUS e GCP)
    2. Exporta√ß√£o completa (todos os dados desde 1997)
    3. Verifica√ß√£o do status final
    
    ATEN√á√ÉO: Esta opera√ß√£o pode demorar v√°rias horas!
    Use apenas quando necess√°rio recarregar todos os dados.
    """
    print("=" * 80)
    print("üîÑ INICIANDO EXPORTA√á√ÉO COMPLETA - BigQuery")
    print("=" * 80)
    print(f"‚è∞ In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    print("‚ö†Ô∏è  ATEN√á√ÉO: Esta opera√ß√£o pode demorar v√°rias horas!")
    print()
    
    # 1. Verificar conex√µes
    print("üì° Verificando conex√µes...")
    conexao_nimbus = verificar_conexao_nimbus()
    credenciais_gcp = verificar_credenciais_gcp()
    
    if not conexao_nimbus or not credenciais_gcp:
        print("‚ùå Falha na verifica√ß√£o de conex√µes. Abortando flow.")
        return {
            'sucesso': False,
            'mensagem': 'Falha na verifica√ß√£o de conex√µes',
            'conexao_nimbus': conexao_nimbus,
            'credenciais_gcp': credenciais_gcp
        }
    
    print()
    
    # 2. Executar exporta√ß√£o completa
    print("üì¶ Executando exporta√ß√£o completa...")
    resultado_export = exportar_pluviometricos_completo()
    
    print()
    
    # 3. Verificar status final
    print("üìä Verificando status final no BigQuery...")
    status_bq = verificar_status_bigquery()
    
    print()
    print("=" * 80)
    print("‚úÖ EXPORTA√á√ÉO COMPLETA CONCLU√çDA")
    print("=" * 80)
    print(f"‚è∞ Fim: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return {
        'sucesso': resultado_export.get('sucesso', False),
        'exportacao': resultado_export,
        'status_bigquery': status_bq,
        'timestamp': datetime.now().isoformat()
    }

if __name__ == "__main__":
    """
    Configura√ß√£o de Execu√ß√£o:
    
    OP√á√ÉO 1: Prefect Cloud (Recomendado - executa mesmo com m√°quina desligada)
    - Fa√ßa login: prefect cloud login
    - Crie work pool no Prefect Cloud UI
    - Deploy: prefect deploy prefect_workflow_bigquery.py:sincronizacao_incremental_flow --pool seu-work-pool
    - Inicie agent em servidor dedicado: prefect agent start seu-work-pool
    
    OP√á√ÉO 2: Prefect Local (s√≥ funciona com m√°quina ligada)
    - Descomente: os.environ["PREFECT_API_URL"] = "http://127.0.0.1:4200/api"
    - Inicie servidor: prefect server start
    - Execute este script
    """
    
    # Op√ß√£o 1: Execu√ß√£o local √∫nica (teste)
    # sincronizacao_incremental_flow()
    
    # Op√ß√£o 2: Execu√ß√£o com agendamento (serve mode) - ATUALIZA A CADA 5 MINUTOS
    # Para Prefect Cloud: use 'prefect deploy' em vez de .serve()
    # Para Prefect Local: use .serve() abaixo (certifique-se de que servidor est√° rodando)
    
    sincronizacao_incremental_flow.serve(
        name="sincronizacao-bigquery-incremental",
        cron="*/5 * * * *",  # A cada 5 minutos
        description="Sincroniza√ß√£o incremental de dados pluviom√©tricos do NIMBUS para BigQuery. Atualiza BigQuery a cada 5 minutos e monitora erros de carregamento."
        # work_pool_name="bigquery-sync-pool",  # Descomente se usar Prefect Cloud com work pool
    )
    
    # Para exporta√ß√£o completa (descomente se necess√°rio)
    # exportacao_completa_flow.serve(
    #     name="exportacao-bigquery-completa",
    #     cron="0 2 * * 0",  # Todo domingo √†s 2h da manh√£
    #     description="Exporta√ß√£o completa de dados pluviom√©tricos do NIMBUS para BigQuery"
    #     # work_pool_name="bigquery-sync-pool",  # Descomente se usar Prefect Cloud
    # )

