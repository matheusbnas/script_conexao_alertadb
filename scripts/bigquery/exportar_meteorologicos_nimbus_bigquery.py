#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üå§Ô∏è EXPORTA√á√ÉO DIRETA - DADOS METEOROL√ìGICOS NIMBUS ‚Üí BigQuery

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéØ PROP√ìSITO DESTE SCRIPT:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Este script exporta dados METEOROL√ìGICOS diretamente do banco NIMBUS (alertadb) 
para o BigQuery, usando DISTINCT ON (mesma l√≥gica dos dados pluviom√©tricos).

ARQUITETURA:
    NIMBUS (alertadb) ‚Üí Parquet ‚Üí BigQuery
              ‚Üë [ESTE SCRIPT - DIRETO]

QUERY UTILIZADA:
    ‚úÖ DISTINCT ON (l."horaLeitura", l.estacao_id)
    ‚úÖ ORDER BY l."horaLeitura" ASC, l.estacao_id ASC, l.id DESC
    ‚úÖ Garante apenas um registro por (data_hora, estacao_id) - o mais recente
    ‚úÖ Usa subquery com ROW_NUMBER para pivotar dados de sensores

VANTAGENS:
    ‚úÖ Mais r√°pido (menos camadas)
    ‚úÖ Dados sempre da fonte original (NIMBUS)
    ‚úÖ BigQuery otimizado para an√°lises
    ‚úÖ Formato Parquet (5-10x mais r√°pido que CSV)
    ‚úÖ Exporta√ß√£o completa (todos os dados desde 1997)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã O QUE ESTE SCRIPT FAZ:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ Conecta diretamente ao banco NIMBUS (alertadb)
‚úÖ Busca TODOS os dados meteorol√≥gicos usando DISTINCT ON
‚úÖ Exporta para formato Parquet completo
‚úÖ Carrega no BigQuery automaticamente
‚úÖ Cria/atualiza tabela no BigQuery
‚úÖ Processa em lotes de 10.000 registros para otimizar mem√≥ria
‚úÖ Usa TIMESTAMP para coluna dia (igual aos pluviom√©tricos)
‚úÖ Converte timezone para UTC (padr√£o BigQuery)
‚úÖ Particionamento por coluna dia (melhora performance)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã CONFIGURA√á√ÉO:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Vari√°veis obrigat√≥rias no .env:
- DB_ORIGEM_HOST, DB_ORIGEM_NAME, DB_ORIGEM_USER, DB_ORIGEM_PASSWORD
- BIGQUERY_PROJECT_ID

Vari√°veis opcionais:
- BIGQUERY_DATASET_ID_NIMBUS (padr√£o: alertadb_cor_raw)
- BIGQUERY_TABLE_ID_METEOROLOGICOS (padr√£o: meteorologicos)
- BIGQUERY_CREDENTIALS_PATH (opcional: caminho para credentials.json)

üìö GUIA COMPLETO: Veja docs/BIGQUERY_CONFIGURAR_VARIAVEIS.md para saber
   onde encontrar cada configura√ß√£o no console GCP/BigQuery.
"""

import psycopg2
import pandas as pd
from sqlalchemy import create_engine
from google.cloud import bigquery
from google.oauth2 import service_account
import os
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path
import tempfile

# Carregar vari√°veis de ambiente
project_root = Path(__file__).parent.parent.parent
load_dotenv(dotenv_path=project_root / '.env')

def obter_variavel(nome, obrigatoria=True, padrao=None):
    """Obt√©m vari√°vel de ambiente."""
    valor = os.getenv(nome)
    if not valor or (isinstance(valor, str) and valor.strip() == ''):
        if obrigatoria:
            raise ValueError(f"‚ùå Vari√°vel obrigat√≥ria n√£o encontrada: {nome}")
        return padrao
    return valor.strip() if isinstance(valor, str) else valor

def carregar_configuracoes():
    """Carrega configura√ß√µes do .env."""
    try:
        # Banco ORIGEM - NIMBUS (alertadb)
        origem = {
            'host': obter_variavel('DB_ORIGEM_HOST'),
            'port': obter_variavel('DB_ORIGEM_PORT', obrigatoria=False, padrao='5432'),
            'dbname': obter_variavel('DB_ORIGEM_NAME'),
            'user': obter_variavel('DB_ORIGEM_USER'),
            'password': obter_variavel('DB_ORIGEM_PASSWORD'),
            'sslmode': obter_variavel('DB_ORIGEM_SSLMODE', obrigatoria=False, padrao='disable'),
            'connect_timeout': 10
        }

        # BigQuery
        credentials_padrao = project_root / 'credentials' / 'credentials.json'
        
        credentials_path_env = obter_variavel('BIGQUERY_CREDENTIALS_PATH', obrigatoria=False)
        if credentials_path_env:
            credentials_path = Path(credentials_path_env)
            if not credentials_path.exists():
                print(f"   ‚ö†Ô∏è  Caminho no .env n√£o encontrado: {credentials_path}")
                print(f"   üí° Tentando usar caminho padr√£o: {credentials_padrao}")
                if credentials_padrao.exists():
                    credentials_path = credentials_padrao
                else:
                    credentials_path = None
        elif credentials_padrao.exists():
            credentials_path = credentials_padrao
        else:
            credentials_path = None
        
        bigquery_config = {
            'project_id': obter_variavel('BIGQUERY_PROJECT_ID'),
            'dataset_id': obter_variavel('BIGQUERY_DATASET_ID_NIMBUS', obrigatoria=False, padrao='alertadb_cor_raw'),
            'table_id': obter_variavel('BIGQUERY_TABLE_ID_METEOROLOGICOS', obrigatoria=False, padrao='meteorologicos'),
            'credentials_path': str(credentials_path) if credentials_path else None,
        }
        
        return origem, bigquery_config
    
    except ValueError as e:
        print("=" * 70)
        print("‚ùå ERRO DE CONFIGURA√á√ÉO")
        print("=" * 70)
        print(str(e))
        print("\nüìù Configure no .env:")
        print("   # Banco NIMBUS (origem)")
        print("   DB_ORIGEM_HOST=10.2.223.114")
        print("   DB_ORIGEM_NAME=alertadb")
        print("   DB_ORIGEM_USER=planejamento_cor")
        print("   DB_ORIGEM_PASSWORD=sua_senha")
        print("")
        print("   # BigQuery (destino)")
        print("   BIGQUERY_PROJECT_ID=seu-projeto-gcp")
        print("   BIGQUERY_DATASET_ID_NIMBUS=alertadb_cor_raw (opcional)")
        print("   BIGQUERY_TABLE_ID_METEOROLOGICOS=meteorologicos (opcional)")
        print("   BIGQUERY_CREDENTIALS_PATH=/caminho/credentials.json (opcional)")
        print("=" * 70)
        raise

ORIGEM, BIGQUERY_CONFIG = carregar_configuracoes()

def testar_conexao_nimbus():
    """Testa conex√£o com NIMBUS."""
    print("=" * 70)
    print("TESTE DE CONEX√ÉO")
    print("=" * 70)
    
    try:
        conn = psycopg2.connect(**ORIGEM)
        cur = conn.cursor()
        cur.execute("SELECT 1;")
        print(f"   ‚úÖ NIMBUS: SUCESSO!")
        print(f"      {ORIGEM['dbname']}@{ORIGEM['host']}:{ORIGEM['port']}")
        cur.close()
        conn.close()
        return True
    except Exception as e:
        print(f"   ‚ùå NIMBUS: FALHA!")
        print(f"      Erro: {e}")
        return False

def query_todos_dados_meteorologicos():
    """Retorna query para buscar TODOS os dados meteorol√≥gicos dispon√≠veis no banco NIMBUS.
    
    Usa DISTINCT ON para garantir apenas um registro por (horaLeitura, estacao_id),
    mantendo o registro com o maior ID (mais recente), seguindo a mesma l√≥gica
    dos dados pluviom√©tricos.
    
    A query usa uma CTE para pivotar os dados dos sensores
    (Chuva, Dire√ß√£o Vento, Velocidade Vento, Temperatura, Press√£o, Umidade)
    em colunas, agrupando por leitura_id. Depois usa DISTINCT ON para garantir
    apenas uma leitura por (horaLeitura, estacao_id), pegando a mais recente.
    
    IMPORTANTE: A ordem do ORDER BY deve corresponder √† ordem do DISTINCT ON,
    e depois ordenar por leitura_id DESC para pegar o registro mais recente.
    
    A coluna horaLeitura √© TIMESTAMPTZ NOT NULL no NIMBUS, preservando o timezone original.
    Esta √© a MESMA l√≥gica usada em query_todos_dados_pluviometricos() para garantir consist√™ncia.
    """
    return """
WITH sensores_pivotados AS (
    SELECT
        l."horaLeitura" AS data_hora,
        l.estacao_id,
        e.nome AS nome_estacao,
        l.id AS leitura_id,
        MAX(CASE WHEN s.nome = 'Chuva'                 THEN ls.valor END) AS chuva,
        MAX(CASE WHEN s.nome = 'Dire√ß√£o Vento'         THEN ls.valor END) AS dirVento,
        MAX(CASE WHEN s.nome = 'Velocidade Vento'      THEN ls.valor END) AS velVento,
        MAX(CASE WHEN s.nome = 'Temperatura do Ar'     THEN ls.valor END) AS temperatura,
        MAX(CASE WHEN s.nome = 'Press√£o Atmosf√©rica'   THEN ls.valor END) AS pressao,
        MAX(CASE WHEN s.nome = 'Umidade do Ar'         THEN ls.valor END) AS umidade
    FROM public.estacoes_leiturasensor ls
    JOIN public.estacoes_leitura l
          ON ls.leitura_id = l.id
    JOIN public.estacoes_sensor s
          ON ls.sensor_id = s.id
    JOIN public.estacoes_estacao e
          ON e.id = l.estacao_id
    WHERE l."horaLeitura" >= '1997-01-01'
    GROUP BY l."horaLeitura", l.estacao_id, e.nome, l.id
)
SELECT DISTINCT ON (data_hora, estacao_id)
    data_hora AS "Dia",  -- TIMESTAMPTZ NOT NULL (preserva timezone original)
    estacao_id,
    nome_estacao AS "Estacao",
    chuva,
    dirVento,
    velVento,
    temperatura,
    pressao,
    umidade
FROM sensores_pivotados
ORDER BY data_hora ASC, estacao_id ASC, leitura_id DESC;
"""

def obter_schema_meteorologicos():
    """Retorna schema do BigQuery para tabela meteorologicos."""
    return [
        bigquery.SchemaField("dia", "TIMESTAMP", mode="REQUIRED", description="Data e hora em que foi realizada a medi√ß√£o. Origem: TIMESTAMPTZ NOT NULL do NIMBUS (preserva timezone original). Armazenado em UTC no BigQuery."),
        bigquery.SchemaField("dia_original", "STRING", mode="NULLABLE", description="Data e hora no formato exato do banco original da NIMBUS (ex: 2009-02-16 02:12:20.000 -0300)"),
        bigquery.SchemaField("utc_offset", "STRING", mode="NULLABLE", description="Offset UTC do timezone original (ex: -0300 para hor√°rio padr√£o do Brasil, -0200 para hor√°rio de ver√£o)"),
        bigquery.SchemaField("estacao", "STRING", mode="NULLABLE", description="Nome da esta√ß√£o meteorol√≥gica"),
        bigquery.SchemaField("estacao_id", "INTEGER", mode="REQUIRED", description="ID da esta√ß√£o meteorol√≥gica"),
        bigquery.SchemaField("chuva", "FLOAT64", mode="NULLABLE", description="Chuva (mm)"),
        bigquery.SchemaField("dirVento", "FLOAT64", mode="NULLABLE", description="Dire√ß√£o do vento (graus)"),
        bigquery.SchemaField("velVento", "FLOAT64", mode="NULLABLE", description="Velocidade do vento (m/s ou km/h)"),
        bigquery.SchemaField("temperatura", "FLOAT64", mode="NULLABLE", description="Temperatura do ar (¬∞C)"),
        bigquery.SchemaField("pressao", "FLOAT64", mode="NULLABLE", description="Press√£o atmosf√©rica (hPa)"),
        bigquery.SchemaField("umidade", "FLOAT64", mode="NULLABLE", description="Umidade do ar (%)"),
    ]

def criar_dataset_se_nao_existir(client, dataset_id):
    """Cria dataset no BigQuery se n√£o existir."""
    try:
        dataset_ref = client.dataset(dataset_id)
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "US"
        dataset.description = "Dados meteorol√≥gicos do NIMBUS"
        
        dataset = client.create_dataset(dataset, exists_ok=True)
        print(f"‚úÖ Dataset '{dataset_id}' criado/verificado no BigQuery!")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar dataset: {e}")
        return False

def criar_tabela_com_schema(client, dataset_id, table_id, schema):
    """Cria tabela no BigQuery com schema e particionamento por data se n√£o existir ou atualiza schema se necess√°rio."""
    try:
        table_ref = client.dataset(dataset_id).table(table_id)
        
        # Verificar se a tabela j√° existe
        try:
            table = client.get_table(table_ref)
            # Se existe, verificar se tem schema v√°lido
            if not table.schema or len(table.schema) == 0:
                print(f"   ‚ö†Ô∏è  Tabela existe mas sem schema.")
                if table.num_rows > 0:
                    print(f"   üìã Tabela tem {table.num_rows:,} registros. Recriando com schema e particionamento...")
                    client.delete_table(table_ref)
                    table = bigquery.Table(table_ref, schema=schema)
                    table.description = "Dados meteorol√≥gicos do NIMBUS (desde 1997)"
                    table.time_partitioning = bigquery.TimePartitioning(
                        type_=bigquery.TimePartitioningType.MONTH,
                        field="dia"  # Particionamento por coluna dia (TIMESTAMP) - agrupado por M√äS
                    )
                    table = client.create_table(table)
                    print(f"‚úÖ Tabela '{table_id}' recriada com schema e particionamento!")
                else:
                    print(f"   üìã Tabela vazia sem schema. Recriando com schema e particionamento...")
                    client.delete_table(table_ref)
                    table = bigquery.Table(table_ref, schema=schema)
                    table.description = "Dados meteorol√≥gicos do NIMBUS (desde 1997)"
                    table.time_partitioning = bigquery.TimePartitioning(
                        type_=bigquery.TimePartitioningType.MONTH,
                        field="dia"  # Particionamento por coluna dia (TIMESTAMP) - agrupado por M√äS
                    )
                    table = client.create_table(table)
                    print(f"‚úÖ Tabela '{table_id}' recriada com schema e particionamento!")
                return True
            else:
                # Verificar se j√° tem particionamento
                # Verificar se o particionamento est√° correto (por coluna dia, tipo MONTH)
                if table.time_partitioning and table.time_partitioning.field:
                    # Verificar se est√° usando DAY (precisa mudar para MONTH) ou se j√° est√° MONTH
                    if table.time_partitioning.field != "dia":
                        print(f"   ‚ö†Ô∏è  Tabela '{table_id}' existe com particionamento por campo '{table.time_partitioning.field}'.")
                        print(f"   üí° Precisamos recriar a tabela com particionamento por 'dia' (M√äS).")
                        print(f"   üîÑ Deletando tabela para recriar com particionamento correto...")
                        client.delete_table(table_ref)
                        table = bigquery.Table(table_ref, schema=schema)
                        table.description = "Dados meteorol√≥gicos do NIMBUS (desde 1997)"
                        table.time_partitioning = bigquery.TimePartitioning(
                            type_=bigquery.TimePartitioningType.MONTH,
                            field="dia"  # Particionamento por coluna dia (TIMESTAMP) - agrupado por M√äS
                        )
                        table = client.create_table(table)
                        print(f"‚úÖ Tabela '{table_id}' recriada com particionamento por coluna 'dia' (M√äS)!")
                    elif table.time_partitioning.type_ != bigquery.TimePartitioningType.MONTH:
                        # Tabela tem particionamento por DIA, mas precisa ser MONTH
                        print(f"   ‚ö†Ô∏è  Tabela '{table_id}' existe com particionamento por DIA.")
                        print(f"   ‚ö†Ô∏è  Isso pode exceder o limite de 10.000 parti√ß√µes!")
                        print(f"   üí° Precisamos recriar a tabela com particionamento por M√äS.")
                        print(f"   üîÑ Deletando tabela para recriar com particionamento por M√äS...")
                        client.delete_table(table_ref)
                        table = bigquery.Table(table_ref, schema=schema)
                        table.description = "Dados meteorol√≥gicos do NIMBUS (desde 1997)"
                        table.time_partitioning = bigquery.TimePartitioning(
                            type_=bigquery.TimePartitioningType.MONTH,
                            field="dia"  # Particionamento por coluna dia (TIMESTAMP) - agrupado por M√äS
                        )
                        table = client.create_table(table)
                        print(f"‚úÖ Tabela '{table_id}' recriada com particionamento por M√äS!")
                    else:
                        print(f"‚úÖ Tabela '{table_id}' j√° existe com particionamento por coluna 'dia' (M√äS)!")
                elif not table.time_partitioning:
                    if table.num_rows == 0:
                        print(f"   üìã Tabela existe mas sem particionamento e est√° vazia.")
                        print(f"   üîÑ Recriando tabela com particionamento por M√äS...")
                        client.delete_table(table_ref)
                        table = bigquery.Table(table_ref, schema=schema)
                        table.description = "Dados meteorol√≥gicos do NIMBUS (desde 1997)"
                        table.time_partitioning = bigquery.TimePartitioning(
                            type_=bigquery.TimePartitioningType.MONTH,
                            field="dia"  # Particionamento por coluna dia (TIMESTAMP) - agrupado por M√äS
                        )
                        table = client.create_table(table)
                        print(f"‚úÖ Tabela '{table_id}' recriada com particionamento por M√äS!")
                    else:
                        print(f"   ‚ö†Ô∏è  Tabela '{table_id}' existe mas SEM particionamento e tem {table.num_rows:,} registros.")
                        print(f"   üí° BigQuery n√£o permite converter tabela n√£o particionada em particionada.")
                        print(f"   üìã Continuando sem particionamento (dados ser√£o substitu√≠dos com WRITE_TRUNCATE).")
                        print(f"   üí° Para ter particionamento, delete a tabela manualmente e execute o script novamente.")
                else:
                    print(f"‚úÖ Tabela '{table_id}' j√° existe com schema ({len(table.schema)} campos) e particionamento por M√äS!")
                return True
        except Exception as e:
            # Tabela n√£o existe, criar
            if "Not found" in str(e) or "404" in str(e) or "does not exist" in str(e).lower():
                print(f"   üìã Criando tabela '{table_id}' com schema e particionamento...")
            else:
                print(f"   ‚ö†Ô∏è  Erro ao verificar tabela: {e}")
                raise
        
        # Criar tabela com schema e particionamento
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "Dados meteorol√≥gicos do NIMBUS (desde 1997)"
        # Como dia √© TIMESTAMP, usar particionamento por M√äS para evitar exceder limite de 10.000 parti√ß√µes
        # Com dados desde 1997, particionamento por DIA excederia o limite (mais de 10.000 dias)
        # Particionamento por M√äS reduz para ~340 parti√ß√µes (desde 1997 at√© hoje)
        table.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.MONTH,
            field="dia"  # Particionamento por coluna dia (TIMESTAMP) - agrupado por M√äS
        )
        table = client.create_table(table, exists_ok=False)
        print(f"‚úÖ Tabela '{table_id}' criada com schema e particionamento por M√äS no BigQuery!")
        print(f"   üí° Particionamento melhora performance de queries e reduz custos")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar/atualizar tabela: {e}")
        import traceback
        traceback.print_exc()
        return False

def processar_e_carregar_tabela(engine_nimbus, client_bq, dataset_id, table_id, schema, query, descricao):
    """Processa e carrega uma tabela espec√≠fica no BigQuery."""
    print(f"üì¶ Buscando dados {descricao} do NIMBUS...")
    print(f"   üí° Isso pode levar alguns minutos dependendo do volume de dados...")
    
    inicio_query = datetime.now()
    chunksize = 10000
    total_registros = 0
    chunk_numero = 1
    
    table_ref = client_bq.dataset(dataset_id).table(table_id)
    
    # Configurar job de carga
    job_config = bigquery.LoadJobConfig(
        schema=schema,
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        source_format=bigquery.SourceFormat.PARQUET,
    )
    
    print(f"\nüì¶ Processando e carregando dados {descricao} no BigQuery...")
    print(f"   üí° Usando formato Parquet para melhor performance")
    print(f"   üí° Query usa DISTINCT ON (mesma l√≥gica dos dados pluviom√©tricos)\n")
    
    # Criar diret√≥rio tempor√°rio para m√∫ltiplos arquivos Parquet
    temp_dir = tempfile.mkdtemp()
    parquet_files = []
    
    import gc
    chunks_list = []
    batch_size = 4
    batch_file_num = 1
    
    for chunk_df in pd.read_sql(query, engine_nimbus, chunksize=chunksize):
        print(f"   üì¶ Processando chunk {chunk_numero} ({len(chunk_df):,} registros)...")
        
        # Renomear colunas para corresponder ao schema BigQuery
        chunk_df = chunk_df.rename(columns={
            'Dia': 'dia',
            'Estacao': 'estacao'
        })
        
        # Remover colunas auxiliares se existirem
        colunas_para_remover = ['TimezoneOffset', 'leitura_id']
        for col in colunas_para_remover:
            if col in chunk_df.columns:
                chunk_df = chunk_df.drop(columns=[col])
        
        # Processar coluna dia: TIMESTAMPTZ do NIMBUS ‚Üí TIMESTAMP (UTC) do BigQuery
        def processar_dia_timestamp(dt):
            """Processa TIMESTAMPTZ do PostgreSQL (NIMBUS) para TIMESTAMP do BigQuery (UTC).
            
            A coluna horaLeitura no banco NIMBUS √© TIMESTAMPTZ NOT NULL (preserva timezone original).
            O BigQuery armazena TIMESTAMP em UTC internamente, ent√£o convertemos preservando o valor correto.
            
            IMPORTANTE: Preserva o timezone original do TIMESTAMPTZ antes de converter para UTC.
            """
            if pd.isna(dt):
                return None
            try:
                # Converter para pandas Timestamp se necess√°rio
                if isinstance(dt, str):
                    # Tentar parsear preservando timezone se presente na string
                    dt_parsed = pd.to_datetime(dt)
                elif isinstance(dt, pd.Timestamp):
                    dt_parsed = dt
                elif hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
                    # Se j√° √© datetime com timezone (TIMESTAMPTZ do PostgreSQL)
                    dt_parsed = pd.Timestamp(dt)
                else:
                    dt_parsed = pd.to_datetime(dt)
                
                # Se j√° tem timezone (TIMESTAMPTZ do PostgreSQL), converter para UTC preservando o valor
                if isinstance(dt_parsed, pd.Timestamp) and dt_parsed.tz is not None:
                    # Converter para UTC mantendo o valor absoluto correto
                    dt_utc = dt_parsed.tz_convert('UTC')
                    # Remover timezone info para BigQuery (ele armazena como UTC internamente)
                    return dt_utc.tz_localize(None)
                elif isinstance(dt_parsed, pd.Timestamp):
                    # Se n√£o tem timezone, pode ser que o PostgreSQL retornou sem timezone
                    # Neste caso, assumir que j√° est√° no timezone do servidor (Brasil -03:00)
                    # e converter para UTC
                    from datetime import timezone, timedelta
                    tz_brasil = timezone(timedelta(hours=-3))
                    dt_com_tz = dt_parsed.tz_localize(tz_brasil)
                    dt_utc = dt_com_tz.tz_convert('UTC')
                    return dt_utc.tz_localize(None)
                else:
                    return dt_parsed
            except Exception as e:
                print(f"      ‚ö†Ô∏è  Erro ao processar timestamp: {e}")
                return None
        
        def formatar_dia_original(dt):
            """Formata datetime no formato exato da NIMBUS: 2009-02-16 02:12:20.000 -0300
            
            Preserva o formato STRING original como vem do banco da NIMBUS/servidor166.
            """
            if pd.isna(dt):
                return None
            try:
                # Se j√° √© string no formato correto, retornar como est√°
                if isinstance(dt, str):
                    # Verificar se j√° est√° no formato correto (tem timezone no final)
                    if len(dt) > 10 and (dt[-5:].startswith('-') or dt[-5:].startswith('+')):
                        return dt
                    # Tentar converter
                    dt_parsed = pd.to_datetime(dt)
                elif isinstance(dt, pd.Timestamp):
                    dt_parsed = dt
                else:
                    dt_parsed = pd.to_datetime(dt)
                
                # Extrair timezone offset
                offset_str = "-0300"  # Padr√£o Brasil
                if isinstance(dt_parsed, pd.Timestamp):
                    if dt_parsed.tz is not None:
                        offset = dt_parsed.tz.utcoffset(dt_parsed)
                        if offset:
                            total_seconds = offset.total_seconds()
                            hours = int(total_seconds // 3600)
                            minutes = int((abs(total_seconds) % 3600) // 60)
                            # Formato: -0300 (sem dois pontos, como na NIMBUS)
                            offset_str = f"{hours:+03d}{minutes:02d}"
                
                # Formatar: 2009-02-16 02:12:20.000 -0300
                timestamp_str = dt_parsed.strftime('%Y-%m-%d %H:%M:%S')
                if isinstance(dt_parsed, pd.Timestamp) and dt_parsed.microsecond:
                    # Pegar apenas os 3 primeiros d√≠gitos dos microsegundos
                    microsec_str = str(dt_parsed.microsecond)[:3].zfill(3)
                    timestamp_str += f".{microsec_str}"
                else:
                    timestamp_str += ".000"
                
                return f"{timestamp_str} {offset_str}"
            except Exception:
                return None
        
        def extrair_utc_offset(dt):
            """Extrai o offset UTC do timestamp original (ex: -0300, -0200).
            
            Retorna apenas o offset como string no formato -0300 ou -0200.
            """
            if pd.isna(dt):
                return None
            try:
                # Se j√° √© string no formato correto, extrair offset
                if isinstance(dt, str):
                    # Verificar se j√° est√° no formato correto (tem timezone no final)
                    if len(dt) > 10 and (dt[-5:].startswith('-') or dt[-5:].startswith('+')):
                        # Extrair os √∫ltimos 5 caracteres (ex: -0300)
                        return dt[-5:]
                    # Tentar converter
                    dt_parsed = pd.to_datetime(dt)
                elif isinstance(dt, pd.Timestamp):
                    dt_parsed = dt
                else:
                    dt_parsed = pd.to_datetime(dt)
                
                # Extrair timezone offset
                offset_str = "-0300"  # Padr√£o Brasil
                if isinstance(dt_parsed, pd.Timestamp):
                    if dt_parsed.tz is not None:
                        offset = dt_parsed.tz.utcoffset(dt_parsed)
                        if offset:
                            total_seconds = offset.total_seconds()
                            hours = int(total_seconds // 3600)
                            minutes = int((abs(total_seconds) % 3600) // 60)
                            # Formato: -0300 (sem dois pontos, como na NIMBUS)
                            offset_str = f"{hours:+03d}{minutes:02d}"
                
                return offset_str
            except Exception:
                return None
        
        # Processar todas as colunas: dia (TIMESTAMP), dia_original (STRING) e utc_offset (STRING)
        chunk_df['dia_original'] = chunk_df['dia'].apply(formatar_dia_original)
        chunk_df['utc_offset'] = chunk_df['dia'].apply(extrair_utc_offset)
        chunk_df['dia'] = chunk_df['dia'].apply(processar_dia_timestamp)
        
        if 'estacao_id' in chunk_df.columns:
            chunk_df['estacao_id'] = chunk_df['estacao_id'].astype('Int64')
        
        # Converter colunas num√©ricas se existirem
        colunas_numericas = ['chuva', 'dirVento', 'velVento', 'temperatura', 'pressao', 'umidade']
        for col in colunas_numericas:
            if col in chunk_df.columns:
                chunk_df[col] = pd.to_numeric(chunk_df[col], errors='coerce').astype('float64')
        
        # Filtrar registros com dia NULL
        registros_antes = len(chunk_df)
        chunk_df = chunk_df[chunk_df['dia'].notna()]
        registros_depois = len(chunk_df)
        if registros_antes != registros_depois:
            print(f"      ‚ö†Ô∏è  Removidos {registros_antes - registros_depois} registros com dia NULL")
        
        # IMPORTANTE: N√ÉO converter novamente para datetime aqui!
        # A fun√ß√£o processar_dia_timestamp() j√° retorna o tipo correto (datetime sem timezone)
        # Converter novamente pode causar problemas de precis√£o (nanossegundos vs microssegundos)
        
        if len(chunk_df) > 0:
            chunks_list.append(chunk_df)
        total_registros += len(chunk_df)
        chunk_numero += 1
        
        # Escrever batch em arquivo Parquet
        if len(chunks_list) >= batch_size:
            df_batch = pd.concat(chunks_list, ignore_index=True)
            batch_file = Path(temp_dir) / f'{table_id}_batch_{batch_file_num:04d}.parquet'
            df_batch.to_parquet(
                batch_file, 
                index=False, 
                engine='pyarrow', 
                compression='snappy',
                coerce_timestamps='us'
            )
            parquet_files.append(batch_file)
            print(f"      üíæ Batch {batch_file_num} salvo: {batch_file.stat().st_size / (1024*1024):.2f} MB")
            chunks_list.clear()
            del df_batch
            gc.collect()
            batch_file_num += 1
    
    # Escrever chunks restantes
    if chunks_list:
        df_batch = pd.concat(chunks_list, ignore_index=True)
        df_batch = df_batch[df_batch['dia'].notna()]
        if len(df_batch) > 0:
            batch_file = Path(temp_dir) / f'{table_id}_batch_{batch_file_num:04d}.parquet'
            df_batch.to_parquet(
                batch_file, 
                index=False, 
                engine='pyarrow', 
                compression='snappy',
                coerce_timestamps='us'
            )
            parquet_files.append(batch_file)
            print(f"      üíæ Batch {batch_file_num} salvo: {batch_file.stat().st_size / (1024*1024):.2f} MB")
            del df_batch
            gc.collect()
    
    if total_registros == 0:
        print(f"   ‚ö†Ô∏è  Nenhum dado {descricao} encontrado!")
        return 0
    
    total_size = sum(f.stat().st_size for f in parquet_files) / (1024*1024)
    print(f"\n   ‚úÖ {len(parquet_files)} arquivos Parquet criados: {total_size:.2f} MB total")
    
    tempo_query = (datetime.now() - inicio_query).total_seconds()
    print(f"   ‚úÖ Dados processados: {total_registros:,} registros em {tempo_query:.1f} segundos")
    
    # Carregar arquivos Parquet no BigQuery
    print(f"\nüì§ Carregando {len(parquet_files)} arquivos Parquet no BigQuery...")
    print(f"   Tabela: {client_bq.project}.{dataset_id}.{table_id}")
    
    inicio_carga = datetime.now()
    
    for i, parquet_file in enumerate(parquet_files, 1):
        print(f"   üì§ Carregando arquivo {i}/{len(parquet_files)}: {parquet_file.name}...")
        file_job_config = bigquery.LoadJobConfig(
            schema=schema,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE if i == 1 else bigquery.WriteDisposition.WRITE_APPEND,
            source_format=bigquery.SourceFormat.PARQUET,
        )
        
        with open(parquet_file, 'rb') as source_file:
            job = client_bq.load_table_from_file(
                source_file,
                table_ref,
                job_config=file_job_config
            )
            job.result()
            print(f"      ‚úÖ Arquivo {i}/{len(parquet_files)} carregado com sucesso")
    
    tempo_carga = (datetime.now() - inicio_carga).total_seconds()
    
    # Limpar arquivos tempor√°rios
    print(f"   üßπ Limpando arquivos tempor√°rios...")
    for parquet_file in parquet_files:
        try:
            parquet_file.unlink()
        except Exception:
            pass
    try:
        os.rmdir(temp_dir)
    except Exception:
        pass
    
    print(f"\n   ‚úÖ Tabela '{table_id}' carregada: {total_registros:,} registros em {tempo_carga:.1f} segundos")
    
    return total_registros

def exportar_para_bigquery():
    """Exporta dados meteorol√≥gicos do NIMBUS diretamente para BigQuery."""
    engine_nimbus = None
    client_bq = None
    
    try:
        print("\nüîÑ Iniciando exporta√ß√£o direta NIMBUS ‚Üí BigQuery (DADOS METEOROL√ìGICOS)...")
        print(f"   Origem: alertadb @ NIMBUS")
        print(f"   Destino: BigQuery ({BIGQUERY_CONFIG['project_id']}.{BIGQUERY_CONFIG['dataset_id']}.{BIGQUERY_CONFIG['table_id']})")
        print()
        
        # Conectar ao NIMBUS
        print("üì¶ Conectando ao NIMBUS...")
        connection_string = (
            f"postgresql://{ORIGEM['user']}:{ORIGEM['password']}@"
            f"{ORIGEM['host']}:{ORIGEM['port']}/{ORIGEM['dbname']}"
        )
        engine_nimbus = create_engine(
            connection_string,
            connect_args={'client_encoding': 'UTF8'},
            pool_pre_ping=True
        )
        
        # Conectar ao BigQuery
        print("üì¶ Conectando ao BigQuery...")
        credentials_path = BIGQUERY_CONFIG.get('credentials_path')
        
        if not credentials_path or not Path(credentials_path).exists():
            credentials_padrao = project_root / 'credentials' / 'credentials.json'
            print(f"   üîç Verificando caminho padr√£o: {credentials_padrao}")
            if credentials_padrao.exists():
                credentials_path = credentials_padrao
                print(f"   ‚úÖ Arquivo encontrado no caminho padr√£o!")
            else:
                print(f"   ‚ö†Ô∏è  Arquivo n√£o encontrado no caminho padr√£o")
        
        if credentials_path and Path(credentials_path).exists():
            print(f"   üîë Usando credenciais: {credentials_path}")
            credentials = service_account.Credentials.from_service_account_file(
                str(credentials_path)
            )
            client_bq = bigquery.Client(
                project=BIGQUERY_CONFIG['project_id'],
                credentials=credentials
            )
            print("   ‚úÖ Credenciais carregadas com sucesso!")
        else:
            raise FileNotFoundError(
                f"‚ùå Arquivo de credenciais n√£o encontrado!\n"
                f"   Caminho padr√£o: {project_root / 'credentials' / 'credentials.json'}\n"
                f"   üí° Coloque o arquivo credentials.json em: {project_root / 'credentials' / 'credentials.json'}"
            )
        
        # Criar dataset se n√£o existir
        criar_dataset_se_nao_existir(client_bq, BIGQUERY_CONFIG['dataset_id'])
        
        # Criar tabela
        print("\nüìã Criando/verificando tabela no BigQuery...")
        schema_meteorologicos = obter_schema_meteorologicos()
        criar_tabela_com_schema(
            client_bq, 
            BIGQUERY_CONFIG['dataset_id'], 
            BIGQUERY_CONFIG['table_id'], 
            schema_meteorologicos
        )
        
        # Processar tabela meteorologicos
        print("\n" + "=" * 80)
        print("üå§Ô∏è PROCESSANDO TABELA: meteorologicos")
        print("=" * 80)
        
        query_meteorologicos = query_todos_dados_meteorologicos()
        total_meteorologicos = processar_e_carregar_tabela(
            engine_nimbus=engine_nimbus,
            client_bq=client_bq,
            dataset_id=BIGQUERY_CONFIG['dataset_id'],
            table_id=BIGQUERY_CONFIG['table_id'],
            schema=schema_meteorologicos,
            query=query_meteorologicos,
            descricao="meteorol√≥gicos"
        )
        
        # Resumo final
        print("\n" + "=" * 80)
        print("‚úÖ EXPORTA√á√ÉO CONCLU√çDA")
        print("=" * 80)
        print(f"üå§Ô∏è meteorologicos: {total_meteorologicos:,} registros")
        
        return total_meteorologicos

    except Exception as e:
        print(f'\n‚ùå Erro na exporta√ß√£o: {e}')
        import traceback
        traceback.print_exc()
        return 0

    finally:
        if engine_nimbus:
            engine_nimbus.dispose()

def main():
    """Fun√ß√£o principal."""
    print("=" * 70)
    print("üå§Ô∏è EXPORTA√á√ÉO DIRETA - DADOS METEOROL√ìGICOS NIMBUS ‚Üí BigQuery")
    print("=" * 70)
    print()
    print("üéØ PROP√ìSITO:")
    print("   Exportar TODOS os dados METEOROL√ìGICOS diretamente do NIMBUS")
    print("   para o BigQuery (pulando todas as camadas intermedi√°rias)")
    print()
    print("üìã O QUE SER√Å FEITO:")
    print("   ‚úÖ Buscar TODOS os dados meteorol√≥gicos do NIMBUS (desde 1997)")
    print("   ‚úÖ Usar DISTINCT ON (mesma l√≥gica dos dados pluviom√©tricos)")
    print("   ‚úÖ Criar dataset/tabela no BigQuery se n√£o existir")
    print("   ‚úÖ Exportar em formato Parquet completo")
    print("   ‚úÖ Carregar no BigQuery automaticamente")
    print()
    print("‚ö†Ô∏è  IMPORTANTE:")
    print("   - Requer credenciais do GCP configuradas")
    print("   - Formato: Parquet (mais eficiente para BigQuery)")
    print("   - Exporta√ß√£o completa: todos os dados")
    print("   - Query usa DISTINCT ON para garantir unicidade")
    print("=" * 70)
    
    # Testar conex√£o
    if not testar_conexao_nimbus():
        print("\n‚ùå N√£o foi poss√≠vel conectar ao NIMBUS. Verifique as configura√ß√µes.")
        return
    
    # Exportar
    exportar_para_bigquery()
    
    print("\nüí° PR√ìXIMO PASSO:")
    print("   Configure exporta√ß√£o peri√≥dica ou use este script")
    print("   quando precisar atualizar os dados no BigQuery.\n")

if __name__ == '__main__':
    main()

