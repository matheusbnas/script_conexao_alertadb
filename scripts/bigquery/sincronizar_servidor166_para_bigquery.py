#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üåßÔ∏è SINCRONIZA√á√ÉO INCREMENTAL - Servidor 166 ‚Üí BigQuery

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéØ PROP√ìSITO DESTE SCRIPT:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Este script sincroniza APENAS os dados NOVOS desde a √∫ltima sincroniza√ß√£o
do banco alertadb_cor (servidor 166) para o BigQuery, executando de forma incremental.

ARQUITETURA:
    NIMBUS ‚Üí Servidor 166 (alertadb_cor) ‚Üí Parquet ‚Üí BigQuery (incremental)
              ‚Üë [ESTE SCRIPT - SINCRONIZA√á√ÉO INCREMENTAL]
              Dataset: alertadb_cor166_raw (identifica origem: NIMBUS ‚Üí servidor166 ‚Üí BigQuery)

VANTAGENS:
    ‚úÖ Sincroniza√ß√£o incremental (apenas dados novos)
    ‚úÖ Execu√ß√£o r√°pida (n√£o processa todos os dados)
    ‚úÖ Ideal para cron (executa a cada 5 minutos)
    ‚úÖ BigQuery sempre atualizado
    ‚úÖ Formato Parquet (otimizado)
    ‚úÖ Controle total dos dados (voc√™ √© admin do banco)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã O QUE ESTE SCRIPT FAZ:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ Cria dataset alertadb_cor166_raw se n√£o existir
‚úÖ Cria tabela pluviometricos com TIMESTAMP (igual ao servidor166)
‚úÖ Busca √∫ltimo timestamp no BigQuery (MAX(dia))
‚úÖ Busca APENAS dados novos desde esse timestamp no servidor 166
‚úÖ Exporta para formato Parquet
‚úÖ Carrega no BigQuery usando WRITE_APPEND
‚úÖ Processa em lotes para otimizar mem√≥ria
‚úÖ Usa TIMESTAMP para coluna dia (igual ao servidor166)
‚úÖ Converte timezone para UTC (padr√£o BigQuery)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ö†Ô∏è QUANDO USAR ESTE SCRIPT:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ AP√ìS executar exportar_servidor166_para_bigquery.py (carga inicial)
‚úÖ Para manter os dados atualizados automaticamente via cron
‚úÖ Em produ√ß√£o/servidor para sincroniza√ß√£o cont√≠nua
‚úÖ Quando voc√™ precisa de dados atualizados a cada 5 minutos

‚ö†Ô∏è N√ÉO USE se:
   ‚ùå A tabela BigQuery estiver vazia (use exportar_servidor166_para_bigquery.py primeiro)
   ‚ùå Voc√™ quer carregar dados hist√≥ricos (use exportar_servidor166_para_bigquery.py)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üöÄ COMO USAR:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1. PRIMEIRO: Execute exportar_servidor166_para_bigquery.py para carga inicial
2. Configure o arquivo .env com as credenciais
3. Execute: python sincronizar_servidor166_para_bigquery.py --once
4. Configure cron para executar a cada 5 minutos

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import psycopg2
import pandas as pd
from sqlalchemy import create_engine
from google.cloud import bigquery
from google.oauth2 import service_account
import os
import sys
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from pathlib import Path
import tempfile
import gc
from urllib.parse import quote_plus

# Carregar vari√°veis de ambiente
project_root = Path(__file__).parent.parent.parent
load_dotenv(dotenv_path=project_root / '.env')

def obter_variavel(nome, obrigatoria=True, padrao=None):
    """Obt√©m vari√°vel de ambiente."""
    valor = os.getenv(nome)
    if not valor or (isinstance(valor, str) and valor.strip() == ''):
        if obrigatoria:
            raise ValueError(f"‚ùå Vari√°vel obrigat√≥ria n√£o encontrada: {nome}")
        return padrao
    return valor.strip() if isinstance(valor, str) else valor

def carregar_configuracoes():
    """Carrega configura√ß√µes do .env."""
    try:
        # Banco ORIGEM - Servidor 166 (alertadb_cor)
        origem = {
            'host': obter_variavel('DB_DESTINO_HOST'),
            'port': obter_variavel('DB_DESTINO_PORT', obrigatoria=False, padrao='5432'),
            'dbname': obter_variavel('DB_DESTINO_NAME'),
            'user': obter_variavel('DB_DESTINO_USER'),
            'password': obter_variavel('DB_DESTINO_PASSWORD'),
            'sslmode': obter_variavel('DB_DESTINO_SSLMODE', obrigatoria=False, padrao='disable'),
            'connect_timeout': 10
        }

        # BigQuery
        # Dataset espec√≠fico para dados do servidor166 (NIMBUS ‚Üí servidor166 ‚Üí BigQuery)
        bigquery_config = {
            'project_id': obter_variavel('BIGQUERY_PROJECT_ID'),
            'dataset_id': obter_variavel('BIGQUERY_DATASET_ID_SERVIDOR166', obrigatoria=False, padrao='alertadb_166_raw'),
            'table_id': obter_variavel('BIGQUERY_TABLE_ID', obrigatoria=False, padrao='pluviometricos'),
            'credentials_path': obter_variavel('BIGQUERY_CREDENTIALS_PATH', obrigatoria=False),
        }

        return origem, bigquery_config
    except Exception as e:
        print(f"‚ùå Erro ao carregar configura√ß√µes: {e}")
        raise

def obter_credenciais_bigquery(credentials_path=None):
    """Obt√©m credenciais do BigQuery."""
    if credentials_path and os.path.exists(credentials_path):
        return service_account.Credentials.from_service_account_file(credentials_path)
    
    # Tentar encontrar credentials.json na pasta credentials/
    credentials_file = project_root / 'credentials' / 'credentials.json'
    if credentials_file.exists():
        return service_account.Credentials.from_service_account_file(str(credentials_file))
    
    # Tentar usar credenciais padr√£o do ambiente
    try:
        return None  # BigQuery usar√° credenciais padr√£o do ambiente
    except Exception:
        raise ValueError("‚ùå Credenciais do BigQuery n√£o encontradas. Configure BIGQUERY_CREDENTIALS_PATH ou coloque credentials.json em credentials/")

def testar_conexao_servidor166(origem):
    """Testa conex√£o com banco servidor 166."""
    try:
        # Codificar usu√°rio e senha para URL (trata caracteres especiais)
        user_encoded = quote_plus(origem['user'])
        password_encoded = quote_plus(origem['password'])
        
        engine = create_engine(
            f"postgresql://{user_encoded}:{password_encoded}@{origem['host']}:{origem['port']}/{origem['dbname']}",
            connect_args={'sslmode': origem['sslmode'], 'connect_timeout': origem['connect_timeout']}
        )
        with engine.connect() as conn:
            conn.execute("SELECT 1")
        return True
    except Exception as e:
        print(f"‚ùå Erro ao conectar ao servidor 166: {e}")
        return False

def criar_dataset_se_nao_existir(client, dataset_id):
    """Cria dataset no BigQuery se n√£o existir."""
    try:
        dataset_ref = client.dataset(dataset_id)
        try:
            client.get_dataset(dataset_ref)
            print(f"‚úÖ Dataset '{dataset_id}' j√° existe.")
        except Exception:
            # Dataset n√£o existe, criar
            dataset = bigquery.Dataset(dataset_ref)
            dataset.location = "US"  # Ou outra regi√£o conforme necess√°rio
            dataset.description = f"Dataset para dados do servidor166 (NIMBUS ‚Üí servidor166 ‚Üí BigQuery)"
            dataset = client.create_dataset(dataset, exists_ok=False)
            print(f"‚úÖ Dataset '{dataset_id}' criado com sucesso!")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar dataset: {e}")
        return False

def criar_tabela_com_schema(client, dataset_id, table_id, schema):
    """Cria tabela no BigQuery com schema e particionamento se n√£o existir."""
    try:
        table_ref = client.dataset(dataset_id).table(table_id)
        
        try:
            table = client.get_table(table_ref)
            print(f"‚úÖ Tabela '{table_id}' j√° existe com schema ({len(table.schema)} campos).")
            return True
        except Exception:
            # Tabela n√£o existe, criar
            pass
        
        # Criar tabela com schema e particionamento
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "Dados pluviom√©tricos do servidor166 (NIMBUS ‚Üí servidor166 ‚Üí BigQuery)"
        # Como dia √© TIMESTAMP, usar particionamento por coluna
        table.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="dia"  # Particionamento por coluna dia (TIMESTAMP)
        )
        table = client.create_table(table, exists_ok=False)
        print(f"‚úÖ Tabela '{table_id}' criada com schema e particionamento por coluna 'dia'!")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar tabela: {e}")
        import traceback
        traceback.print_exc()
        return False

def obter_ultima_sincronizacao_bigquery(client, dataset_id, table_id):
    """Obt√©m o √∫ltimo timestamp sincronizado do BigQuery (TIMESTAMP)."""
    try:
        # Como dia √© TIMESTAMP, podemos usar MAX diretamente
        query = f"""
        SELECT MAX(dia) as ultima_sincronizacao
        FROM `{client.project}.{dataset_id}.{table_id}`
        """
        
        query_job = client.query(query)
        results = query_job.result()
        
        for row in results:
            if row.ultima_sincronizacao:
                # Converter para datetime com timezone
                ultima_sync = row.ultima_sincronizacao
                if isinstance(ultima_sync, datetime):
                    # BigQuery retorna TIMESTAMP em UTC, mas sem timezone info
                    # Assumir UTC
                    if ultima_sync.tzinfo is None:
                        ultima_sync = ultima_sync.replace(tzinfo=timezone.utc)
                    return ultima_sync
            break
        
        # Se n√£o encontrou, retornar data de refer√™ncia (1997-01-01)
        return datetime(1997, 1, 1, tzinfo=timezone.utc)
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao obter √∫ltima sincroniza√ß√£o: {e}")
        # Retornar data de refer√™ncia em caso de erro
        return datetime(1997, 1, 1, tzinfo=timezone.utc)

def processar_dia_timestamp(dt):
    """Processa datetime para TIMESTAMP do BigQuery (UTC) - igual ao servidor166"""
    if pd.isna(dt):
        return None
    try:
        # Converter para datetime preservando timezone
        if isinstance(dt, str):
            dt_parsed = pd.to_datetime(dt)
        elif isinstance(dt, pd.Timestamp):
            dt_parsed = dt
        else:
            dt_parsed = pd.to_datetime(dt)
        
        # Se tem timezone, converter para UTC (BigQuery armazena em UTC)
        if isinstance(dt_parsed, pd.Timestamp) and dt_parsed.tz is not None:
            # Converter para UTC
            dt_utc = dt_parsed.tz_convert('UTC')
            # Remover timezone para BigQuery (ele armazena como UTC internamente)
            return dt_utc.tz_localize(None)
        elif isinstance(dt_parsed, pd.Timestamp):
            # Sem timezone, assumir que j√° est√° no timezone correto
            # Converter para UTC assumindo que est√° em America/Sao_Paulo
            from datetime import timezone, timedelta
            tz_brasil = timezone(timedelta(hours=-3))
            dt_com_tz = dt_parsed.tz_localize(tz_brasil)
            dt_utc = dt_com_tz.tz_convert('UTC')
            return dt_utc.tz_localize(None)
        else:
            return dt_parsed
    except Exception as e:
        return None

def query_dados_incrementais(ultima_sincronizacao):
    """Retorna query para buscar apenas dados novos desde a √∫ltima sincroniza√ß√£o."""
    # Converter timestamp UTC do BigQuery para timestamp do PostgreSQL (servidor166)
    # O servidor 166 tem coluna dia como TIMESTAMP (sem timezone), ent√£o precisamos converter
    if isinstance(ultima_sincronizacao, datetime):
        # Se tem timezone, converter para UTC primeiro
        if ultima_sincronizacao.tzinfo:
            # Converter para UTC
            utc_time = ultima_sincronizacao.astimezone(timezone.utc)
            # Converter para timezone do Brasil para comparar com dados do servidor166
            from datetime import timedelta
            tz_brasil = timezone(timedelta(hours=-3))
            brasil_time = utc_time.astimezone(tz_brasil)
            # Formatar para PostgreSQL (sem timezone, pois servidor166 usa TIMESTAMP sem timezone)
            timestamp_str = brasil_time.strftime('%Y-%m-%d %H:%M:%S')
        else:
            # Sem timezone, assumir que j√° est√° em UTC e converter para Brasil
            from datetime import timedelta
            tz_utc = timezone.utc
            tz_brasil = timezone(timedelta(hours=-3))
            dt_utc = ultima_sincronizacao.replace(tzinfo=tz_utc)
            brasil_time = dt_utc.astimezone(tz_brasil)
            timestamp_str = brasil_time.strftime('%Y-%m-%d %H:%M:%S')
    else:
        timestamp_str = str(ultima_sincronizacao)
    
    # Servidor166 usa TIMESTAMP (sem timezone), ent√£o usar timestamp simples
    return f"""
SELECT 
    dia,
    m05,
    m10,
    m15,
    h01,
    h04,
    h24,
    h96,
    estacao,
    estacao_id
FROM pluviometricos
WHERE dia > '{timestamp_str}'::timestamp
ORDER BY dia ASC, estacao_id ASC;
"""

def sincronizar_incremental():
    """Sincroniza apenas dados novos do servidor 166 para BigQuery."""
    ORIGEM, BIGQUERY_CONFIG = carregar_configuracoes()
    
    # Obter credenciais BigQuery
    credentials = obter_credenciais_bigquery(BIGQUERY_CONFIG.get('credentials_path'))
    
    # Criar cliente BigQuery
    if credentials:
        client = bigquery.Client(credentials=credentials, project=BIGQUERY_CONFIG['project_id'])
    else:
        client = bigquery.Client(project=BIGQUERY_CONFIG['project_id'])
    
    dataset_id = BIGQUERY_CONFIG['dataset_id']
    table_id = BIGQUERY_CONFIG['table_id']
    table_ref = f"{BIGQUERY_CONFIG['project_id']}.{dataset_id}.{table_id}"
    
    print("=" * 80)
    print("üåßÔ∏è SINCRONIZA√á√ÉO INCREMENTAL - Servidor 166 ‚Üí BigQuery")
    print("=" * 80)
    print(f"üìä Dataset: {dataset_id}")
    print(f"üìã Tabela: {table_id}")
    print("=" * 80)
    
    # Testar conex√£o servidor 166
    print("\nüîç Testando conex√£o com servidor 166...")
    if not testar_conexao_servidor166(ORIGEM):
        print("‚ùå Falha na conex√£o com servidor 166!")
        return False
    
    print("‚úÖ Conex√£o com servidor 166: OK")
    
    # Criar dataset se n√£o existir
    print("\nüìã Verificando dataset...")
    criar_dataset_se_nao_existir(client, dataset_id)
    
    # Criar tabela se n√£o existir
    print("\nüìã Verificando tabela...")
    schema = [
        bigquery.SchemaField("dia", "TIMESTAMP", mode="REQUIRED", description="Data e hora em que foi realizada a medi√ß√£o (no formato Y-m-d H:M:S)"),
        bigquery.SchemaField("m05", "FLOAT64", mode="NULLABLE"),
        bigquery.SchemaField("m10", "FLOAT64", mode="NULLABLE"),
        bigquery.SchemaField("m15", "FLOAT64", mode="NULLABLE"),
        bigquery.SchemaField("h01", "FLOAT64", mode="NULLABLE"),
        bigquery.SchemaField("h04", "FLOAT64", mode="NULLABLE"),
        bigquery.SchemaField("h24", "FLOAT64", mode="NULLABLE"),
        bigquery.SchemaField("h96", "FLOAT64", mode="NULLABLE"),
        bigquery.SchemaField("estacao", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("estacao_id", "INTEGER", mode="REQUIRED"),
    ]
    criar_tabela_com_schema(client, dataset_id, table_id, schema)
    
    # Obter √∫ltima sincroniza√ß√£o do BigQuery
    print("\nüîç Obtendo √∫ltima sincroniza√ß√£o do BigQuery...")
    ultima_sincronizacao = obter_ultima_sincronizacao_bigquery(client, dataset_id, table_id)
    
    if ultima_sincronizacao == datetime(1997, 1, 1, tzinfo=timezone.utc):
        print("‚ö†Ô∏è  Tabela BigQuery est√° vazia ou n√£o encontrada!")
        print("   Execute PRIMEIRO: python scripts/bigquery/exportar_servidor166_para_bigquery.py")
        print("   para fazer a carga inicial dos dados hist√≥ricos.")
        return False
    
    print(f"‚úÖ √öltima sincroniza√ß√£o: {ultima_sincronizacao.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    
    # Criar engine SQLAlchemy para pandas
    # Codificar usu√°rio e senha para URL (trata caracteres especiais)
    user_encoded = quote_plus(ORIGEM['user'])
    password_encoded = quote_plus(ORIGEM['password'])
    
    engine = create_engine(
        f"postgresql://{user_encoded}:{password_encoded}@{ORIGEM['host']}:{ORIGEM['port']}/{ORIGEM['dbname']}",
        connect_args={'sslmode': ORIGEM['sslmode'], 'connect_timeout': ORIGEM['connect_timeout']}
    )
    
    # Buscar dados incrementais
    print(f"\nüîç Buscando dados novos desde {ultima_sincronizacao.strftime('%Y-%m-%d %H:%M:%S %Z')}...")
    query = query_dados_incrementais(ultima_sincronizacao)
    
    # Processar em chunks (reduzido para evitar problemas de mem√≥ria)
    chunksize = 10000  # Reduzido de 25000 para evitar erro de mem√≥ria
    total_registros = 0
    parquet_files = []
    
    try:
        # Ler dados em chunks
        for chunk_num, chunk_df in enumerate(pd.read_sql(query, engine, chunksize=chunksize), 1):
            if chunk_df.empty:
                print("   ‚ÑπÔ∏è  Nenhum dado novo encontrado.")
                break
            
            # Processar coluna dia como TIMESTAMP (igual ao servidor166)
            # Converter para UTC (BigQuery armazena em UTC)
            chunk_df['dia'] = chunk_df['dia'].apply(processar_dia_timestamp)
            
            chunk_df['estacao_id'] = chunk_df['estacao_id'].astype('Int64')
            
            # Converter colunas num√©ricas
            numeric_cols = ['m05', 'm10', 'm15', 'h01', 'h04', 'h24', 'h96']
            for col in numeric_cols:
                chunk_df[col] = pd.to_numeric(chunk_df[col], errors='coerce').astype('float64')
            
            # Filtrar registros com dia NULL (BigQuery n√£o aceita NULL em campo REQUIRED)
            chunk_df = chunk_df[chunk_df['dia'].notna()]
            
            if len(chunk_df) == 0:
                continue
            
            # Converter dia para datetime64[us] para Parquet (precis√£o de microsegundos)
            chunk_df['dia'] = pd.to_datetime(chunk_df['dia'], errors='coerce')
            
            # Salvar chunk em Parquet tempor√°rio
            with tempfile.NamedTemporaryFile(delete=False, suffix='.parquet') as tmp_file:
                tmp_path = tmp_file.name
            
            chunk_df.to_parquet(
                tmp_path, 
                index=False,
                engine='pyarrow',
                compression='snappy',
                coerce_timestamps='us'  # Precis√£o de microsegundos (igual ao servidor166)
            )
            parquet_files.append(tmp_path)
            
            total_registros += len(chunk_df)
            print(f"   üì¶ Chunk {chunk_num}: {len(chunk_df):,} registros processados (Total: {total_registros:,})")
            
            # Limpar mem√≥ria
            del chunk_df
            gc.collect()
        
        if total_registros == 0:
            print("\n‚úÖ Nenhum dado novo para sincronizar.")
            return True
        
        # Carregar arquivos Parquet no BigQuery
        print(f"\nüì§ Carregando {total_registros:,} registros no BigQuery...")
        
        # Schema do BigQuery - MESMA estrutura do servidor166
        # Coluna dia como TIMESTAMP (igual ao servidor166 que usa TIMESTAMP)
        schema = [
            bigquery.SchemaField("dia", "TIMESTAMP", mode="REQUIRED", description="Data e hora em que foi realizada a medi√ß√£o (no formato Y-m-d H:M:S)"),
            bigquery.SchemaField("m05", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("m10", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("m15", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h01", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h04", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h24", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("h96", "FLOAT64", mode="NULLABLE"),
            bigquery.SchemaField("estacao", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("estacao_id", "INTEGER", mode="REQUIRED"),
        ]
        
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.PARQUET,
            schema=schema,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # Adiciona dados novos
        )
        
        # Carregar cada arquivo Parquet
        for i, parquet_file in enumerate(parquet_files, 1):
            print(f"   üì§ Carregando arquivo {i}/{len(parquet_files)}...")
            with open(parquet_file, 'rb') as source_file:
                job = client.load_table_from_file(
                    source_file,
                    table_ref,
                    job_config=job_config
                )
                job.result()  # Aguarda conclus√£o
            
            # Remover arquivo tempor√°rio
            os.unlink(parquet_file)
        
        print(f"\n‚úÖ Sincroniza√ß√£o conclu√≠da!")
        print(f"   üìä Total sincronizado: {total_registros:,} registros")
        
        # Obter novo √∫ltimo timestamp
        nova_ultima_sincronizacao = obter_ultima_sincronizacao_bigquery(client, dataset_id, table_id)
        print(f"   üïê √öltima sincroniza√ß√£o atualizada: {nova_ultima_sincronizacao.strftime('%Y-%m-%d %H:%M:%S %Z')}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Erro durante sincroniza√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # Limpar arquivos tempor√°rios restantes
        for parquet_file in parquet_files:
            if os.path.exists(parquet_file):
                try:
                    os.unlink(parquet_file)
                except:
                    pass

def main():
    """Fun√ß√£o principal."""
    try:
        if '--once' in sys.argv:
            # Modo √∫nico (para cron)
            sucesso = sincronizar_incremental()
            sys.exit(0 if sucesso else 1)
        else:
            # Modo interativo
            print("=" * 80)
            print("üåßÔ∏è SINCRONIZA√á√ÉO INCREMENTAL - Servidor 166 ‚Üí BigQuery")
            print("=" * 80)
            print("\n‚ö†Ô∏è  Para usar com cron, execute com --once:")
            print("   python scripts/bigquery/sincronizar_servidor166_para_bigquery.py --once")
            print("\nüîÑ Executando sincroniza√ß√£o √∫nica...\n")
            sucesso = sincronizar_incremental()
            if sucesso:
                print("\n‚úÖ Sincroniza√ß√£o conclu√≠da com sucesso!")
            else:
                print("\n‚ùå Sincroniza√ß√£o falhou!")
            sys.exit(0 if sucesso else 1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrompido pelo usu√°rio.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erro fatal: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()

