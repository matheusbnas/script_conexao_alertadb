# üìä BigQuery - Formatos de Arquivo Suportados

Guia completo sobre os formatos de arquivo que o BigQuery aceita para importa√ß√£o.

---

## ‚úÖ Formatos Suportados

O BigQuery aceita os seguintes formatos de arquivo para importa√ß√£o:

### 1. **CSV** (Comma-Separated Values)
- ‚úÖ **Suportado:** Sim
- ‚úÖ **Comprimido:** Sim (GZIP)
- üìä **Uso:** Dados tabulares simples
- ‚ö° **Performance:** Boa
- üíæ **Tamanho:** M√©dio

**Exemplo:**
```csv
dia,m05,m10,m15,h01,h04,h24,h96,estacao,estacao_id
2024-01-01 00:00:00,10.5,20.3,30.1,40.2,50.0,60.5,70.0,Esta√ß√£o A,1
```

### 2. **JSON** (JavaScript Object Notation)
- ‚úÖ **Suportado:** Sim
- ‚úÖ **Comprimido:** Sim (GZIP)
- üìä **Uso:** Dados estruturados/hier√°rquicos
- ‚ö° **Performance:** M√©dia
- üíæ **Tamanho:** Grande

**Exemplo:**
```json
[
  {
    "dia": "2024-01-01T00:00:00",
    "m05": 10.5,
    "m10": 20.3,
    "estacao": "Esta√ß√£o A",
    "estacao_id": 1
  }
]
```

### 3. **Parquet** ‚≠ê RECOMENDADO
- ‚úÖ **Suportado:** Sim
- ‚úÖ **Comprimido:** Sim (nativo)
- üìä **Uso:** Dados tabulares grandes
- ‚ö° **Performance:** **Excelente** (mais r√°pido)
- üíæ **Tamanho:** **Menor** (mais eficiente)
- üéØ **Vantagens:**
  - Formato colunar (otimizado para an√°lises)
  - Compress√£o autom√°tica
  - Preserva tipos de dados
  - Mais r√°pido para carregar

**Por que usar Parquet:**
- ‚úÖ 5-10x mais r√°pido que CSV
- ‚úÖ 50-80% menor que CSV
- ‚úÖ Preserva tipos de dados (n√£o precisa convers√£o)
- ‚úÖ Ideal para BigQuery

### 4. **Avro**
- ‚úÖ **Suportado:** Sim
- ‚úÖ **Comprimido:** Sim (nativo)
- üìä **Uso:** Dados estruturados com schema
- ‚ö° **Performance:** Boa
- üíæ **Tamanho:** M√©dio

### 5. **ORC** (Optimized Row Columnar)
- ‚úÖ **Suportado:** Sim
- ‚úÖ **Comprimido:** Sim (nativo)
- üìä **Uso:** Dados tabulares grandes
- ‚ö° **Performance:** Excelente
- üíæ **Tamanho:** Menor

---

## üìä Compara√ß√£o de Formatos

| Formato | Velocidade | Tamanho | Compress√£o | Preserva Tipos | Recomendado |
|---------|------------|---------|------------|----------------|-------------|
| **Parquet** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Nativa | ‚úÖ Sim | ‚úÖ **SIM** |
| **ORC** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Nativa | ‚úÖ Sim | ‚úÖ Sim |
| **Avro** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚úÖ Nativa | ‚úÖ Sim | ‚ö†Ô∏è M√©dio |
| **CSV** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ö†Ô∏è GZIP | ‚ùå N√£o | ‚ö†Ô∏è B√°sico |
| **JSON** | ‚≠ê‚≠ê | ‚≠ê | ‚ö†Ô∏è GZIP | ‚ö†Ô∏è Parcial | ‚ùå N√£o |

---

## üöÄ Recomenda√ß√£o para Seu Caso

### **Para Dados Pluviom√©tricos:**

**Use Parquet** porque:
- ‚úÖ Dados tabulares (perfeito para Parquet)
- ‚úÖ Volume grande (Parquet √© mais eficiente)
- ‚úÖ An√°lises no BigQuery (formato colunar otimizado)
- ‚úÖ Preserva tipos num√©ricos (m05, m10, etc.)
- ‚úÖ Compress√£o autom√°tica (menor custo de storage)

---

## üìù Como Usar Cada Formato

### 1. CSV

```python
from google.cloud import bigquery

job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.CSV,
    skip_leading_rows=1,
    autodetect=True,  # Detecta schema automaticamente
)

# Ou especificar schema manualmente
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.CSV,
    skip_leading_rows=1,
    schema=[
        bigquery.SchemaField("dia", "TIMESTAMP"),
        bigquery.SchemaField("m05", "NUMERIC"),
        # ...
    ],
)
```

### 2. JSON

```python
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
    autodetect=True,
)
```

### 3. Parquet ‚≠ê RECOMENDADO

```python
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.PARQUET,
    schema=[
        bigquery.SchemaField("dia", "TIMESTAMP", mode="REQUIRED"),
        bigquery.SchemaField("m05", "NUMERIC", mode="NULLABLE"),
        bigquery.SchemaField("m10", "NUMERIC", mode="NULLABLE"),
        # ...
    ],
    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Substitui dados
)
```

### 4. Avro

```python
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.AVRO,
)
```

### 5. ORC

```python
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.ORC,
)
```

---

## üîÑ Script Criado

Criei o script `scripts/bigquery/exportar_nimbus_para_bigquery.py` que:

- ‚úÖ Conecta diretamente ao NIMBUS
- ‚úÖ Exporta em formato **Parquet** (recomendado)
- ‚úÖ Carrega automaticamente no BigQuery
- ‚úÖ Processa em chunks (otimiza mem√≥ria)
- ‚úÖ Cria dataset/tabela automaticamente

**Uso:**
```bash
# Configurar .env
BIGQUERY_PROJECT_ID=seu-projeto-gcp
BIGQUERY_DATASET_ID=pluviometricos
BIGQUERY_TABLE_ID=pluviometricos
BIGQUERY_CREDENTIALS_PATH=/caminho/credentials.json  # Opcional

# Executar
python scripts/bigquery/exportar_nimbus_para_bigquery.py
```

---

## üìä Tamanhos Estimados

Para 1 milh√£o de registros:

| Formato | Tamanho (sem compress√£o) | Tamanho (comprimido) |
|---------|--------------------------|----------------------|
| CSV | ~100 MB | ~20 MB (GZIP) |
| JSON | ~150 MB | ~25 MB (GZIP) |
| **Parquet** | **~15 MB** | **~15 MB** (nativo) |
| Avro | ~20 MB | ~18 MB (nativo) |
| ORC | ~12 MB | ~12 MB (nativo) |

**Parquet √© 5-7x menor que CSV comprimido!**

---

## ‚ö° Performance de Carga

Para 1 milh√£o de registros:

| Formato | Tempo de Carga |
|---------|----------------|
| CSV | ~30-60 segundos |
| JSON | ~45-90 segundos |
| **Parquet** | **~5-10 segundos** |
| Avro | ~8-15 segundos |
| ORC | ~5-10 segundos |

**Parquet √© 5-10x mais r√°pido!**

---

## üéØ Conclus√£o

**Para seus dados pluviom√©tricos:**

1. ‚úÖ **Use Parquet** - Mais r√°pido e eficiente
2. ‚úÖ **Script j√° criado** - `exportar_nimbus_para_bigquery.py`
3. ‚úÖ **Formato otimizado** - Ideal para BigQuery

**Outros formatos s√£o suportados, mas Parquet √© claramente a melhor escolha!**

---

**√öltima atualiza√ß√£o:** 2025

