# ğŸ“Š BigQuery - Guia Completo

Guia completo para integraÃ§Ã£o com Google BigQuery, incluindo configuraÃ§Ã£o, credenciais, exportaÃ§Ã£o, visualizaÃ§Ã£o e automaÃ§Ã£o.

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [ConfiguraÃ§Ã£o Inicial](#configuraÃ§Ã£o-inicial)
3. [Obter Credenciais](#obter-credenciais)
4. [Configurar VariÃ¡veis](#configurar-variÃ¡veis)
5. [ExportaÃ§Ã£o de Dados](#exportaÃ§Ã£o-de-dados)
6. [Formatos Suportados](#formatos-suportados)
7. [Visualizar Dados](#visualizar-dados)
8. [Formato dos Dados](#formato-dos-dados)
9. [AutomaÃ§Ã£o](#automaÃ§Ã£o)
10. [Compartilhar Acesso](#compartilhar-acesso)

---

## ğŸ¯ VisÃ£o Geral

### OpÃ§Ãµes de IntegraÃ§Ã£o

**OpÃ§Ã£o 1: NIMBUS â†’ BigQuery (Direto)** â­ RECOMENDADO
- Exporta diretamente do NIMBUS para BigQuery
- Mais rÃ¡pido (menos camadas)
- Script: `scripts/bigquery/exportar_pluviometricos_nimbus_bigquery.py`

**OpÃ§Ã£o 2: Cloud SQL â†’ BigQuery (Federated Queries)**
- Consulta dados do Cloud SQL diretamente no BigQuery
- Sem necessidade de copiar dados
- Dados sempre atualizados

**OpÃ§Ã£o 3: Cloud SQL â†’ BigQuery (ExportaÃ§Ã£o)**
- Exporta dados do Cloud SQL para BigQuery
- Dados em BigQuery (mais rÃ¡pido para consultas)
- Requer sincronizaÃ§Ã£o periÃ³dica

---

## âš™ï¸ ConfiguraÃ§Ã£o Inicial

### PrÃ©-requisitos

1. **Conta Google Cloud Platform** ativa
2. **Projeto GCP** criado
3. **Python 3.8+** instalado
4. **Bibliotecas Python** instaladas:
   ```bash
   pip install google-cloud-bigquery google-auth pyarrow pandas
   ```

### Script Automatizado

O script `scripts/bigquery/exportar_pluviometricos_nimbus_bigquery.py` faz automaticamente:
1. âœ… Conecta ao PostgreSQL (NIMBUS)
2. âœ… Busca dados usando DISTINCT ON (mesma lÃ³gica do script original)
3. âœ… Exporta para formato **Parquet** (mais eficiente que CSV/SQL)
4. âœ… Carrega automaticamente no BigQuery
5. âœ… Cria dataset/tabela se nÃ£o existir

**âš ï¸ IMPORTANTE:** NÃ£o existe formato "SQL" para BigQuery!
- BigQuery **NÃƒO** aceita arquivos `.sql` com INSERT statements
- BigQuery **NÃƒO** aceita dumps PostgreSQL diretamente
- VocÃª precisa **exportar** dados do PostgreSQL para CSV/Parquet/JSON primeiro
- O script faz isso **automaticamente**!

---

## ğŸ”‘ Obter Credenciais

### MÃ©todo Recomendado: Service Account

#### Passo 1: Acessar o Console do GCP

1. Acesse: https://console.cloud.google.com
2. Selecione seu projeto (ou crie um novo se necessÃ¡rio)

#### Passo 2: Criar Service Account

1. No menu lateral, vÃ¡ em **"IAM & Admin"** â†’ **"Service Accounts"**
   - Ou acesse diretamente: https://console.cloud.google.com/iam-admin/serviceaccounts

2. Clique em **"Create Service Account"** ou **"Criar conta de serviÃ§o"**

3. Preencha os dados:
   - **Service account name:** `bigquery-exporter` (ou o nome que preferir)
   - **Service account ID:** SerÃ¡ gerado automaticamente
   - **Description:** `Service account para exportar dados para BigQuery`

4. Clique em **"Create and Continue"**

#### Passo 3: Conceder PermissÃµes (Roles)

Adicione as seguintes roles (uma por vez):

**Role 1: BigQuery Data Editor**
- Procure por: `BigQuery Data Editor`
- Selecione: **"BigQuery Data Editor"**

**Role 2: BigQuery Job User**
- Procure por: `BigQuery Job User`
- Selecione: **"BigQuery Job User"**

**Role 3: BigQuery User** (opcional, mas recomendado)
- Procure por: `BigQuery User`
- Selecione: **"BigQuery User"**

#### Passo 4: Criar e Baixar a Chave JSON

1. Na lista de Service Accounts, clique na conta criada
2. VÃ¡ na aba **"Keys"** (Chaves)
3. Clique em **"Add Key"** â†’ **"Create new key"**
4. Selecione o formato: **"JSON"**
5. Clique em **"Create"**
6. O arquivo JSON serÃ¡ baixado automaticamente!

**âš ï¸ IMPORTANTE:** 
- Guarde este arquivo com seguranÃ§a
- NÃ£o compartilhe publicamente
- NÃ£o faÃ§a commit no Git (jÃ¡ estÃ¡ no .gitignore)

#### Passo 5: Salvar o Arquivo

1. Renomeie o arquivo baixado para: `credentials.json`
2. Coloque na pasta `credentials/` na raiz do projeto:

```
projeto/
â””â”€â”€ credentials/
    â””â”€â”€ credentials.json  â† Arquivo baixado aqui
```

### MÃ©todo Alternativo: Credenciais PadrÃ£o do Ambiente

Se vocÃª nÃ£o quiser usar Service Account, pode usar as credenciais padrÃ£o do ambiente:

```bash
# Instalar Google Cloud SDK
# https://cloud.google.com/sdk/docs/install

# Autenticar-se
gcloud auth application-default login

# NÃ£o precisa configurar BIGQUERY_CREDENTIALS_PATH no .env
```

**âš ï¸ LimitaÃ§Ã£o:** Este mÃ©todo usa suas credenciais pessoais, nÃ£o Ã© ideal para produÃ§Ã£o.

---

## ğŸ”§ Configurar VariÃ¡veis

### VariÃ¡veis NecessÃ¡rias no .env

```env
# BigQuery
BIGQUERY_PROJECT_ID=seu-projeto-id
BIGQUERY_DATASET_ID=nome-do-dataset
BIGQUERY_TABLE_ID=nome-da-tabela
BIGQUERY_CREDENTIALS_PATH=/caminho/credentials.json  # Opcional
```

### 1. BIGQUERY_PROJECT_ID

**Onde encontrar:**

#### Via Console GCP (Mais FÃ¡cil)
1. Acesse: https://console.cloud.google.com
2. No topo da pÃ¡gina, vocÃª verÃ¡ o **ID do projeto** ao lado do nome do projeto
3. Exemplo: Se o nome Ã© "Meu Projeto", o ID pode ser `1029418267270`

#### Via Linha de Comando
```bash
# Listar projetos
gcloud projects list

# Ver projeto atual
gcloud config get-value project
```

**ğŸ’¡ Dica:** Use o **ID numÃ©rico**, nÃ£o o nome do projeto!

### 2. BIGQUERY_DATASET_ID

**Onde encontrar:**

1. Acesse: https://console.cloud.google.com/bigquery
2. No painel esquerdo, vocÃª verÃ¡ os **datasets** do projeto
3. Se nÃ£o existir, vocÃª precisa criar um:

**Criar Dataset:**
1. No BigQuery Console, clique em **"Create Dataset"** ou **"Criar conjunto de dados"**
2. Configure:
   - **Dataset ID:** `pluviometricos` (ou o nome que preferir)
   - **Location type:** `Multi-region` ou `Region` (ex: `us-west1`)
   - **Default table expiration:** Deixe em branco ou configure
3. Clique em **"Create dataset"**

**ğŸ’¡ Dica:** O dataset serÃ¡ criado automaticamente pelo script se nÃ£o existir!

### 3. BIGQUERY_TABLE_ID

**Onde encontrar:**

1. Acesse: https://console.cloud.google.com/bigquery
2. Expanda o dataset no painel esquerdo
3. VocÃª verÃ¡ as tabelas dentro do dataset
4. Se nÃ£o existir, o script criarÃ¡ automaticamente!

**ğŸ’¡ Dica:** O script criarÃ¡ a tabela automaticamente na primeira execuÃ§Ã£o!

### 4. BIGQUERY_CREDENTIALS_PATH (Opcional)

**Quando usar:**
- âœ… Se vocÃª estÃ¡ rodando o script **localmente** (nÃ£o no GCP)
- âœ… Se precisa de autenticaÃ§Ã£o especÃ­fica
- âœ… Se nÃ£o quer usar `gcloud auth application-default login`

**ğŸ’¡ Dica:** O script detecta automaticamente `credentials/credentials.json` se nÃ£o configurar no `.env`

---

## ğŸ“¤ ExportaÃ§Ã£o de Dados

### Processo Completo: PostgreSQL â†’ Arquivo â†’ BigQuery

**NÃ£o existe formato "SQL" para BigQuery!**

O processo Ã©:
1. **PostgreSQL (NIMBUS)** â†’ Dados em tabelas (formato interno)
2. **Exportar** â†’ Converter para arquivo (CSV/Parquet/JSON)
3. **BigQuery** â†’ Importar arquivo â†’ Dados em tabelas

### Usar Script Automatizado (Recomendado)

```bash
# Configurar .env primeiro
BIGQUERY_PROJECT_ID=seu-projeto-gcp
BIGQUERY_DATASET_ID=pluviometricos
BIGQUERY_TABLE_ID=pluviometricos

# Executar
python scripts/bigquery/exportar_pluviometricos_nimbus_bigquery.py
```

**Pronto!** Os dados estarÃ£o no BigQuery! ğŸ‰

### O que o Script Faz

1. âœ… **Conecta ao PostgreSQL (NIMBUS)**
2. âœ… **Busca dados usando SQL** com DISTINCT ON
3. âœ… **Converte para DataFrame (Pandas)**
4. âœ… **Exporta para Parquet** (formato otimizado)
5. âœ… **Carrega no BigQuery automaticamente**

**VocÃª nÃ£o precisa se preocupar com formatos!** O script faz tudo.

---

## ğŸ“Š Formatos Suportados

O BigQuery aceita os seguintes formatos de arquivo para importaÃ§Ã£o:

### 1. CSV (Comma-Separated Values)
- âœ… **Suportado:** Sim
- âœ… **Comprimido:** Sim (GZIP)
- âš¡ **Performance:** Boa
- ğŸ’¾ **Tamanho:** MÃ©dio

### 2. JSON (JavaScript Object Notation)
- âœ… **Suportado:** Sim
- âœ… **Comprimido:** Sim (GZIP)
- âš¡ **Performance:** MÃ©dia
- ğŸ’¾ **Tamanho:** Grande

### 3. Parquet â­ RECOMENDADO
- âœ… **Suportado:** Sim
- âœ… **Comprimido:** Sim (nativo)
- âš¡ **Performance:** **Excelente** (mais rÃ¡pido)
- ğŸ’¾ **Tamanho:** **Menor** (mais eficiente)
- ğŸ¯ **Vantagens:**
  - Formato colunar (otimizado para anÃ¡lises)
  - CompressÃ£o automÃ¡tica
  - Preserva tipos de dados
  - Mais rÃ¡pido para carregar

**Por que usar Parquet:**
- âœ… 5-10x mais rÃ¡pido que CSV
- âœ… 50-80% menor que CSV
- âœ… Preserva tipos de dados (nÃ£o precisa conversÃ£o)
- âœ… Ideal para BigQuery

### 4. Avro
- âœ… **Suportado:** Sim
- âœ… **Comprimido:** Sim (nativo)
- âš¡ **Performance:** Boa
- ğŸ’¾ **Tamanho:** MÃ©dio

### 5. ORC (Optimized Row Columnar)
- âœ… **Suportado:** Sim
- âœ… **Comprimido:** Sim (nativo)
- âš¡ **Performance:** Excelente
- ğŸ’¾ **Tamanho:** Menor

### ComparaÃ§Ã£o de Formatos

| Formato | Velocidade | Tamanho | CompressÃ£o | Preserva Tipos | Recomendado |
|---------|------------|---------|------------|----------------|-------------|
| **Parquet** | â­â­â­â­â­ | â­â­â­â­â­ | âœ… Nativa | âœ… Sim | âœ… **SIM** |
| **ORC** | â­â­â­â­â­ | â­â­â­â­â­ | âœ… Nativa | âœ… Sim | âœ… Sim |
| **Avro** | â­â­â­â­ | â­â­â­ | âœ… Nativa | âœ… Sim | âš ï¸ MÃ©dio |
| **CSV** | â­â­â­ | â­â­ | âš ï¸ GZIP | âŒ NÃ£o | âš ï¸ BÃ¡sico |
| **JSON** | â­â­ | â­ | âš ï¸ GZIP | âš ï¸ Parcial | âŒ NÃ£o |

---

## ğŸ“ Visualizar Dados

### Onde os Dados EstÃ£o Armazenados?

Os dados sÃ£o armazenados **diretamente no BigQuery**, que Ã© um data warehouse gerenciado pelo Google Cloud.

**CaracterÃ­sticas:**
- âœ… Armazenamento **nativo** do BigQuery (nÃ£o precisa configurar buckets)
- âœ… **Otimizado** para consultas SQL rÃ¡pidas
- âœ… **EscalÃ¡vel** automaticamente
- âœ… **Seguro** e gerenciado pelo Google

**âš ï¸ Importante:** Os dados **NÃƒO** sÃ£o salvos automaticamente no Google Cloud Storage. Eles ficam apenas no BigQuery.

### Como Acessar os Dados

#### MÃ©todo 1: BigQuery Console (Web)

1. **Acesse o Console do BigQuery:**
   ```
   https://console.cloud.google.com/bigquery
   ```

2. **Selecione seu projeto:**
   - No topo da pÃ¡gina, clique no dropdown de projetos
   - Selecione o projeto configurado

3. **Navegue atÃ© sua tabela:**
   - No painel esquerdo, expanda seu projeto
   - Expanda o dataset (ex: `alertadb_cor_raw`)
   - Clique na tabela (ex: `pluviometricos`)

4. **Visualize os dados:**
   - Clique na aba **"Preview"** para ver uma prÃ©via dos dados
   - Ou clique em **"Query"** para escrever consultas SQL

#### MÃ©todo 2: Consulta SQL Direta

1. **No BigQuery Console, clique em "Compose new query"** ou use o editor SQL

2. **Escreva sua consulta:**
   ```sql
   -- Ver primeiros 100 registros
   SELECT *
   FROM `seu-projeto.dataset.pluviometricos`
   LIMIT 100;
   ```

3. **Execute a query:**
   - Clique em **"Run"** ou pressione `Ctrl+Enter`
   - Os resultados aparecerÃ£o abaixo

### Exemplos de Consultas Ãšteis

#### Ver Total de Registros
```sql
SELECT COUNT(*) as total_registros
FROM `seu-projeto.dataset.pluviometricos`;
```

#### Ver Dados de uma EstaÃ§Ã£o EspecÃ­fica
```sql
SELECT *
FROM `seu-projeto.dataset.pluviometricos`
WHERE estacao_id = 1
ORDER BY dia DESC
LIMIT 100;
```

#### Ver Dados por PerÃ­odo
```sql
SELECT *
FROM `seu-projeto.dataset.pluviometricos`
WHERE dia >= '2024-01-01'
  AND dia < '2024-02-01'
ORDER BY dia DESC;
```

#### Ver EstatÃ­sticas por EstaÃ§Ã£o
```sql
SELECT 
  estacao,
  estacao_id,
  COUNT(*) as total_registros,
  MIN(dia) as primeira_data,
  MAX(dia) as ultima_data
FROM `seu-projeto.dataset.pluviometricos`
GROUP BY estacao, estacao_id
ORDER BY estacao_id;
```

---

## ğŸ“Š Formato dos Dados

### ComparaÃ§Ã£o: NIMBUS vs BigQuery

**âœ… Os dados sÃ£o praticamente idÃªnticos**, com pequenas diferenÃ§as tÃ©cnicas de tipos de dados que **nÃ£o afetam os valores** nem a visualizaÃ§Ã£o.

### Estrutura das Colunas

| Coluna | NIMBUS (Origem) | BigQuery (Destino) | Mudou? |
|--------|-----------------|---------------------|--------|
| `dia` | `horaLeitura` (TIMESTAMP WITH TIME ZONE) | `dia` (TIMESTAMP) | âœ… Nome padronizado |
| `dia_original` | - | `dia_original` (STRING) | âœ… Novo (formato original com timezone) |
| `m05` | NUMERIC(10,2) | FLOAT64 | âš ï¸ Tipo diferente, mas valores iguais |
| `m10` | NUMERIC(10,2) | FLOAT64 | âš ï¸ Tipo diferente, mas valores iguais |
| `m15` | NUMERIC(10,2) | FLOAT64 | âš ï¸ Tipo diferente, mas valores iguais |
| `h01` | NUMERIC(10,2) | FLOAT64 | âš ï¸ Tipo diferente, mas valores iguais |
| `h04` | NUMERIC(10,2) | FLOAT64 | âš ï¸ Tipo diferente, mas valores iguais |
| `h24` | NUMERIC(10,2) | FLOAT64 | âš ï¸ Tipo diferente, mas valores iguais |
| `h96` | NUMERIC(10,2) | FLOAT64 | âš ï¸ Tipo diferente, mas valores iguais |
| `estacao` | VARCHAR(150) | STRING | âœ… Equivalente |
| `estacao_id` | INTEGER | INTEGER | âœ… IdÃªntico |

### Coluna `dia_original`

A coluna `dia_original` preserva o formato original do NIMBUS com timezone:
- Formato: `2009-02-18 00:57:20.000 -0300`
- Mostra claramente se Ã© horÃ¡rio padrÃ£o (`-0300`) ou horÃ¡rio de verÃ£o (`-0200`)

### Garantias de ConsistÃªncia

- âœ… **Mesma lÃ³gica** de DISTINCT ON
- âœ… **Mesmos valores** (precisÃ£o suficiente)
- âœ… **Mesma ordem** de processamento
- âœ… **Timezone preservado** na coluna `dia_original`

---

## â° AutomaÃ§Ã£o

### SincronizaÃ§Ã£o Incremental AutomÃ¡tica (Recomendado)

Para manter os dados atualizados automaticamente a cada 5 minutos, use o script de sincronizaÃ§Ã£o incremental.

#### PrÃ©-requisitos

1. **Carga inicial concluÃ­da**: Executeu `exportar_pluviometricos_nimbus_bigquery.py` com sucesso
2. **Servidor Linux** com acesso ao banco Nimbus
3. **Python 3.8+** instalado
4. **Credenciais do GCP** configuradas (`credentials/credentials.json`)
5. **VariÃ¡veis de ambiente** configuradas no `.env`

#### ConfiguraÃ§Ã£o RÃ¡pida

1. **Testar o Script Manualmente**
   ```bash
   python scripts/bigquery/sincronizar_pluviometricos_nimbus_bigquery.py --once
   ```

2. **Configurar Cron Automaticamente**
   ```bash
   cd automacao
   ./configurar_cron.sh bigquery
   ```

   Isso configurarÃ¡ o cron para executar a cada 5 minutos automaticamente.

#### ConfiguraÃ§Ã£o Manual

1. **Editar crontab**
   ```bash
   crontab -e
   ```

2. **Adicionar linha para executar a cada 5 minutos**
   ```bash
   # SincronizaÃ§Ã£o incremental BigQuery a cada 5 minutos
   */5 * * * * /caminho/completo/para/automacao/cron.sh bigquery
   ```

### ExportaÃ§Ã£o Completa PeriÃ³dica (Opcional)

Se preferir fazer exportaÃ§Ã£o completa periÃ³dica ao invÃ©s de incremental:

1. **Preparar o Ambiente**
   ```bash
   cd /caminho/para/script_conexao_alertadb
   source .venv/bin/activate  # Se usar ambiente virtual
   pip install -r requirements.txt
   ```

2. **Testar o Script Manualmente**
   ```bash
   python scripts/bigquery/exportar_pluviometricos_nimbus_bigquery.py
   ```

3. **Configurar Cron**
   ```bash
   crontab -e
   ```

4. **Adicionar Linha para Executar Diariamente Ã s 2h**
   ```bash
   # Exportar dados NIMBUS â†’ BigQuery diariamente Ã s 2h (completo)
   0 2 * * * cd /caminho/para/script_conexao_alertadb && /caminho/para/python3 scripts/bigquery/exportar_pluviometricos_nimbus_bigquery.py >> /var/log/bigquery_export.log 2>&1
   ```

   **Ou executar a cada 6 horas:**
   ```bash
   # Exportar dados NIMBUS â†’ BigQuery a cada 6 horas (completo)
   0 */6 * * * cd /caminho/para/script_conexao_alertadb && /caminho/para/python3 scripts/bigquery/exportar_pluviometricos_nimbus_bigquery.py >> /var/log/bigquery_export.log 2>&1
   ```

### Formato Cron

```
* * * * * comando
â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€ Dia da semana (0-7, 0 e 7 = domingo)
â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€ MÃªs (1-12)
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€ Dia do mÃªs (1-31)
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hora (0-23)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minuto (0-59)
```

### Exemplos de Agendamento

| Cron | DescriÃ§Ã£o |
|------|-----------|
| `0 2 * * *` | Diariamente Ã s 2h |
| `0 */6 * * *` | A cada 6 horas |
| `0 3 * * 0` | Semanalmente (domingo Ã s 3h) |
| `0 0 1 * *` | Mensalmente (dia 1 Ã s 0h) |
| `*/30 * * * *` | A cada 30 minutos |

### Verificar Logs

```bash
# Ver logs em tempo real
tail -f /var/log/bigquery_export.log

# Ver Ãºltimas 100 linhas
tail -n 100 /var/log/bigquery_export.log

# Procurar erros
grep -i error /var/log/bigquery_export.log
```

---

## ğŸš¨ Troubleshooting

### Erro: "Permission denied"
**Causa:** Service Account nÃ£o tem permissÃµes suficientes.

**SoluÃ§Ã£o:**
1. VÃ¡ em **IAM & Admin** â†’ **Service Accounts**
2. Clique na sua Service Account
3. VÃ¡ em **"Permissions"** ou **"IAM"**
4. Adicione as roles: `BigQuery Data Editor`, `BigQuery Job User`, `BigQuery User`

### Erro: "Invalid credentials"
**Causa:** Arquivo JSON corrompido ou caminho incorreto.

**SoluÃ§Ã£o:**
1. Verifique se o arquivo estÃ¡ em `credentials/credentials.json`
2. Verifique se o arquivo nÃ£o estÃ¡ corrompido
3. Tente baixar novamente a chave JSON

### Erro: "Project not found"
**Causa:** `project_id` no JSON nÃ£o corresponde ao projeto atual.

**SoluÃ§Ã£o:**
1. Verifique o `BIGQUERY_PROJECT_ID` no `.env`
2. Certifique-se de que corresponde ao `project_id` no JSON

### Erro de MemÃ³ria
**Causa:** Processando muitos dados de uma vez.

**SoluÃ§Ã£o:**
- O script jÃ¡ estÃ¡ otimizado para usar mÃ­nima memÃ³ria
- Processa em chunks pequenos (25k registros)
- Escreve mÃºltiplos arquivos Parquet (nÃ£o acumula tudo)

---

## ğŸ” Compartilhar Acesso

Para conceder acesso de **somente leitura** (consulta) no BigQuery para clientes usando Service Accounts, consulte o guia completo:

ğŸ“š **[Guia Completo: Compartilhar Acesso ao BigQuery](BIGQUERY_COMPARTILHAR_ACESSO.md)**

### Resumo RÃ¡pido

**Service Account do Cliente:**
- Email: `lncc-cefet@rj-cor.iam.gserviceaccount.com`
- Project ID: `rj-cor`

**Como Conceder Acesso:**

1. **Via Console GCP:**
   - BigQuery â†’ Dataset â†’ Share dataset
   - Adicionar: `lncc-cefet@rj-cor.iam.gserviceaccount.com`
   - Role: **BigQuery Data Viewer** (somente leitura)
   - Salvar

2. **Via CLI:**
   ```bash
   bq add-iam-member \
     --member="serviceAccount:lncc-cefet@rj-cor.iam.gserviceaccount.com" \
     --role="roles/bigquery.dataViewer" \
     "alertadb-cor:alertadb_cor_raw"
   ```
---

## ğŸ“š Links Ãšteis

- **BigQuery Console:** https://console.cloud.google.com/bigquery
- **Service Accounts:** https://console.cloud.google.com/iam-admin/serviceaccounts
- **BigQuery Connections:** https://console.cloud.google.com/bigquery/connections
- **GCP Projects:** https://console.cloud.google.com/cloud-resource-manager
- **DocumentaÃ§Ã£o BigQuery:** https://cloud.google.com/bigquery/docs
- **Compartilhar Acesso:** [BIGQUERY_COMPARTILHAR_ACESSO.md](BIGQUERY_COMPARTILHAR_ACESSO.md)

---

**Ãšltima atualizaÃ§Ã£o:** 2025

