# üìä Processo Completo: PostgreSQL (NIMBUS) ‚Üí BigQuery

Guia passo a passo explicando como os dados fluem do PostgreSQL para o BigQuery.

---

## üéØ Entendendo o Problema

### ‚ùå O que voc√™ PODE estar pensando:

"Quero enviar um arquivo SQL do banco NIMBUS para o BigQuery"

### ‚úÖ O que REALMENTE acontece:

**N√£o existe formato "SQL" para BigQuery!**

O processo √©:
1. **PostgreSQL (NIMBUS)** ‚Üí Dados em tabelas (formato interno)
2. **Exportar** ‚Üí Converter para arquivo (CSV/Parquet/JSON)
3. **BigQuery** ‚Üí Importar arquivo ‚Üí Dados em tabelas

---

## üîÑ Fluxo Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL (NIMBUS) ‚îÇ
‚îÇ   (Tabelas SQL)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ [EXPORTAR]
           ‚îÇ SELECT ... ‚Üí Arquivo
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Arquivo    ‚îÇ
    ‚îÇ CSV/Parquet/ ‚îÇ
    ‚îÇ   JSON/etc   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ [IMPORTAR]
           ‚îÇ BigQuery l√™ arquivo
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   BigQuery   ‚îÇ
    ‚îÇ  (Tabelas)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù Por Que N√£o Existe Formato "SQL"?

### **No PostgreSQL:**
- Dados est√£o em **tabelas relacionais**
- Formato interno do PostgreSQL (n√£o √© um arquivo)
- Voc√™ pode fazer `pg_dump` que gera arquivo `.sql` com comandos `INSERT`

### **No BigQuery:**
- Aceita arquivos de **dados tabulares**: CSV, JSON, Parquet, Avro, ORC
- **N√ÉO aceita:** Arquivos `.sql` com comandos `INSERT`
- **N√ÉO aceita:** Dumps PostgreSQL diretamente

### **Por qu√™?**
- BigQuery √© um data warehouse (n√£o um banco relacional)
- Precisa de dados em formato tabular/colunar
- N√£o executa comandos SQL de INSERT

---

## üöÄ Solu√ß√£o: Script Automatizado

Criei o script `scripts/bigquery/exportar_nimbus_para_bigquery.py` que faz **TUDO automaticamente**:

### O que o script faz:

1. ‚úÖ **Conecta ao PostgreSQL (NIMBUS)**
   ```python
   conn = psycopg2.connect(**ORIGEM)
   ```

2. ‚úÖ **Busca dados usando SQL**
   ```sql
   SELECT DISTINCT ON (...) 
   FROM estacoes_leitura ...
   ```

3. ‚úÖ **Converte para DataFrame (Pandas)**
   ```python
   df = pd.read_sql(query, conn)
   ```

4. ‚úÖ **Exporta para Parquet** (formato otimizado)
   ```python
   df.to_parquet('arquivo.parquet')
   ```

5. ‚úÖ **Carrega no BigQuery automaticamente**
   ```python
   client.load_table_from_file(...)
   ```

**Voc√™ n√£o precisa se preocupar com formatos!** O script faz tudo.

---

## üìä Formatos: Do Banco Para Arquivo

### **Op√ß√£o 1: CSV** (mais comum, mas n√£o ideal)

```bash
# Exportar do PostgreSQL
psql -h 10.2.223.114 -U user -d alertadb \
  -c "COPY (SELECT * FROM tabela) TO STDOUT WITH CSV HEADER" \
  > dados.csv

# Importar no BigQuery
bq load --source_format=CSV projeto:dataset.tabela dados.csv
```

**Problemas:**
- ‚ö†Ô∏è Perde tipos de dados (tudo vira string)
- ‚ö†Ô∏è Mais lento
- ‚ö†Ô∏è Arquivo maior

### **Op√ß√£o 2: Parquet** ‚≠ê RECOMENDADO

```python
# Exportar do PostgreSQL
import pandas as pd
import psycopg2

conn = psycopg2.connect(...)
df = pd.read_sql("SELECT * FROM tabela", conn)
df.to_parquet('dados.parquet')  # Preserva tipos!

# Importar no BigQuery
from google.cloud import bigquery
client = bigquery.Client()
client.load_table_from_file(open('dados.parquet', 'rb'), ...)
```

**Vantagens:**
- ‚úÖ Preserva tipos de dados
- ‚úÖ 5-10x mais r√°pido
- ‚úÖ 50-80% menor
- ‚úÖ Formato colunar (otimizado)

### **Op√ß√£o 3: JSON**

```python
df.to_json('dados.json', orient='records', lines=True)
```

**Problemas:**
- ‚ö†Ô∏è Arquivo muito grande
- ‚ö†Ô∏è Mais lento que Parquet

---

## üîç Exemplo Pr√°tico

### **Cen√°rio:** Voc√™ tem dados no PostgreSQL e quer no BigQuery

#### **‚ùå N√ÉO funciona:**

```sql
-- Arquivo: dados.sql
INSERT INTO tabela VALUES (1, '2024-01-01', 10.5);
INSERT INTO tabela VALUES (2, '2024-01-02', 20.3);
```

BigQuery **N√ÉO** aceita isso!

#### **‚úÖ FUNCIONA:**

```csv
# Arquivo: dados.csv
dia,m05,m10,estacao,estacao_id
2024-01-01 00:00:00,10.5,20.3,Esta√ß√£o A,1
2024-01-02 00:00:00,15.2,25.1,Esta√ß√£o B,2
```

Ou melhor ainda, **Parquet** (bin√°rio, mais eficiente).

---

## üéØ Resumo

### **Pergunta:** Como enviar dados do PostgreSQL (NIMBUS) para BigQuery em formato SQL?

### **Resposta:**

1. ‚ùå **N√£o existe formato "SQL"** para BigQuery
2. ‚úÖ **Exporte dados** do PostgreSQL para CSV/Parquet/JSON
3. ‚úÖ **Importe arquivo** no BigQuery
4. ‚úÖ **Ou use o script** que faz tudo automaticamente

### **O Script Faz:**

```
PostgreSQL ‚Üí Python (pandas) ‚Üí Parquet ‚Üí BigQuery
```

**Voc√™ s√≥ executa:**
```bash
python scripts/bigquery/exportar_nimbus_para_bigquery.py
```

**N√£o precisa se preocupar com formatos!** üéâ

---

## üìö Documenta√ß√£o Relacionada

- [Integra√ß√£o BigQuery](BIGQUERY_INTEGRACAO.md) - Guia completo
- [Formatos Suportados](BIGQUERY_FORMATOS_SUPORTADOS.md) - Detalhes t√©cnicos
- Script: `scripts/bigquery/exportar_nimbus_para_bigquery.py`

---

**√öltima atualiza√ß√£o:** 2025

