# ðŸ“Š IntegraÃ§Ã£o NIMBUS/Cloud SQL â†’ BigQuery

Guia completo para exportar dados para o BigQuery e disponibilizar para stakeholders.

---

## ðŸŽ¯ VisÃ£o Geral

Ã‰ **totalmente possÃ­vel** exportar dados para o BigQuery! Existem vÃ¡rias abordagens:

### **OpÃ§Ã£o 1: NIMBUS â†’ BigQuery (Direto)** â­ NOVO
- Exporta diretamente do NIMBUS para BigQuery
- Mais rÃ¡pido (menos camadas)
- Script: `scripts/bigquery/exportar_nimbus_para_bigquery.py`

### **OpÃ§Ã£o 2: Cloud SQL â†’ BigQuery (Federated Queries)**
- Consulta dados do Cloud SQL diretamente no BigQuery
- Sem necessidade de copiar dados
- Dados sempre atualizados

### **OpÃ§Ã£o 3: Cloud SQL â†’ BigQuery (ExportaÃ§Ã£o)**
- Exporta dados do Cloud SQL para BigQuery
- Dados em BigQuery (mais rÃ¡pido para consultas)
- Requer sincronizaÃ§Ã£o periÃ³dica

---

## ðŸš€ OpÃ§Ã£o 1: NIMBUS â†’ BigQuery (Direto) â­ RECOMENDADO

### Como Funciona:

O script faz automaticamente:
1. âœ… Conecta ao PostgreSQL (NIMBUS)
2. âœ… Busca dados usando DISTINCT ON (mesma lÃ³gica do script original)
3. âœ… Exporta para formato **Parquet** (mais eficiente que CSV/SQL)
4. âœ… Carrega automaticamente no BigQuery
5. âœ… Cria dataset/tabela se nÃ£o existir

**âš ï¸ IMPORTANTE:** NÃ£o existe formato "SQL" para BigQuery!
- BigQuery **NÃƒO** aceita arquivos `.sql` com INSERT statements
- BigQuery **NÃƒO** aceita dumps PostgreSQL diretamente
- VocÃª precisa **exportar** dados do PostgreSQL para CSV/Parquet/JSON primeiro
- O script faz isso **automaticamente**!

### Vantagens:
- âœ… Mais rÃ¡pido (menos camadas)
- âœ… Dados sempre da fonte original
- âœ… Formato Parquet (5-10x mais rÃ¡pido que CSV)
- âœ… Ideal para anÃ¡lises pesadas
- âœ… Processa tudo automaticamente

### Como Usar:

#### 1. Configurar .env

```env
# BigQuery
BIGQUERY_PROJECT_ID=seu-projeto-gcp
BIGQUERY_DATASET_ID=pluviometricos
BIGQUERY_TABLE_ID=pluviometricos
BIGQUERY_CREDENTIALS_PATH=/caminho/credentials.json  # Opcional
```

#### 2. Configurar Credenciais GCP

```bash
# OpÃ§Ã£o 1: Usar credenciais padrÃ£o (recomendado)
gcloud auth application-default login

# OpÃ§Ã£o 2: Usar arquivo de credenciais
# Baixar JSON do Console GCP â†’ IAM â†’ Service Accounts
```

#### 3. Executar Script

```bash
python scripts/bigquery/exportar_nimbus_para_bigquery.py
```

**Pronto!** Os dados estarÃ£o no BigQuery! ðŸŽ‰

ðŸ“š **Mais detalhes:** [ExportaÃ§Ã£o SQL â†’ BigQuery](BIGQUERY_EXPORTACAO_SQL.md)

---

## ðŸ“Š Formatos Suportados pelo BigQuery

O BigQuery aceita os seguintes formatos:

1. âœ… **Parquet** â­ RECOMENDADO (mais rÃ¡pido e eficiente)
2. âœ… **CSV** (comprimido ou nÃ£o)
3. âœ… **JSON** (comprimido ou nÃ£o)
4. âœ… **Avro**
5. âœ… **ORC**

ðŸ“š **Guia completo:** [Formatos Suportados](BIGQUERY_FORMATOS_SUPORTADOS.md)

---

## ðŸ”„ OpÃ§Ã£o 2: Cloud SQL â†’ BigQuery (Federated Queries)

Ã‰ **totalmente possÃ­vel** conectar Cloud SQL ao BigQuery! Existem duas abordagens principais:

### **OpÃ§Ã£o 1: Federated Queries (Recomendado)**
- Consulta dados do Cloud SQL diretamente no BigQuery
- Sem necessidade de copiar dados
- Dados sempre atualizados
- Ideal para consultas ad-hoc

### **OpÃ§Ã£o 2: ExportaÃ§Ã£o PeriÃ³dica**
- Exporta dados do Cloud SQL para BigQuery
- Dados em BigQuery (mais rÃ¡pido para consultas)
- Requer sincronizaÃ§Ã£o periÃ³dica
- Ideal para anÃ¡lises pesadas

---

## ðŸš€ OpÃ§Ã£o 1: Federated Queries (Recomendado)

### Vantagens:
- âœ… Dados sempre atualizados (em tempo real)
- âœ… NÃ£o precisa copiar dados
- âœ… Sem custo de armazenamento no BigQuery
- âœ… FÃ¡cil de configurar

### Desvantagens:
- âš ï¸ Consultas podem ser mais lentas (depende da latÃªncia)
- âš ï¸ Custo por query (mas muito baixo)

### Como Configurar:

#### 1. Habilitar Cloud SQL Connection no BigQuery

```bash
# Via Console GCP
# BigQuery â†’ Data â†’ External Data Sources â†’ Add Data Source
# Escolher: Cloud SQL â†’ PostgreSQL
```

Ou via `bq` CLI:

```bash
bq mk --connection \
  --connection_type='CLOUD_SQL' \
  --properties='{"instanceId":"alertadb-cor:us-west1:alertadb-cor","database":"alertadb_cor","type":"POSTGRES"}' \
  --connection_credential='{"username":"postgres","password":"sua_senha"}' \
  --project_id=seu-projeto \
  --location=us-west1 \
  cloudsql_connection
```

#### 2. Criar Tabela Externa no BigQuery

```sql
CREATE OR REPLACE EXTERNAL TABLE `seu-projeto.dataset.pluviometricos`
WITH CONNECTION `seu-projeto.us-west1.cloudsql_connection`
OPTIONS (
  object_metadata='LIST',
  uris=['alertadb-cor:us-west1:alertadb-cor/alertadb_cor/public/pluviometricos']
);
```

#### 3. Consultar no BigQuery

```sql
-- Agora vocÃª pode consultar diretamente!
SELECT 
  dia,
  estacao,
  h24,
  COUNT(*) as total_registros
FROM `seu-projeto.dataset.pluviometricos`
WHERE dia >= '2024-01-01'
GROUP BY dia, estacao, h24
ORDER BY dia DESC;
```

---

## ðŸ“¤ OpÃ§Ã£o 2: ExportaÃ§Ã£o PeriÃ³dica para BigQuery

### Vantagens:
- âœ… Consultas muito rÃ¡pidas (dados no BigQuery)
- âœ… Ideal para anÃ¡lises pesadas
- âœ… Pode usar recursos do BigQuery (ML, etc.)

### Desvantagens:
- âš ï¸ Dados podem estar desatualizados (depende da frequÃªncia)
- âš ï¸ Custo de armazenamento no BigQuery
- âš ï¸ Requer script de sincronizaÃ§Ã£o

### Como Configurar:

#### 1. Criar Dataset no BigQuery

```bash
bq mk --dataset --location=us-west1 seu-projeto:pluviometricos
```

#### 2. Exportar Dados do Cloud SQL para BigQuery

**Via Console GCP:**
1. Cloud SQL â†’ InstÃ¢ncias â†’ `alertadb-cor`
2. **Export** â†’ **Export to BigQuery**
3. Configurar:
   - Database: `alertadb_cor`
   - Table: `pluviometricos`
   - Dataset: `pluviometricos`
   - Table: `pluviometricos`

**Via `bq` CLI:**

```bash
# Exportar tabela completa
bq extract \
  --destination_format=CSV \
  --compression=GZIP \
  alertadb-cor:us-west1:alertadb_cor.pluviometricos \
  gs://seu-bucket/pluviometricos/export_*.csv

# Carregar no BigQuery
bq load \
  --source_format=CSV \
  --skip_leading_rows=1 \
  --replace \
  seu-projeto:pluviometricos.pluviometricos \
  gs://seu-bucket/pluviometricos/export_*.csv \
  dia:TIMESTAMP,m05:NUMERIC,m10:NUMERIC,m15:NUMERIC,h01:NUMERIC,h04:NUMERIC,h24:NUMERIC,h96:NUMERIC,estacao:STRING,estacao_id:INTEGER
```

#### 3. Automatizar ExportaÃ§Ã£o (Script Python)

Criar script para exportar periodicamente:

```python
from google.cloud import bigquery
from google.cloud import sql
import psycopg2

# Conectar ao Cloud SQL
conn = psycopg2.connect(
    host='34.82.95.242',
    database='alertadb_cor',
    user='postgres',
    password='senha'
)

# Ler dados
query = "SELECT * FROM pluviometricos WHERE dia >= CURRENT_DATE - INTERVAL '1 day'"
df = pd.read_sql(query, conn)

# Carregar no BigQuery
client = bigquery.Client()
table_id = 'seu-projeto.pluviometricos.pluviometricos'
df.to_gbq(table_id, project_id='seu-projeto', if_exists='append')
```

---

## ðŸ”„ SincronizaÃ§Ã£o AutomÃ¡tica (Recomendado)

### Usar Cloud Functions + Cloud Scheduler

1. **Cloud Function** que exporta dados
2. **Cloud Scheduler** executa diariamente/horariamente
3. Dados sempre atualizados no BigQuery

---

## ðŸ“Š ComparaÃ§Ã£o das OpÃ§Ãµes

| CaracterÃ­stica | Federated Queries | ExportaÃ§Ã£o PeriÃ³dica |
|----------------|-------------------|----------------------|
| **Dados atualizados** | âœ… Tempo real | âš ï¸ Depende da frequÃªncia |
| **Velocidade consulta** | âš ï¸ Mais lento | âœ… Muito rÃ¡pido |
| **Custo armazenamento** | âœ… GrÃ¡tis | âš ï¸ Pago |
| **Complexidade** | âœ… Simples | âš ï¸ MÃ©dia |
| **Ideal para** | Consultas ad-hoc | AnÃ¡lises pesadas |

---

## ðŸŽ¯ RecomendaÃ§Ã£o

### Para Stakeholders:

**Use Federated Queries** se:
- âœ… Precisam de dados sempre atualizados
- âœ… Consultas nÃ£o sÃ£o muito pesadas
- âœ… Querem simplicidade

**Use ExportaÃ§Ã£o PeriÃ³dica** se:
- âœ… Precisam de anÃ¡lises muito pesadas
- âœ… Velocidade Ã© crÃ­tica
- âœ… Podem trabalhar com dados de atÃ© 1 dia

### HÃ­brido (Melhor dos dois mundos):

1. **Federated Queries** para consultas em tempo real
2. **ExportaÃ§Ã£o diÃ¡ria** para anÃ¡lises pesadas
3. **Dashboard** no BigQuery usando dados exportados

---

## ðŸ“ Exemplo de Consulta para Stakeholders

### No BigQuery (Federated Query):

```sql
-- EstatÃ­sticas por estaÃ§Ã£o
SELECT 
  estacao,
  COUNT(*) as total_registros,
  MIN(dia) as primeira_data,
  MAX(dia) as ultima_data,
  AVG(h24) as media_precipitacao_24h,
  SUM(h24) as total_precipitacao
FROM `seu-projeto.dataset.pluviometricos`
WHERE dia >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY estacao
ORDER BY total_precipitacao DESC;
```

---

## ðŸ”§ PrÃ³ximos Passos

1. âœ… Decidir entre Federated Queries ou ExportaÃ§Ã£o
2. âœ… Configurar conexÃ£o no BigQuery
3. âœ… Criar views/datasets para stakeholders
4. âœ… Configurar permissÃµes de acesso
5. âœ… Criar dashboards (Data Studio, Looker, etc.)

---

**Ãšltima atualizaÃ§Ã£o:** 2025

